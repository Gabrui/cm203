{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776b8f27",
   "metadata": {},
   "source": [
    "**Instituto Tecnol√≥gico de Aeron√°utica ‚Äì ITA**\n",
    "\n",
    "**Vis√£o Computacional - CM-203**\n",
    "\n",
    "**Professores:** \n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "Gabriel Adriano de Melo\n",
    "\n",
    "\n",
    "**Orienta√ß√µes padr√£o:**\n",
    "\n",
    "Antes de voc√™ entregar o Lab, tenha certeza de que tudo est√° rodando corretamente (sequencialmente): Primeiro, **reinicie o kernel** (`Runtime->Restart Runtime` no Colab ou `Kernel->Restart` no Jupyter), depois rode todas as c√©lulas (`Runtime->Run All` no Colab ou `Cell->Run All` no Jupyter) e verifique que as c√©lulas rodem sem erros, principalmente as de corre√ß√£o autom√°tica que apresentem os `assert`s.\n",
    "\n",
    "√â muito importante que voc√™s n√£o apaguem as c√©lulas de resposta para preenchimento, isto √©, as que contenham o `ESCREVA SEU C√ìDIGO AQUI` ou o \"ESCREVA SUA RESPOSTA AQUI\", al√©m das c√©lulas dos `assert`, pois elas cont√©m metadados com o id da c√©lula para os sistemas de corre√ß√£o automatizada e manual. O sistema de corre√ß√£o automatizada executa todo o c√≥digo do notebook, adicionando testes extras nas c√©lulas de teste. N√£o tem problema voc√™s criarem mais c√©lulas, mas n√£o apaguem as c√©lulas de corre√ß√£o. Mantenham a solu√ß√£o dentro do espa√ßo determinado, por organiza√ß√£o. Se por acidente acontecer de apagarem alguma c√©lula que deveria ter a resposta, recomendo iniciar de outro notebook (ou dar um `Undo` se poss√≠vel), pois n√£o adianta recriar a c√©lula porque perdeu o ID. Ou ent√£o voc√™ baixa e abre o notebook como texto (√© um JSON) e readiciona o campo de ID. Neste ano n√≥s tamb√©m colocamos um coment√°rio nessas c√©lulas que √© igual ao ID delas, para ser um failsafe em caso de sumirem com o ID das c√©lulas, ent√£o N√ÉO apaguem esse coment√°rio com ID.\n",
    "\n",
    "Os notebooks voc√™s podem alterar √† vontade, podem criar novas c√©lulas, modificar as existentes, apagar (a menos das c√©lulas de corre√ß√£o). O corretor autom√°tico executar√° todas as c√©lulas e verificar√° a presen√ßa de erro nos `asserts`, depois haver√° a corre√ß√£o manual das quest√µes com aprecia√ß√£o da resposta e coment√°rios gerados em HTML. Se ele n√£o achar a c√©lula com os asserts, fica sem a nota da quest√£o, se ele n√£o achar a c√©lular com a quest√£o, fica sem os coment√°rios. Mas voc√™s podem escreve sim c√≥digo fora dos espa√ßo delemitado pelo `ESCREVA SEU C√ìDIGO AQUI` sem problemas, s√≥ n√£o altera a assinatura da fun√ß√£o. Esse espa√ßo foi pensado para facilitar a sua implementa√ß√£o.\n",
    "\n",
    "Os Notebooks foram programados para serem compat√≠veis com o Google Colab, instalando as depend√™ncias necess√°rias automaticamente a baixando os datasets necess√°rios a cada Lab. Os comandos que se inicial por ! (ponto de exclama√ß√£o) s√£o de bash e tamb√©m podem ser executados no terminal linux, que justamente instalam as depend√™ncias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d6459",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb3ebd",
   "metadata": {},
   "source": [
    "# Laborat√≥rio 2 - nanoGrad\n",
    "\n",
    "**N√ÉO** use LLMs (ChatGPT).  **N√ÉO** pesquise a resposta na internet (**N√ÉO** copie o c√≥digo de reposit√≥rios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documenta√ß√£o do NumPy (mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias). Este laborat√≥rio foi baseado no material do [Andrej Karpathy](https://github.com/karpathy). Por isso voc√™ n√£o pode copiar o c√≥digo dele, mas pode assistir as [aulas dele](https://www.youtube.com/watch?v=VMj-3S1tku0) e tamb√©m outras aulas na internet, s√≥ n√£o copie o c√≥digo sem pensar/sem entender efetivamente o que ele faz.\n",
    "\n",
    "\n",
    "Neste laborat√≥rio vamos implementar o nosso pr√≥prio framework para calcular os gradientes automaticamente. O importante aqui √© que voc√™ entenda a intui√ß√£o de que a rede neural √© um grafo computacional, e cada n√≥ desse grafo computacional representa um valor (estado intermedi√°rio) que sofre opera√ß√µes (arestas direcionadas) e que resulta em novos valores (n√≥s). Tudo isso no c√°lculo direto.\n",
    "\n",
    "Mas tamb√©m temos o nosso c√°lculo dos gradientes (por backpropagation) que √© fazer o caminho reverso do que fizemos no nosso c√°lculo direto.\n",
    "\n",
    "E no final, todo esse trabalho tem o objetivo de otimizarmos os nossos par√¢metros: atualiz√°-los na dire√ß√£o do gradiente, isto √©, mudar os par√¢metros um pouquinho para cima ou para baixo a depender de que essa mudan√ßa melhore a nossa fun√ß√£o custo (aumentar ou diminuir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f275a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from pathlib import Path             # J√° importado pelo fastai\n",
    "# # from typing import Callable          # J√° importado pelo fastai\n",
    "# # import numpy as np                   # J√° importado pelo fastai\n",
    "# # import pandas as pd                  # J√° importado pelo fastai\n",
    "# # from PIL import Image                # J√° importado pelo fastai\n",
    "# # from matplotlib import pyplot as plt # J√° importado pelo fastai\n",
    "# from fastai.vision.all import *        # fastai==2.7.12\n",
    "# import cv2                             # opencv-contrib-python-headless==4.8.0.74\n",
    "# # \n",
    "\n",
    "\n",
    "\n",
    "# # Caso esteja executando o notebook localmente, reimplementa o cv2_imshow\n",
    "# from IPython.utils import io\n",
    "# from IPython.display import display\n",
    "# import warnings\n",
    "# try:\n",
    "#     from google.colab.patches import cv2_imshow\n",
    "# except:\n",
    "#     def cv2_imshow(img):\n",
    "#         display(Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "# # Faz o numpy imprimir mais n√∫meros por linha\n",
    "# np.set_printoptions(edgeitems=40, linewidth=130)\n",
    "\n",
    "# import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24345ce3",
   "metadata": {},
   "source": [
    "Importar as bibliotecas, vamos usar apenas o NumPy para fazer a nossas opera√ß√µes, o graphviz √© para plotar o grafo, o matplotlib para plotar os gr√°ficos, o fastcore √© s√≥ para usarmos o m√©todo de patch_to (monkey patching para alterar uma classe j√° existente), anota√ß√µes de tipos em Python 3.8, e o dataset sint√©tico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.10.12 ou >3.8\n",
    "import numpy as np                      # numpy==1.22.4\n",
    "from graphviz import Digraph            # graphviz==0.20.1\n",
    "import matplotlib.pyplot as plt         # matplotlib==3.7.1\n",
    "from fastcore.basics import patch_to    # fastcore==1.5.29\n",
    "\n",
    "\n",
    "from typing import Tuple, Callable, Union, Set, List\n",
    "from sklearn.datasets import make_moons # sklearn==1.3.0\n",
    "\n",
    "np.set_printoptions(precision=4) # pode mudar aqui se quiser\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401fa20",
   "metadata": {},
   "source": [
    "Dados sint√©ticos para treinar a rede neural (al√©m de mudar o diret√≥rio do notebook para o /content , que √© o que o Colab j√° faz):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe668352",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afef6e7abdb81ae12340522169a06294",
     "grade": false,
     "grade_id": "dataset_lab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# dataset_lab\n",
    "\n",
    "matriz_X, matriz_Y = make_moons(n_samples=200, noise=0.15, random_state=25)\n",
    "matriz_Y = matriz_Y.reshape(-1, 1).astype(np.float64)\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37c2f4",
   "metadata": {},
   "source": [
    "## N√≥s e Opera√ß√µes Computacionais B√°sicos (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce00d5",
   "metadata": {},
   "source": [
    "**Explica√ß√£o sobre o assunto**\n",
    "\n",
    "O elemento mais b√°sico de um grafo computacional s√£o os seus n√≥s (bolinhas), que na nossa representa√ß√£o, define um valor num√©rico (ou n-dimensional).\n",
    "\n",
    "O que d√° 'liga' aos elementos s√£o as suas conex√µes (arestas, setinhas), que na nossa representa√ß√£o, define a opera√ß√£o (transforma√ß√£o) que o valor sofre.\n",
    "\n",
    "Com as defini√ß√µes de n√≥s e arestas direcionadas podemos construir um grafo direcionado que consegue representar uma fun√ß√£o comput√°vel e, a menos do controle de fluxo abitr√°rio (loops com condi√ß√µes de parada n√£o-fixas), tamb√©m um programa 'bem comportado'.\n",
    "\n",
    "O n√≠vel de granularidade dos n√≥s e das opera√ß√µes (apenas um escalar vs matriz n-dimensional) depende do gosto do 'fregu√™s', inclusive, um n√≥ poderia ser uma rede neural inteira! S√≥ precisamos saber calcular a derivada da sa√≠da com rela√ß√£o √† entrada do n√≥ (isto √© o jacobiano para o caso n-dimensional).\n",
    "\n",
    "Mas para n√£o assustar (espero ter motivado), vamos come√ßar do caso mais simples: um n√≥ sendo apenas um escalar e com opera√ß√µes sobre valores escalares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0b83d",
   "metadata": {},
   "source": [
    "Observe com aten√ß√£o o c√≥digo abaixo, √© simples, mas √© crucial que voc√™ entenda o funcionamento do N√≥ Computacional e do seu contexto, que simplesmente guarda uma refer√™ncia para os seus pais e para a opera√ß√£o que os pais fizeram para poder ger√°-lo. Apenas essas informa√ß√µes (al√©m do valor) j√° √© o suficiente para definir uma estrutura computacional de grafo direcionado.\n",
    "\n",
    "O m√©todo `__init__` √© o inicializador da classe de Python (quando instanciamos um novo objeto da classe e passamos argumentos).\n",
    "\n",
    "O m√©todo `__add__` √© o que Python chama quando fazemos uma opera√ß√£o de `+` entre dois objetos da mesma classe (`NoComp(1) + NoComp(3)`).\n",
    "\n",
    "Semelhantemente o m√©todo `__mul__` √© para multiplica√ß√£o (operador `*`), isto √©, `NoComp(5) + NoComp(7)`.\n",
    "\n",
    "E finalmente o `tanh` √© uma fun√ß√£o com nome qualquer, mas que no caso implementa a fun√ß√£o tangente hiperb√≥lica, isto √©, `NoComp(1.2).tanh()`.\n",
    "\n",
    "Essas opera√ß√µes todas devem resultar em um novo objeto do `NoComp`, que √© a quest√£o que voc√™s v√£o desenvolver abaixo.\n",
    "\n",
    "A lista de filhos que colocamos √© opcional no sentido que a pr√≥pria constru√ß√£o do grafo (por meio das opera√ß√µes) j√° induz a no√ß√£o de filhos (cuidado se for pela aula do Andej que ele troca pai por filho). Mas deixar a lista expl√≠cita fica mais c√¥modo para imprimir o grafo ou se quisermos reaproveit√°-lo depois (em vez de construir um novo grafo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contexto:\n",
    "    \"\"\"Situa o n√≥ (quem o gerou): os seus pais e a opera√ß√£o que eles fizeram\"\"\"\n",
    "    \n",
    "    def __init__(self, pais:Tuple['NoComp'], operacao: Callable):\n",
    "        self.pais = pais\n",
    "        self.operacao = operacao\n",
    "\n",
    "class NoComp:\n",
    "    \"\"\"N√≥ Computacional que realiza opera√ß√µes b√°sicas, armazena o seu valor, gradiente e contexto no grafo\"\"\"\n",
    "    \n",
    "    def __init__(self, valor, ctx:Union[Contexto,None]=None, nome_visual:str=''):\n",
    "        self.valor = valor\n",
    "        self.ctx = ctx\n",
    "        self.nome_visual = nome_visual\n",
    "        self.gradiente = np.zeros_like(self.valor) if type(self.valor) == np.ndarray else 0\n",
    "        self.filhos = [] # Essa lista de filhos √© opcional\n",
    "        if ctx is not None:\n",
    "            for pai in ctx.pais:\n",
    "                pai.filhos.append(self)\n",
    "\n",
    "    def __add__(self, outro_noh):\n",
    "        contexto_filho = Contexto((self, outro_noh), adicao)\n",
    "        filho = adicao(contexto_filho)\n",
    "        return filho\n",
    "\n",
    "    def __mul__(self, outro_noh):\n",
    "        return multiplicacao(Contexto((self, outro_noh), multiplicacao))\n",
    "    \n",
    "    def __matmul__(self, outro_noh):\n",
    "        return multi_matrici(Contexto((self, outro_noh), multi_matrici))\n",
    "\n",
    "    def tanh(self):\n",
    "        return tangente_hipe(Contexto((self,), tangente_hipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8734ae",
   "metadata": {},
   "source": [
    "Voc√™ pode estar se perguntando agora o porque de guardarmos a opera√ß√£o no contexto e tamb√©m o que √© o `self.gradiente`, mas n√≥s vamos us√°-los para na pr√≥xima tarefa (retropropaga√ß√£o).\n",
    "\n",
    "Observe a cria√ß√£o de um novo valor abaixo (sem pais), para voc√™ que ainda vai se habituar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse c√≥digo aqui √© s√≥ para imprimir na tela üòú\n",
    "@patch_to(NoComp) # O patch_to altera a classe üòä\n",
    "def __str__(self: NoComp):\n",
    "    return f\"{{ {self.nome_visual} | valor {self.valor} }}\"\n",
    "\n",
    "@patch_to(NoComp) # O patch_to altera a classe üòä\n",
    "def __repr__(self: NoComp):\n",
    "    return f\"N√≥Computacional({str(self)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc0415",
   "metadata": {},
   "source": [
    "Cria um novo n√≥ computacional, veja como √© 'legal' o valor que ele imprime na c√©lula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "noh_teste = NoComp(12.19, nome_visual='üòá')\n",
    "noh_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857b17f",
   "metadata": {},
   "source": [
    "Ainda n√£o podemos fazer opera√ß√µes, voc√™ ainda vai implementar a seguida, mas podemos acessar os seus valores dessa forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noh_teste.valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52faeda",
   "metadata": {},
   "source": [
    "**Enunciado da Quest√£o**\n",
    "\n",
    "Gentilmente, o c√≥digo b√°sico de um n√≥ computacional (e de seu contexto) j√° foi cedido (poderia ter sido uma quest√£o por si s√≥).\n",
    "\n",
    "Voc√™ precisa apenas implementar as opera√ß√µes b√°sicas de adic√£o, de multiplica√ß√£o e de uma fun√ß√£o de ativa√ß√£o tangente hiperb√≥lica. Para a `tanh`, use a fun√ß√£o `np.tanh` do NumPy (n√£o use `math.`)\n",
    "\n",
    "**N√ÉO** use LLMs (ChatGPT).  **N√ÉO** pesquise a resposta na internet (**N√ÉO** copie o c√≥digo de reposit√≥rios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documenta√ß√£o do NumPy (mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "A resposta √© simplesmente uma linha: crie um novo n√≥, com o valor correto (√© a soma ou a multiplica√ß√£o ou o np.tanh dos valores dos pais) e com o contexto correto (o ctx que j√° foi passado como argument).\n",
    "\n",
    "Por enquanto estamos trabalhando apenas com escalares, mas utilize fun√ß√µes vetoriz√°veis `np.tanh` e as opera√ß√µes de `+` e `*` que s√£o vetoriz√°veis, para ficar mais f√°cil as outras quest√µes.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf1f06",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "823c133a2510b461cccf4467ddba7657",
     "grade": false,
     "grade_id": "questao_computa_basico",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_computa_basico\n",
    "\n",
    "def adicao(ctx: Contexto, gradiente=None) -> NoComp:\n",
    "    \"\"\" Cria um novo n√≥ computacional cujo valor √© a soma dos valores de seus pais\n",
    "    \n",
    "    :param ctx: Contexto que cont√©m os dois pais que ir√£o gerar o novo n√≥.\n",
    "    :param gradiente: N√£o se preocupe com ela agora, √© sempre None (nesta primeira fase)\n",
    "    H√° apenas dois pais SEMPRE, n√£o precisa se preocupar.\n",
    "    \n",
    "    Retorna um novo n√≥ computacional, um objeto do tipo NoComp.\n",
    "    \"\"\"\n",
    "    if gradiente is None:\n",
    "        # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        retropropaga_adicao(ctx, gradiente)\n",
    "\n",
    "def multiplicacao(ctx: Contexto, gradiente=None) -> NoComp:\n",
    "    \"\"\" Igual a de cima, mas agora √© multiplica√ß√£o \"\"\"\n",
    "    if gradiente is None:\n",
    "        # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        retropropaga_multiplicacao(ctx, gradiente)\n",
    "\n",
    "def tangente_hipe(ctx: Contexto, gradiente=None) -> NoComp:\n",
    "    \"\"\" Cria um novo n√≥ computacional cujo valor √© a tangente hiperb√≥lica do valor do pai\n",
    "    \n",
    "    :param ctx: Contexto que cont√©m o √∫nico pai que ir√° gerar o novo n√≥.\n",
    "    H√° apenas UM pai SEMPRE, n√£o precisa se preocupar. Utilize np.tanh.\n",
    "    \n",
    "    Retorna um novo n√≥ computacional, um objeto do tipo NoComp.\n",
    "    \"\"\"\n",
    "    if gradiente is None:\n",
    "        # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        retropropaga_tangente_hipe(ctx, gradiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93060ecd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a877e7265cfbfb3d07ba694fd9f4bed",
     "grade": true,
     "grade_id": "testa_computa_basico",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_computa_basico\n",
    "\n",
    "# Coloquei muitos testes para voc√™s n√£o errarem...\n",
    "raiz1 = NoComp(12)\n",
    "raiz2 = NoComp(19)\n",
    "raiz3 = NoComp(25)\n",
    "ctx_testa_manual1 = Contexto((raiz1, raiz2), adicao)\n",
    "ctx_testa_manual2 = Contexto((raiz1, raiz2), multiplicacao)\n",
    "ctx_testa_manual3 = Contexto((raiz3,)      , tangente_hipe)\n",
    "assert type(adicao       (ctx_testa_manual1)) == NoComp\n",
    "assert type(multiplicacao(ctx_testa_manual2)) == NoComp\n",
    "assert type(tangente_hipe(ctx_testa_manual3)) == NoComp\n",
    "assert type(adicao       (ctx_testa_manual1).ctx) == Contexto\n",
    "assert type(multiplicacao(ctx_testa_manual2).ctx) == Contexto\n",
    "assert type(tangente_hipe(ctx_testa_manual3).ctx) == Contexto\n",
    "assert abs(adicao       (ctx_testa_manual1).valor -  31) < 1e-9\n",
    "assert abs(multiplicacao(ctx_testa_manual2).valor - 228) < 1e-9\n",
    "assert abs(tangente_hipe(ctx_testa_manual3).valor -   1) < 1e-9\n",
    "\n",
    "# Agora fica legal, observe, estamos montando o grafo:\n",
    "a = raiz1 + raiz2\n",
    "b = raiz2 * raiz3\n",
    "c = a + b * a + b * b\n",
    "d = c.tanh()\n",
    "e = NoComp(1.3).tanh()\n",
    "f = NoComp(1.3) + NoComp(4.1) * NoComp(-1.9) + NoComp(0).tanh()\n",
    "\n",
    "assert abs(a.valor -     31) < 1e-9\n",
    "assert abs(b.valor -    475) < 1e-9\n",
    "assert abs(c.valor - 240381) < 1e-9\n",
    "assert abs(d.valor -      1) < 1e-9\n",
    "assert abs(e.valor - 0.86172315931) < 1e-9\n",
    "assert a.ctx.pais[0] == raiz1\n",
    "assert a.ctx.pais[1] == raiz2\n",
    "assert len(a.ctx.pais) == 2\n",
    "assert b.ctx.pais[0] == raiz2\n",
    "assert b.ctx.pais[1] == raiz3\n",
    "assert len(b.ctx.pais) == 2\n",
    "assert d.ctx.pais[0] == c\n",
    "assert len(d.ctx.pais) == 1\n",
    "assert abs(f.valor + 6.49) < 1e-9\n",
    "\n",
    "# Deve abstrair os valores n-dimensionais:\n",
    "raizm = NoComp(np.array([[1, 34, 4], [-1, -3, 8]]))\n",
    "contasm = raizm + raizm * raizm.tanh()\n",
    "assert np.linalg.norm(contasm.valor - np.array((\n",
    "      [[ 1.7616e+00,  6.8000e+01,  7.9973e+00],\n",
    "       [-2.3841e-01, -1.4836e-02,  1.6000e+01]]))) < 1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4e5ef",
   "metadata": {},
   "source": [
    "Eu coloquei um zilh√£o de testes a√≠ em cima, por que se voc√™ errar essa quest√£o, voc√™ erra o lab inteiro. Tente entende o que cada teste desses est√° fazendo. Por exemplo, para comparar dois n√∫meros de pontos flutuantes n√≥s verificamos se a diferen√ßa entre eles est√° dentro de um delta.\n",
    "\n",
    "Bem, agora que voc√™ implementou as opera√ß√µes b√°sicas, voc√™ merece ver o grafo resultante.\n",
    "\n",
    "S√≥ vamos primeiro salvar o nome das vari√°veis que declaramos anteriormente como tamb√©m o valor da string nome_visual dos nossos N√≥s Computacionais. E tamb√©m vamos definir um nome mais sucinto para os nossos operatores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nome, obj in list(globals().items()):\n",
    "    if type(obj) == NoComp:\n",
    "        obj.nome_visual = nome\n",
    "\n",
    "nomes_op = {adicao: '+', multiplicacao: '*', tangente_hipe: 'tanh'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd0ed2",
   "metadata": {},
   "source": [
    "E agora visualizamos o grafo com a biblioteca graphviz. Salvamos essa fun√ß√£o como m√©todo da classe `NoComp`.\n",
    "\n",
    "Perceba como nossos n√≥s s√£o os ret√¢ngulos com os cantos arredondados, h√° n√≥s de contas intermedi√°rias, sem nomes, e as opera√ß√µes s√£o as arestas. Olhe por exemplo que no caso do valor `b`, h√° duas arestas saindo dele pr√≥prio para um mesmo n√≥ intermedi√°rio (o caso de `b*b`), √© muito importante ter cuidado nisso durante a implementa√ß√£o do backpropagation na pr√≥xima parte.\n",
    "\n",
    "Inclusive, esse c√≥digo do `desenha` j√° √© uma forma de percorrer o gr√°fico (recursivamente), mas que foi implementado em profundidade e n√£o em largura. Al√©m disso, ele n√£o repete elementos que j√° foram visitados (`ja_desenhados`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201820a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(NoComp)\n",
    "def desenha(self: NoComp, grafo_visual: Union[Digraph,None]=None, ja_desenhados: Union[Digraph, Set]=None):\n",
    "    ja_desenhados = ja_desenhados or set()\n",
    "    if grafo_visual is None:\n",
    "        grafo_visual = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "        grafo_visual.attr('node', shape='record', style='rounded')\n",
    "    if self in ja_desenhados:\n",
    "        return grafo_visual\n",
    "    grafo_visual.node(str(id(self)), str(self))\n",
    "    ja_desenhados.add(self)\n",
    "    if self.ctx is not None:\n",
    "        for pai in self.ctx.pais:\n",
    "            grafo_visual.edge(str(id(pai)), str(id(self)), nomes_op[self.ctx.operacao])\n",
    "            pai.desenha(grafo_visual, ja_desenhados)\n",
    "    return grafo_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c3bd2",
   "metadata": {},
   "source": [
    "**BIZU PARA A PR√ìXIMA QUEST√ÉO**\n",
    "\n",
    "Vou deixar aqui essa outra implementa√ß√£o que percorre o grafo em largura, como bizu para a pr√≥xima quest√£o (para facilitar a vida). Observe que a recurs√£o aqui perde um pouco o 'f√¥lego' e √© substitu√≠da mais claramento por um loop. A intui√ß√£o para isso √© que agora temos mais uma fila (√© uma lista em Python) `a_desenhar` que representa a ordem dos pr√≥ximos n√≥s a serem visitados. Perceba que novamente n√£o desenhamos n√≥s que j√° foram desenhados (a implementa√ß√£o mais eficiente disso seria com uma flag, mas usamos um `set()` em Python que √© mais lento mas fica mais f√°cil para n√£o precisar acrescentar mais atributos ao n√≥)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eeea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(NoComp)\n",
    "def desenha(self: NoComp, grafo_visual:Union[Digraph,None]=None):\n",
    "    if grafo_visual is None:\n",
    "        grafo_visual = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "        grafo_visual.attr('node', shape='record', style='rounded')\n",
    "    grafo_visual.node(str(id(self)), str(self))\n",
    "    ja_desenhados = set()\n",
    "    a_desenhar = [self]\n",
    "    while len(a_desenhar) > 0:\n",
    "        atual = a_desenhar.pop(0)\n",
    "        if atual in ja_desenhados:\n",
    "            continue\n",
    "        if atual.ctx is not None:\n",
    "            a_desenhar.extend(atual.ctx.pais)\n",
    "            for pai in atual.ctx.pais:\n",
    "                grafo_visual.edge(str(id(pai)), str(id(atual)), nomes_op[atual.ctx.operacao])\n",
    "        grafo_visual.node(str(id(atual)), str(atual))\n",
    "        ja_desenhados.add(atual)\n",
    "    return grafo_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "contasm.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb3345",
   "metadata": {},
   "source": [
    "## Retropropaga√ß√£o B√°sica (3 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e5620",
   "metadata": {},
   "source": [
    "**Explica√ß√£o sobre o assunto**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0477e",
   "metadata": {},
   "source": [
    "Temos que inicializar o nosso atributo dos gradientes (inicialmente estava `None`, mas depois j√° copiei pro init). Vamos fazer logo recursivamente, para inicializarmos todos os elementos da rede (se come√ßarmos pelo elemento mais 'longe', 'mais no fundo', a loss). Ela j√° est√° vector-friendly utilizando o `np.zeros_like` do NumPy.\n",
    "\n",
    "Observe que essa fun√ß√£o `inicializa_gradiente` percorre o grafo em profundidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(NoComp)\n",
    "def inicializa_gradiente(self: NoComp):\n",
    "    self.gradiente = np.zeros_like(self.valor) if type(self.valor) == np.ndarray else 0\n",
    "    if self.ctx is None:\n",
    "        return\n",
    "    for pai in self.ctx.pais:\n",
    "        if pai.gradiente is None or np.any(pai.gradiente != 0):\n",
    "            pai.inicializa_gradiente()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0cdc5b",
   "metadata": {},
   "source": [
    "**Enunciado da Quest√£o**\n",
    "\n",
    "Implemente as fun√ß√µes `retropropaga_multiplicacao` e `retropropaga_tangente_hipe` com base no exemplo do `retropropaga_adicao`. Essas fun√ß√µes s√£o chamadas durante a retropropaga√ß√£o e elas acumulam o gradiente que o n√≥ filho tem e passa para os seus pais. Para isso voc√™ precisa utilizar a regra da cadeia e saber calcular a *derivada* da opera√ß√£o que voc√™s fizeram. Essa retropropaga√ß√£o dessas fun√ß√µes √© **LOCAL**, isto √©, ela opera apenas nos n√≥s imediatos.\n",
    "\n",
    "Para fazer a retropropaga√ß√£o **GLOBAL**, implemente a fun√ß√£o `retropropaga` (que vai ser usada como m√©todo da classe NoComp)\n",
    "\n",
    "Aten√ß√£o, voc√™ deve percorrer o grafo em largura (e n√£o em profundidade), pois temos que garantir que todos os gradientes de um n√≥ j√° foram completamente acumulados, antes de que ele possa retropropagar para seus pais.\n",
    "\n",
    "![Anima√ß√£o de Percorrer o Grafo por Largura](https://upload.wikimedia.org/wikipedia/commons/4/46/Animated_BFS.gif)\n",
    "\n",
    "Atente-se para o fato que os gradientes devem se acumular os gradientes (e n√£o sobreescrev√™-los, lembre-se de `b*b` l√° de cima (se somar as derivadas de cada um d√° $b + b = 2 b$ que √© o valor esperado da retropropaga√ß√£o.\n",
    "\n",
    "Se tiver d√∫vidas sobre BFS, pode se sentir a vontade para pesquisar outras aulas / materiais (mas n√£o a resposta pronta). Veja por exemplo do [Nipun Ramakrishnan no youtube](https://www.youtube.com/watch?v=xlVX7dXLS64)\n",
    "\n",
    "**N√ÉO** use LLMs (ChatGPT).  **N√ÉO** pesquise a resposta pronta na internet (**N√ÉO** copie o c√≥digo de reposit√≥rios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documenta√ß√£o do NumPy (mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Para as quest√µes do `retropropaga_multiplicacao` e do `retropropaga_tangente_hipe` basta modificar o gradiente dos seus pais de acordo com a regra da cadeia e a derivada anal√≠tica da fun√ß√£o. N√£o se esque√ßa que os gradientes devem ser acumulados, e n√£o sobreescritos.\n",
    "    \n",
    "A derivada da multiplica√ß√£o √© o outro elemento que est√° multiplicando.\n",
    "\n",
    "A derivada de tanh(x) √© (1 - tanh(x)**2), use np.tanh e **2 para x¬≤.\n",
    "\n",
    "Para a quest√£o do `retropropaga` olhe a fun√ß√£o do desenha que implementei duas vezes justamente para voc√™ ficar de bizu (a √∫ltima implementa√ß√£o √© por largura), basta voc√™ verificar que as opera√ß√µes s√£o exatamente as mesmas (s√≥ cuidado para n√£o deixar o loop mais interno, porque o retropropaga_adicao/multiplicacao/tangente_hipe j√° retropropaga um n√≠vel para todos os pais.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89485e84",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce678f3237fee47696962a9cf4808584",
     "grade": false,
     "grade_id": "questao_retropropaga_basico",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_retropropaga_basico\n",
    "\n",
    "def retropropaga_adicao(ctx: Contexto, gradiente: Union[float,np.ndarray]):\n",
    "    \"\"\" Esse eu deixei de DICA, para dar uma dica ainda maior do que fazer \n",
    "        Perceba que o valor da derivade de c = a + b √© 1 para dc/db e dc/da\"\"\"\n",
    "    ctx.pais[0].gradiente += gradiente * 1\n",
    "    ctx.pais[1].gradiente += gradiente * 1\n",
    "\n",
    "def retropropaga_multiplicacao(ctx: Contexto, gradiente: Union[float,np.ndarray]):\n",
    "    \"\"\" Retropropaga o gradiente localmente, da multiplica√ß√£o, apenas para os pais imediatos \n",
    "    \n",
    "    :param ctx: Contexto do n√≥ atual que est√° retropropagando para os n√≥s que o gerou (seus pais)\n",
    "    :param gradiente: Valor float ou matriz numpy que representa o gradiente que o n√≥ atual tem\n",
    "    \n",
    "    Essa fun√ß√£o n√£o retorna nenhuma valor, mas modifica os valores dos gradientes dos ctx.pais.\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def retropropaga_tangente_hipe(ctx: Contexto, gradiente: Union[float,np.ndarray]):\n",
    "    \"\"\" Retropropaga o gradiente localmente, da tangente hiperb√≥lica, apenas para o seu pai imediato\n",
    "    \n",
    "    :param ctx: Contexto do n√≥ atual que est√° retropropagando para o n√≥ que o gerou (seu pai)\n",
    "    :param gradiente: Valor float ou matriz numpy que representa o gradiente que o n√≥ atual tem\n",
    "    \n",
    "    Essa fun√ß√£o n√£o retorna nenhuma valor, mas modifica o valor do gradiente do ctx.pais[0].\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "@patch_to(NoComp)\n",
    "def retropropaga(self: NoComp, gradiente: Union[float, np.ndarray] = None):\n",
    "    \"\"\" Retropropaga o gradiente globalmente, a todos os seus antepassados at√© as ra√≠zes.\n",
    "    \n",
    "    :param self: Inst√¢ncia de NoComp\n",
    "    :param gradiente: Float ou matrix numpy que representa o gradiente a ser retropropagado,\n",
    "                      se n√£o for passado, √© assumido como valor unit√°rio (ou matriz)\n",
    "    \n",
    "    Essa fun√ß√£o n√£o retorna nenhuma valor, mas modifica os valores dos gradientes de todos os n√≥s antecessores.\n",
    "    \"\"\"\n",
    "    self.gradiente = gradiente or np.ones_like(self.valor) if type(self.valor) == np.ndarray else 1\n",
    "    if gradiente is None and np.size(self.gradiente) > 1:\n",
    "        print('ATEN√á√ÉO, essa fun√ß√£o apenas calcula os gradientes, precisamos de um escalar, (o jacobiano √© interno), assumimos pesos unit√°rios para as dimens√µes')\n",
    "    ja_retropropagados = set()\n",
    "    a_retropropagar = [self]\n",
    "    while len(a_retropropagar) > 0:\n",
    "        # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19318b7e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b66cf41f902c94db0080feac0cb45848",
     "grade": true,
     "grade_id": "testa_retropropaga_basico",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_retropropaga_basico\n",
    "\n",
    "m = NoComp(0.4, nome_visual='m')\n",
    "b = NoComp(-0.1, nome_visual='b')\n",
    "x = NoComp(8.1, nome_visual='x')\n",
    "y = m * x + b; y.nome_visual = 'y'\n",
    "y.inicializa_gradiente()\n",
    "y.retropropaga()\n",
    "assert abs(b.gradiente - 1  ) < 1e-9\n",
    "assert abs(x.gradiente - 0.4) < 1e-9\n",
    "assert abs(m.gradiente - 8.1) < 1e-9\n",
    "\n",
    "def testa_func(dw01=0, dw02=0, dw11=0, dw22=0, dx2=0):\n",
    "    x0, x1, x2, x3 = NoComp(1), NoComp(4.1), NoComp(-0.3+dx2), NoComp(2.1)\n",
    "    w01, w11, w21, w31 = NoComp(0.1+dw01), NoComp(-0.3+dw11), NoComp(-1), NoComp(0.9)\n",
    "    w02, w12, w22, w32 = NoComp(-0.3+dw02), NoComp(0.5), NoComp(-0.2+dw22), NoComp(0.4)\n",
    "    z1 = x0*w01 + x1*w11 + x2*w21 + x3*w31\n",
    "    z2 = x0*w02 + x1*w12 + x2*w22 + x3*w32\n",
    "    a0, a1, a2 = NoComp(1), z1.tanh(), z2.tanh()\n",
    "    wf0, wf1, wf2 =  NoComp(0.2), NoComp(-0.8), NoComp(0.4)\n",
    "    zf = a0*wf0 + a1*wf1 + a2*wf2\n",
    "    yf = zf.tanh()\n",
    "    for nome, obj in list(locals().items()):\n",
    "        if type(obj) == NoComp:\n",
    "            obj.nome_visual = nome\n",
    "    return yf, (w01, w02, w11, w22, x2)\n",
    "\n",
    "yf0,_ = testa_func()\n",
    "delta = 1e-9\n",
    "yfdw01, (dw01, _, _, _, _) = testa_func(dw01=delta)\n",
    "yfdw01.inicializa_gradiente()\n",
    "yfdw01.retropropaga()\n",
    "dyfdw01 = (yfdw01.valor - yf0.valor)/delta\n",
    "assert abs(dyfdw01 - dw01.gradiente) < 1e-6\n",
    "yfdx2, (_, _, _, _, dx2) = testa_func(dx2=delta)\n",
    "yfdx2.inicializa_gradiente()\n",
    "yfdx2.retropropaga()\n",
    "dyfdx2 = (yfdx2.valor - yf0.valor)/delta\n",
    "assert abs(dyfdx2 - dx2.gradiente) < 1e-6\n",
    "\n",
    "# Deve abstrair os valores n-dimensionais:\n",
    "raizm = NoComp(np.array([[0.6, -1.8, -0.5], [-0.2, 0.4, 1.2]]))\n",
    "contasm = raizm + raizm * raizm.tanh()\n",
    "contasm.inicializa_gradiente()\n",
    "contasm.retropropaga()\n",
    "assert np.linalg.norm(raizm.gradiente - np.array([\n",
    "    [ 1.96399622455, -0.13321108611,  0.14465897626],\n",
    "    [ 0.61041608318,  1.72220447669,  2.19967860246]])) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab561e3",
   "metadata": {},
   "source": [
    "Como vamos calcular os gradientes, seria legal se pud√©ssemos ver o seu valor no grafo tamb√©m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limita_str(valor):\n",
    "    string = str(valor)\n",
    "    return string if len(string) < 30 else valor.shape\n",
    "\n",
    "@patch_to(NoComp) # Vamos mostrar os gradientes agora tamb√©m\n",
    "def __str__(self: NoComp):\n",
    "    return f\"{{ {self.nome_visual} | valor {limita_str(self.valor)} | grad {limita_str(self.gradiente)}}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14177b81",
   "metadata": {},
   "source": [
    "Se estiver com dificuldades, observe os grafos abaixo, principalmente o primeiro mais simples do $y=m x + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb59fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6f84d",
   "metadata": {},
   "source": [
    "E agora o grafo maior que √© quase como uma implementa√ß√£o de uma pequena rede neural, sendo que cada n√≥ √© um neur√¥nio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762de501",
   "metadata": {},
   "outputs": [],
   "source": [
    "yfdx2.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258895d",
   "metadata": {},
   "source": [
    "Se voc√™ n√£o ficou emocionado com esse grafo, voc√™ n√£o tem cora√ß√£o...\n",
    "\n",
    "Tente brincar com ele, mude os valores (recalcule) e repropague e veja o que acontece com os gradientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e740b56",
   "metadata": {},
   "source": [
    "## Opera√ß√µes Vetorizadas (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae7b11",
   "metadata": {},
   "source": [
    "**Explica√ß√£o sobre o assunto**\n",
    "\n",
    "Tratar cada n√≥ como um neur√¥nio separado √© muito ineficiente. Podemos fazer toda essa opera√ß√£o de combina√ß√£o linear (multiplica√ß√£o pelos pesos e soma) por uma opera√ß√£o de multiplica√ß√£o matricial.\n",
    "\n",
    "Assim, retornamos ao mesmo paradigma da primeira quest√£o e da quest√£o anterior, mas agora estamos trabalhando apenas com valores que s√£o matrizes. Inclusive, se voc√™ tiver implementado agnosticamente do tipo do valor, as quest√µes anteriores j√° funcionam com matrizes, sem que voc√™ tenha se preocupado com isso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef04da",
   "metadata": {},
   "source": [
    "**Enunciado da Quest√£o**\n",
    "\n",
    "Implemente a fun√ß√£o abaixo, `multi_matrici`, no mesmo estilo das fun√ß√µes da primeira quest√£o (`adicao`, `multiplicacao`, `tangente_hipe`) mas agora os valores dos pais s√£o garantidamente matrizes numpy compat√≠veis.\n",
    "\n",
    "\n",
    "**N√ÉO** use LLMs (ChatGPT).  **N√ÉO** pesquise a resposta na internet (**N√ÉO** copie o c√≥digo de reposit√≥rios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documenta√ß√£o do NumPy (mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Observe o que j√° foi implementado nas quest√µes anteriores.\n",
    "\n",
    "Verifique se √© propaga√ß√£o direta (gradiente is None) ou se √© retropropagacao.\n",
    "Garantidamente ctx.pais s√£o vetores numpy cujas as dimens√µes est√£o corretas.\n",
    "H√° apenas DOIS pais, n√£o precisa se preocupar. Utilize @ para operar sobre matrizes numpy.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2022928",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76f9b40f260bfa22551facda72cb06ef",
     "grade": false,
     "grade_id": "questao_vetoriza",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_vetoriza\n",
    "\n",
    "def multi_matrici(ctx: Contexto, gradiente: np.ndarray = None) -> Union[NoComp, None]:\n",
    "    \"\"\" Cria um novo n√≥ computacional cujo valor √© a multiplica√ß√£o matricial entre os pais. (a ordem importa)\n",
    "    \n",
    "    :param ctx: Contexto que cont√©m os dois pais que ir√£o gerar o novo n√≥.\n",
    "    \n",
    "    Verifique se √© propaga√ß√£o direta (gradiente is None) ou se √© retropropagacao.\n",
    "    Garantidamente ctx.pais s√£o vetores numpy cujas as dimens√µes est√£o corretas.\n",
    "    H√° apenas DOIS pais, n√£o precisa se preocupar. Utilize @ para operar sobre matrizes numpy.\n",
    "    \n",
    "    Retorna um novo n√≥ computacional resultad da multiplica√ß√£o matricial\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f18d19",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d0d2ffe4198945fc5ab531f6420165a",
     "grade": true,
     "grade_id": "testa_vetoriza",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_vetoriza\n",
    "\n",
    "X = NoComp(np.array([[ 1  ,  4.1, -0.3, 2.1]]), nome_visual='X')\n",
    "W = NoComp(np.array([[ 0.1, -0.3],\n",
    "                     [-0.3,  0.5],\n",
    "                     [-1  , -0.2],\n",
    "                     [ 0.9,  0.4]]), nome_visual='W')\n",
    "Z = X @ W; Z.nome_visual='Z'\n",
    "A = Z.tanh(); A.nome_visual='A'\n",
    "Wf = NoComp(np.array([[-0.8],[0.4]])); Wf.nome_visual='Wf'\n",
    "Zf = A @ Wf; Zf.nome_visual = 'Zf'\n",
    "Yf = Zf.tanh(); Yf.nome_visual='Yf'\n",
    "\n",
    "Yf.inicializa_gradiente()\n",
    "Yf.retropropaga()\n",
    "assert np.linalg.norm(X.gradiente - np.array([\n",
    "  [-0.0312697368 ,  0.09081125174,  0.28871369872, -0.25819345159]])) < 1e-6\n",
    "\n",
    "L = Yf @ Yf @ Yf\n",
    "L.inicializa_gradiente()\n",
    "L.retropropaga()\n",
    "assert np.linalg.norm(X.gradiente - np.array([\n",
    "  [-0.00489385712,  0.0142123771 ,  0.04518501706, -0.04040845851]])) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f0f47",
   "metadata": {},
   "source": [
    "Vamos atualizar a nossa vari√°vel global com os nomes das nossas opera√ß√µes, `nomes_op`, para acrescentar a nossa opera√ß√£o de `@` e imprimir o nosso grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_op[multi_matrici] = '@'\n",
    "\n",
    "Yf.inicializa_gradiente()\n",
    "Yf.retropropaga()\n",
    "\n",
    "Yf.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976549a",
   "metadata": {},
   "source": [
    "## Rede Neural (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ccd40",
   "metadata": {},
   "source": [
    "**Explica√ß√£o sobre o assunto**\n",
    "\n",
    "Agora temos todas as ferramentas a nossa disposi√ß√£o para constru√≠rmos a nossa pr√≥pria rede neural. Veja o exemplo da classe abaixo que recebe uma tupla de inteiros que indica quantos neur√¥nios temos por camada.\n",
    "\n",
    "Ela guarda os par√¢metros que s√£o trein√°veis, s√£o os n√≥s ra√≠zes (folhas da retropropaga√ß√£o), pois se o X mudar eles n√£o mudam (enquanto as ativa√ß√µes s√£o todas recalculadas).\n",
    "\n",
    "Olha o legal da classe abaixo: quando voc√™ invoca um objeto que √© inst√¢ncia dela (fun√ß√£o `__call__`) ela te retorna um novo n√≥ computacional. √â por isso que os frameworks de deep learning tem os 'par√™nteses duplos', tipo torch.ReLU()(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedeNeuralSequencial:\n",
    "    def __init__(self, neuronios_por_camada: Tuple[int]):\n",
    "        self.pesos_sinapticos, self.bias = constroi_rede_neural_sequencial(neuronios_por_camada)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        ativacao = x\n",
    "        for i, (peso, b) in enumerate(zip(self.pesos_sinapticos, self.bias)):\n",
    "            combinacao_linear = (ativacao @ peso) + b\n",
    "            ativacao = combinacao_linear.tanh()\n",
    "            combinacao_linear.nome_visual = f\"Z^({i+1})\"\n",
    "            ativacao.nome_visual = f\"A^({i+1})\"\n",
    "        return ativacao\n",
    "    \n",
    "    def parametros(self):\n",
    "        return self.pesos_sinapticos + self.bias\n",
    "    \n",
    "    def zera_gradientes_parametros(self):\n",
    "        for parametro in self.parametros():\n",
    "            parametro.gradiente.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084f004",
   "metadata": {},
   "source": [
    "Para inicializar os pesos da nossa rede aleatoriamente, j√° implementamos a fun√ß√£o abaixo que amostra de uma distribui√ß√£o gaussiana com vari√¢ncia escalada pela quantidade de neur√¥nios da camada anterior, isto √©, a quantidade de entradas a um neur√¥nio, o seu $fan_{in}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04717980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valor_inicial(n_linhas:int, n_colunas:int) -> np.ndarray:\n",
    "    \"\"\" Retorna uma matriz numpy com (n_linhas, n_colunas) inicializada aleatoriamente\"\"\"\n",
    "    shape = (n_linhas, n_colunas)\n",
    "    if n_linhas == 1:\n",
    "        return np.zeros(shape, dtype=np.float64)\n",
    "    return np.random.normal(scale=1/np.sqrt(n_linhas), size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67610ef",
   "metadata": {},
   "source": [
    "Al√©m disso, como estamos usando um termo de bias agora (pois n√£o implementamos um n√≥ de concatena√ß√£o), e pelo fato do NumPy j√° fazer automaticamente o broadcasting, temos que consertar a nossa fun√ß√£o de retropropaga√ß√£o para poder fazer o inverso do broadcasting, isto √©, se o broadcasting repete as dimens√µes, n√≥s temos que reduzi-las (pela soma). S√≥ fizemos isso para a soma e s√≥ pensando no caso $(N, M)$, mas se fosse um framework geral, ter√≠amos que pensar tamb√©m nas outras opera√ß√µes e em dimens√µes maiores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retropropaga_adicao(ctx: Contexto, gradiente):\n",
    "    for pai in ctx.pais: # Para resolver o broadcasting do bias\n",
    "        if type(gradiente)==np.ndarray and gradiente.shape[0] > pai.gradiente.shape[0] and pai.gradiente.shape[0] == 1:\n",
    "            pai.gradiente += gradiente.sum(axis=0)[None]\n",
    "        else:\n",
    "            pai.gradiente += gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1697915",
   "metadata": {},
   "source": [
    "**Enunciado da Quest√£o**\n",
    "\n",
    "Implemente a fun√ß√£o `constroi_rede_neural_sequencial`.\n",
    "\n",
    "**N√ÉO** use LLMs (ChatGPT).  **N√ÉO** pesquise a resposta na internet (**N√ÉO** copie o c√≥digo de reposit√≥rios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documenta√ß√£o do NumPy (mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Basta usar a fun√ß√£o `.append`, criando novos Nos Computacionais com as dimens√µes adequadas.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e793e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b5d25b3515b98a79770ced801a8049f",
     "grade": false,
     "grade_id": "questao_rede_neural",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_rede_neural\n",
    "\n",
    "\n",
    "def constroi_rede_neural_sequencial(neuronios_camada: Tuple[int]) -> Tuple[List[NoComp], List[NoComp]]:\n",
    "    \"\"\" Fun√ß√£o que constr√≥i os par√¢metros de uma rede neural sequencial\n",
    "    \n",
    "    :param neuronios_camada: Tupla de inteiros que indica a quantidade de neur√¥nios por camada\n",
    "    \n",
    "    Utilize a fun√ß√£o valor_inicial para obter uma matriz numpy com valores iniciais de cada camada.\n",
    "    \n",
    "    Retorna uma lista de n√≥s que representam o peso sin√°ptico de cada camada e outra lista de n√≥s para os bias\n",
    "    \"\"\"\n",
    "    pesos_sinapticos: List[NoComp] = []\n",
    "    bias: List[NoComp] = []\n",
    "      \n",
    "    for qtd_anterior, qtd_posterior in zip(neuronios_camada, neuronios_camada[1:]):\n",
    "        # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    for i, (peso, b) in enumerate(zip(pesos_sinapticos, bias)):\n",
    "        peso.nome_visual = f\"W^({i+1})\"\n",
    "        b.nome_visual = f\"B^({i+1})\"\n",
    "    \n",
    "    return pesos_sinapticos, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00ae13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32b44474e7f67e0995c30c4276b49694",
     "grade": true,
     "grade_id": "testa_rede_neural",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_rede_neural\n",
    "\n",
    "np.random.seed(42)\n",
    "pesos_sinapticos, bias = constroi_rede_neural_sequencial((2, 32, 64, 32, 1))\n",
    "assert len(pesos_sinapticos) == 4\n",
    "assert len(bias) == 4\n",
    "assert pesos_sinapticos[0].valor.shape == (2, 32)\n",
    "assert pesos_sinapticos[2].valor.shape == (64,32)\n",
    "assert pesos_sinapticos[3].valor.shape == (32, 1)\n",
    "assert bias[1].valor.shape == (1, 64)\n",
    "np.random.seed(42)\n",
    "w1v = valor_inicial(2, 32)\n",
    "assert np.all(w1v == pesos_sinapticos[0].valor)\n",
    "\n",
    "\n",
    "modelo = RedeNeuralSequencial((2, 32, 64, 32, 1))\n",
    "X = NoComp(np.random.uniform(size=(19, 2)), nome_visual='X')\n",
    "estimativa = modelo(X); estimativa.nome_visual='≈∂'\n",
    "assert estimativa.valor.shape == (19, 1)\n",
    "estimativa.retropropaga() # Isso imprime a mesagem de aten√ß√£o, pois n√£o √© um escalar\n",
    "\n",
    "um_res = RedeNeuralSequencial((2, 1))(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150247b",
   "metadata": {},
   "source": [
    "Observe como fica a nossa rede neural sequencial. Atente para o seu grafo e os nomes dos par√¢metros, sobretudo os n√≥s que n√£o tem pai (eles s√£o os par√¢metros que iremos otimizar, a menos do X que √© valor de entrada).\n",
    "\n",
    "√â claro que se um modelo puramente sequencial √© um grafo muito simplificado (uma 'tripa')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17962df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimativa.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96053ca2",
   "metadata": {},
   "source": [
    "## Gradiente Descendente e Otimiza√ß√£o (1 ponto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bf04f",
   "metadata": {},
   "source": [
    "**Explica√ß√£o sobre o assunto**\n",
    "\n",
    "Agora j√° temos as nossas ferramentas e o nosso modelo (a faca e o bolo), devemos trein√°-lo, isto √©, aproveit√°-lo.\n",
    "\n",
    "Para isso vamos definir uma loss, no caso a √∫nica que d√° para fazer com as opera√ß√µes que temos √© a MSE.\n",
    "\n",
    "Al√©m disso para a fun√ß√£o custo, precisamos somar em todas as dimens√µes, por isso, implementamos ainda a fun√ß√£o soma abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28295133",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(NoComp)\n",
    "def soma(self: NoComp):\n",
    "    return soma_total(Contexto((self,), soma_total))\n",
    "\n",
    "def soma_total(ctx, gradiente=None):\n",
    "    if gradiente is None:\n",
    "        return NoComp(np.sum(ctx.pais[0].valor), ctx)\n",
    "    ctx.pais[0].gradiente += gradiente * np.ones_like(ctx.pais[0])\n",
    "\n",
    "nomes_op = {adicao: '+', multiplicacao: '*', multi_matrici: '@', tangente_hipe: 'tanh', soma_total:'soma'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce417f1",
   "metadata": {},
   "source": [
    "**Enunciado da Quest√£o**\n",
    "\n",
    "Implemente a fun√ß√£o `gradiente_descendente`. Observe a fun√ß√£o de teste e veja como a loss e o custo foram implementados.\n",
    "\n",
    "\n",
    "**N√ÉO** use LLMs (ChatGPT).  **N√ÉO** pesquise a resposta na internet (**N√ÉO** copie o c√≥digo de reposit√≥rios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documenta√ß√£o do NumPy (mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Basta implementar a equa√ß√£o de atualiza√ß√£o dos pesos que o gradiente faz $\\theta_{atualizado} = \\theta_{antigo} - \\alpha \\nabla_w\\theta$\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216df3f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06d060d100cd70983c3e96fee074b997",
     "grade": false,
     "grade_id": "questao_gradiente",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_gradiente\n",
    "\n",
    "def gradiente_descendente(parametros: List[NoComp], gradiente: List[np.ndarray], alpha:float) -> None:\n",
    "    \"\"\"Implementa apenas um passo do algoritmo de otimiza√ß√£o do gradiente descendente.\n",
    "    \n",
    "    :param parametros: Uma lista de n√≥s computacionais que representa os par√¢metros a serem atualizados\n",
    "    :param gradiente: Uma lista de matrizes numpy (ou floats) que representam os gradientes que calculamos (pela retropropaga√ß√£o)\n",
    "    \n",
    "    Essa fun√ß√£o n√£o retorna nenhuma valor\n",
    "    \"\"\"\n",
    "    for p, grad in zip(parametros, gradiente):\n",
    "        # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f157b0e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48214772224196957301c3c33ef1569b",
     "grade": true,
     "grade_id": "testa_gradiente",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_gradiente\n",
    "\n",
    "np.random.seed(42)\n",
    "modelo = RedeNeuralSequencial((2, 32, 64, 32, 1))\n",
    "X = NoComp(matriz_X, nome_visual='X')\n",
    "Y = NoComp(matriz_Y, nome_visual='y')\n",
    "\n",
    "def uma_epoca(rede, x, y, alpha=0.001):\n",
    "    ≈∑ = rede(x); ≈∑.nome_visual = '≈∂'\n",
    "    \n",
    "    # Como n√≥s n√£o temos opera√ß√£o de subtra√ß√£o, tive que multiplicar por -1, kkkkk\n",
    "    loss = ((≈∑ + NoComp(-1) * y) * (≈∑ + NoComp(-1) * y)); loss.nome_visual = 'ùìõ'\n",
    "    custo = loss.soma(); custo.nome_visual = 'ùìí'\n",
    "    \n",
    "    rede.zera_gradientes_parametros()\n",
    "    custo.retropropaga()\n",
    "    \n",
    "    params = rede.parametros()\n",
    "    gradiente_descendente(params, [p.gradiente for p in params], alpha)\n",
    "    return custo, ≈∑\n",
    "\n",
    "custo1, ≈∑1 = uma_epoca(modelo, X, Y)\n",
    "epocas = 100\n",
    "custos = []\n",
    "for i in range(epocas):\n",
    "  custo2, ≈∑2 = uma_epoca(modelo, X, Y, alpha=0.0020-0.0015*i/epocas)\n",
    "  custos.append(custo2.valor)\n",
    "assert np.all(custo2.valor < custo1.valor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6330ea",
   "metadata": {},
   "source": [
    "Estude com aten√ß√£o a fun√ß√£o `uma_epoca`, entenda como ela calcula a loss, depois o custo, depois zera os gradientes dos par√¢metros, faz e retropropaga√ß√£o, e por √∫ltimo faz o gradiente descentende (atualiza√ß√£o dos par√¢metros).\n",
    "\n",
    "Ela era uma fun√ß√£o que voc√™s deveriam implementar, mas como j√° estava estourando o tempo do lab, pe√ßo que voc√™ entendam ela, principalmente o porque de zerar os gradientes dos par√¢metros entre uma √©poca e outra (se n√£o estar√≠amos fazendo uma soma dos gradientes ao longo das √©pocas).\n",
    "\n",
    "√â exatamente esse mesmo loop que iremos implementar no pr√≥ximo Laborat√≥rio em PyTorch. Voc√™ entender√£o o PyTorch muito mais rapidamente (√© a mesma intui√ß√£o do nanoGrad, s√≥ que mais bodoso).\n",
    "\n",
    "Observe como o custo diminuiu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "custo1, custo2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac41936",
   "metadata": {},
   "source": [
    "Veja tamb√©m que o custo √© um escalar (por isso n√£o tivemos a chamada de aten√ß√£o do retropropaga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb09824",
   "metadata": {},
   "outputs": [],
   "source": [
    "custo1.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927776b9",
   "metadata": {},
   "source": [
    "Veja como o nosso custo evolui ao longo das √©pocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([custo1.valor] + custos)\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('Custo do Treinamento (MSE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7196505",
   "metadata": {},
   "source": [
    "Perceba que apesar de dos dados Y serem 0 ou 1 (indicarem uma categoria) n√≥s utilizamos uma fun√ß√£o custo MSE s√≥ porque n√£o implementamos nem a fun√ß√£o log, nem a exponencial, para podermos calcular a entropia bin√°ria. Mas isso na teoria √© 'feio', apesar de 'funcionar'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fdcf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-1.5, 2.5),\n",
    "                     np.linspace(-1, 1.5))\n",
    "x_amostra = np.concatenate([xx.reshape(-1,1), yy.reshape(-1,1)], axis=1)\n",
    "\n",
    "\n",
    "≈∑ = modelo(NoComp(x_amostra))\n",
    "\n",
    "\n",
    "plt.contourf(xx, yy, np.clip(≈∑.valor.reshape(xx.shape),0,1), levels=50, cmap='plasma')\n",
    "plt.colorbar()\n",
    "plt.scatter(matriz_X[matriz_Y[:,0] == 0][:, 0], matriz_X[matriz_Y[:,0] == 0][:, 1], color='cyan', marker='^', label='$y = 0$')\n",
    "plt.scatter(matriz_X[matriz_Y[:,0] == 1][:, 0], matriz_X[matriz_Y[:,0] == 1][:, 1], color='crimson', marker='o', label='$y = 1$')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.legend(labelcolor='linecolor')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d724e1e",
   "metadata": {},
   "source": [
    "Veja como ficar√≠a se n√£o tiv√©ssemos treinado o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cfe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-1.5, 2.5),\n",
    "                     np.linspace(-1, 1.5))\n",
    "x_amostra = np.concatenate([xx.reshape(-1,1), yy.reshape(-1,1)], axis=1)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "modelo_sem_treinar = RedeNeuralSequencial((2, 32, 64, 32, 1))\n",
    "≈∑_sem_treinar = modelo_sem_treinar(NoComp(x_amostra))\n",
    "\n",
    "\n",
    "plt.contourf(xx, yy, np.clip(≈∑_sem_treinar.valor.reshape(xx.shape),0,1), levels=50, cmap='plasma')\n",
    "plt.colorbar()\n",
    "plt.scatter(matriz_X[matriz_Y[:,0] == 0][:, 0], matriz_X[matriz_Y[:,0] == 0][:, 1], color='cyan', marker='^', label='$y = 0$')\n",
    "plt.scatter(matriz_X[matriz_Y[:,0] == 1][:, 0], matriz_X[matriz_Y[:,0] == 1][:, 1], color='crimson', marker='o', label='$y = 1$')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.legend(labelcolor='linecolor')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78564c",
   "metadata": {},
   "source": [
    "Espero que voc√™ olhem para o grafo computacional de cima e sintam orgulho do trabalho que voc√™s fizeram nesse lab. Isso √© a base dos frameworks de redes neurais PyTorch, TensorFlow, JAX e hoje voc√™s conseguiram implementar a sua ess√™ncia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9fb06b",
   "metadata": {},
   "source": [
    "# Seus dados e feedback aqui:\n",
    "\n",
    "Coloque o seu feedback sobre o lab aqui para podermos melhor√°-lo para as pr√≥ximas turmas e tamb√©m 'calibrar' os pr√≥ximos labs (idealmente os 80% dos alunos terminar em bem menos de 3h)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48adc30",
   "metadata": {},
   "source": [
    "Preencha as seguintes vari√°veis com a quantidade de horas gasta no lab, a dificuldade percebida e a nota esperada (pode apagar o `raise` e o coment√°rio):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42120429",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "778a2222d3ca581bc7f769af2c73f0a8",
     "grade": true,
     "grade_id": "meta_eval",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# meta_eval\n",
    "\n",
    "horas_gastas = None    # 1.5   - N√∫mero float com a quantidade de horas \n",
    "dificuldade_lab = None # 0     - N√∫mero float de 0 a 10 (inclusive)\n",
    "nota_esperada = None   # 10    - N√∫mero float de 0 a 10 (inclusive)\n",
    "\n",
    "# ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f90712",
   "metadata": {},
   "source": [
    "Escreva abaixo (na c√©lula discursiva) outros coment√°rios e feedbacks sobre o lab, pode ser em termos gerais, ou espec√≠fico sobre alguma quest√£o. Se tiver alguma d√∫vida que restou tamb√©m pode colocar aqui.\n",
    "\n",
    "Quaisquer erros, por menor que forem (portugu√™s, o jupyter n√£o tem corretor gramatical para portugu√™s e a extens√£o do navegador n√£o pega na c√©lula), pode comentar abaixo para podermos melhorar e corrigir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e5355",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f2631795cfc36bf2f1ec9ad84af8d32",
     "grade": true,
     "grade_id": "meta_eval_discursivo",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "ESCREVA A SOLU√á√ÉO ABAIXO (n√£o mude essa primeira linha):\n",
    "**ATEN√á√ÉO**\n",
    "\n",
    "**ATEN√á√ÉO**\n",
    "\n",
    "**ATEN√á√ÉO**\n",
    "\n",
    "**ATEN√á√ÉO**\n",
    "\n",
    "**QUEST√ÉO DISCURSIVA**\n",
    "\n",
    "ESCREVA SUA RESPOSTA AQUI (n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "\n",
    "**ATEN√á√ÉO**\n",
    "\n",
    "**ATEN√á√ÉO**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551bc06",
   "metadata": {},
   "source": [
    "Fim do laborat√≥rio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
