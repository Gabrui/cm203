{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776b8f27",
   "metadata": {},
   "source": [
    "**Instituto Tecnológico de Aeronáutica – ITA**\n",
    "\n",
    "**Visão Computacional - CM-203**\n",
    "\n",
    "**Professores:** \n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "Gabriel Adriano de Melo\n",
    "\n",
    "\n",
    "**Orientações padrão:**\n",
    "\n",
    "Antes de você entregar o Lab, tenha certeza de que tudo está rodando corretamente (sequencialmente): Primeiro, **reinicie o kernel** (`Runtime->Restart Runtime` no Colab ou `Kernel->Restart` no Jupyter), depois rode todas as células (`Runtime->Run All` no Colab ou `Cell->Run All` no Jupyter) e verifique que as células rodem sem erros, principalmente as de correção automática que apresentem os `assert`s.\n",
    "\n",
    "É muito importante que vocês não apaguem as células de resposta para preenchimento, isto é, as que contenham o `ESCREVA SEU CÓDIGO AQUI` ou o \"ESCREVA SUA RESPOSTA AQUI\", além das células dos `assert`, pois elas contém metadados com o id da célula para os sistemas de correção automatizada e manual. O sistema de correção automatizada executa todo o código do notebook, adicionando testes extras nas células de teste. Não tem problema vocês criarem mais células, mas não apaguem as células de correção. Mantenham a solução dentro do espaço determinado, por organização. Se por acidente acontecer de apagarem alguma célula que deveria ter a resposta, recomendo iniciar de outro notebook (ou dar um `Undo` se possível), pois não adianta recriar a célula porque perdeu o ID. Ou então você baixa e abre o notebook como texto (é um JSON) e readiciona o campo de ID. Neste ano nós também colocamos um comentário nessas células que é igual ao ID delas, para ser um failsafe em caso de sumirem com o ID das células, então NÃO apaguem esse comentário com ID.\n",
    "\n",
    "Os notebooks vocês podem alterar à vontade, podem criar novas células, modificar as existentes, apagar (a menos das células de correção). O corretor automático executará todas as células e verificará a presença de erro nos `asserts`, depois haverá a correção manual das questões com apreciação da resposta e comentários gerados em HTML. Se ele não achar a célula com os asserts, fica sem a nota da questão, se ele não achar a célular com a questão, fica sem os comentários. Mas vocês podem escreve sim código fora dos espaço delemitado pelo `ESCREVA SEU CÓDIGO AQUI` sem problemas, só não altera a assinatura da função. Esse espaço foi pensado para facilitar a sua implementação.\n",
    "\n",
    "Os Notebooks foram programados para serem compatíveis com o Google Colab, instalando as dependências necessárias automaticamente a baixando os datasets necessários a cada Lab. Os comandos que se inicial por ! (ponto de exclamação) são de bash e também podem ser executados no terminal linux, que justamente instalam as dependências."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d6459",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb3ebd",
   "metadata": {},
   "source": [
    "# Laboratório 2 - nanoGrad\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT).  **NÃO** pesquise a resposta na internet (**NÃO** copie o código de repositórios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documentação do NumPy (mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências). Este laboratório foi baseado no material do [Andrej Karpathy](https://github.com/karpathy). Por isso você não pode copiar o código dele, mas pode assistir as [aulas dele](https://www.youtube.com/watch?v=VMj-3S1tku0) e também outras aulas na internet, só não copie o código sem pensar/sem entender efetivamente o que ele faz.\n",
    "\n",
    "\n",
    "Neste laboratório vamos implementar o nosso próprio framework para calcular os gradientes automaticamente. O importante aqui é que você entenda a intuição de que a rede neural é um grafo computacional, e cada nó desse grafo computacional representa um valor (estado intermediário) que sofre operações (arestas direcionadas) e que resulta em novos valores (nós). Tudo isso no cálculo direto.\n",
    "\n",
    "Mas também temos o nosso cálculo dos gradientes (por backpropagation) que é fazer o caminho reverso do que fizemos no nosso cálculo direto.\n",
    "\n",
    "E no final, todo esse trabalho tem o objetivo de otimizarmos os nossos parâmetros: atualizá-los na direção do gradiente, isto é, mudar os parâmetros um pouquinho para cima ou para baixo a depender de que essa mudança melhore a nossa função custo (aumentar ou diminuir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f275a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from pathlib import Path             # Já importado pelo fastai\n",
    "# # from typing import Callable          # Já importado pelo fastai\n",
    "# # import numpy as np                   # Já importado pelo fastai\n",
    "# # import pandas as pd                  # Já importado pelo fastai\n",
    "# # from PIL import Image                # Já importado pelo fastai\n",
    "# # from matplotlib import pyplot as plt # Já importado pelo fastai\n",
    "# from fastai.vision.all import *        # fastai==2.7.12\n",
    "# import cv2                             # opencv-contrib-python-headless==4.8.0.74\n",
    "# # \n",
    "\n",
    "\n",
    "\n",
    "# # Caso esteja executando o notebook localmente, reimplementa o cv2_imshow\n",
    "# from IPython.utils import io\n",
    "# from IPython.display import display\n",
    "# import warnings\n",
    "# try:\n",
    "#     from google.colab.patches import cv2_imshow\n",
    "# except:\n",
    "#     def cv2_imshow(img):\n",
    "#         display(Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "# # Faz o numpy imprimir mais números por linha\n",
    "# np.set_printoptions(edgeitems=40, linewidth=130)\n",
    "\n",
    "# import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24345ce3",
   "metadata": {},
   "source": [
    "Importar as bibliotecas, vamos usar apenas o NumPy para fazer a nossas operações, o graphviz é para plotar o grafo, o matplotlib para plotar os gráficos, o fastcore é só para usarmos o método de patch_to (monkey patching para alterar uma classe já existente), anotações de tipos em Python 3.8, e o dataset sintético."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.10.12 ou >3.8\n",
    "import numpy as np                      # numpy==1.22.4\n",
    "from graphviz import Digraph            # graphviz==0.20.1\n",
    "import matplotlib.pyplot as plt         # matplotlib==3.7.1\n",
    "from fastcore.basics import patch_to    # fastcore==1.5.29\n",
    "\n",
    "\n",
    "from typing import Tuple, Callable, Union, Set, List\n",
    "from sklearn.datasets import make_moons # sklearn==1.3.0\n",
    "\n",
    "np.set_printoptions(precision=4) # pode mudar aqui se quiser\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401fa20",
   "metadata": {},
   "source": [
    "Dados sintéticos para treinar a rede neural (além de mudar o diretório do notebook para o /content , que é o que o Colab já faz):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe668352",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afef6e7abdb81ae12340522169a06294",
     "grade": false,
     "grade_id": "dataset_lab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# dataset_lab\n",
    "\n",
    "matriz_X, matriz_Y = make_moons(n_samples=200, noise=0.15, random_state=25)\n",
    "matriz_Y = matriz_Y.reshape(-1, 1).astype(np.float64)\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37c2f4",
   "metadata": {},
   "source": [
    "## Nós e Operações Computacionais Básicos (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce00d5",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "O elemento mais básico de um grafo computacional são os seus nós (bolinhas), que na nossa representação, define um valor numérico (ou n-dimensional).\n",
    "\n",
    "O que dá 'liga' aos elementos são as suas conexões (arestas, setinhas), que na nossa representação, define a operação (transformação) que o valor sofre.\n",
    "\n",
    "Com as definições de nós e arestas direcionadas podemos construir um grafo direcionado que consegue representar uma função computável e, a menos do controle de fluxo abitrário (loops com condições de parada não-fixas), também um programa 'bem comportado'.\n",
    "\n",
    "O nível de granularidade dos nós e das operações (apenas um escalar vs matriz n-dimensional) depende do gosto do 'freguês', inclusive, um nó poderia ser uma rede neural inteira! Só precisamos saber calcular a derivada da saída com relação à entrada do nó (isto é o jacobiano para o caso n-dimensional).\n",
    "\n",
    "Mas para não assustar (espero ter motivado), vamos começar do caso mais simples: um nó sendo apenas um escalar e com operações sobre valores escalares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0b83d",
   "metadata": {},
   "source": [
    "Observe com atenção o código abaixo, é simples, mas é crucial que você entenda o funcionamento do Nó Computacional e do seu contexto, que simplesmente guarda uma referência para os seus pais e para a operação que os pais fizeram para poder gerá-lo. Apenas essas informações (além do valor) já é o suficiente para definir uma estrutura computacional de grafo direcionado.\n",
    "\n",
    "O método `__init__` é o inicializador da classe de Python (quando instanciamos um novo objeto da classe e passamos argumentos).\n",
    "\n",
    "O método `__add__` é o que Python chama quando fazemos uma operação de `+` entre dois objetos da mesma classe (`NoComp(1) + NoComp(3)`).\n",
    "\n",
    "Semelhantemente o método `__mul__` é para multiplicação (operador `*`), isto é, `NoComp(5) + NoComp(7)`.\n",
    "\n",
    "E finalmente o `tanh` é uma função com nome qualquer, mas que no caso implementa a função tangente hiperbólica, isto é, `NoComp(1.2).tanh()`.\n",
    "\n",
    "Essas operações todas devem resultar em um novo objeto do `NoComp`, que é a questão que vocês vão desenvolver abaixo.\n",
    "\n",
    "A lista de filhos que colocamos é opcional no sentido que a própria construção do grafo (por meio das operações) já induz a noção de filhos (cuidado se for pela aula do Andej que ele troca pai por filho). Mas deixar a lista explícita fica mais cômodo para imprimir o grafo ou se quisermos reaproveitá-lo depois (em vez de construir um novo grafo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contexto:\n",
    "    \"\"\"Situa o nó (quem o gerou): os seus pais e a operação que eles fizeram\"\"\"\n",
    "    \n",
    "    def __init__(self, pais:Tuple['NoComp'], operacao: Callable):\n",
    "        self.pais = pais\n",
    "        self.operacao = operacao\n",
    "\n",
    "class NoComp:\n",
    "    \"\"\"Nó Computacional que realiza operações básicas, armazena o seu valor, gradiente e contexto no grafo\"\"\"\n",
    "    \n",
    "    def __init__(self, valor, ctx:Union[Contexto,None]=None, nome_visual:str=''):\n",
    "        self.valor = valor\n",
    "        self.ctx = ctx\n",
    "        self.nome_visual = nome_visual\n",
    "        self.gradiente = np.zeros_like(self.valor) if type(self.valor) == np.ndarray else 0\n",
    "        self.filhos = [] # Essa lista de filhos é opcional\n",
    "        if ctx is not None:\n",
    "            for pai in ctx.pais:\n",
    "                pai.filhos.append(self)\n",
    "\n",
    "    def __add__(self, outro_noh):\n",
    "        contexto_filho = Contexto((self, outro_noh), adicao)\n",
    "        filho = adicao(contexto_filho)\n",
    "        return filho\n",
    "\n",
    "    def __mul__(self, outro_noh):\n",
    "        return multiplicacao(Contexto((self, outro_noh), multiplicacao))\n",
    "    \n",
    "    def __matmul__(self, outro_noh):\n",
    "        return multi_matrici(Contexto((self, outro_noh), multi_matrici))\n",
    "\n",
    "    def tanh(self):\n",
    "        return tangente_hipe(Contexto((self,), tangente_hipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8734ae",
   "metadata": {},
   "source": [
    "Você pode estar se perguntando agora o porque de guardarmos a operação no contexto e também o que é o `self.gradiente`, mas nós vamos usá-los para na próxima tarefa (retropropagação).\n",
    "\n",
    "Observe a criação de um novo valor abaixo (sem pais), para você que ainda vai se habituar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse código aqui é só para imprimir na tela 😜\n",
    "@patch_to(NoComp) # O patch_to altera a classe 😊\n",
    "def __str__(self: NoComp):\n",
    "    return f\"{{ {self.nome_visual} | valor {self.valor} }}\"\n",
    "\n",
    "@patch_to(NoComp) # O patch_to altera a classe 😊\n",
    "def __repr__(self: NoComp):\n",
    "    return f\"NóComputacional({str(self)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc0415",
   "metadata": {},
   "source": [
    "Cria um novo nó computacional, veja como é 'legal' o valor que ele imprime na célula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "noh_teste = NoComp(12.19, nome_visual='😇')\n",
    "noh_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857b17f",
   "metadata": {},
   "source": [
    "Ainda não podemos fazer operações, você ainda vai implementar a seguida, mas podemos acessar os seus valores dessa forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noh_teste.valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52faeda",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Gentilmente, o código básico de um nó computacional (e de seu contexto) já foi cedido (poderia ter sido uma questão por si só).\n",
    "\n",
    "Você precisa apenas implementar as operações básicas de adicão, de multiplicação e de uma função de ativação tangente hiperbólica. Para a `tanh`, use a função `np.tanh` do NumPy (não use `math.`)\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT).  **NÃO** pesquise a resposta na internet (**NÃO** copie o código de repositórios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documentação do NumPy (mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "A resposta é simplesmente uma linha: crie um novo nó, com o valor correto (é a soma ou a multiplicação ou o np.tanh dos valores dos pais) e com o contexto correto (o ctx que já foi passado como argument).\n",
    "\n",
    "Por enquanto estamos trabalhando apenas com escalares, mas utilize funções vetorizáveis `np.tanh` e as operações de `+` e `*` que são vetorizáveis, para ficar mais fácil as outras questões.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf1f06",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "823c133a2510b461cccf4467ddba7657",
     "grade": false,
     "grade_id": "questao_computa_basico",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_computa_basico\n",
    "\n",
    "def adicao(ctx: Contexto, gradiente=None) -> NoComp:\n",
    "    \"\"\" Cria um novo nó computacional cujo valor é a soma dos valores de seus pais\n",
    "    \n",
    "    :param ctx: Contexto que contém os dois pais que irão gerar o novo nó.\n",
    "    :param gradiente: Não se preocupe com ela agora, é sempre None (nesta primeira fase)\n",
    "    Há apenas dois pais SEMPRE, não precisa se preocupar.\n",
    "    \n",
    "    Retorna um novo nó computacional, um objeto do tipo NoComp.\n",
    "    \"\"\"\n",
    "    if gradiente is None:\n",
    "        # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        retropropaga_adicao(ctx, gradiente)\n",
    "\n",
    "def multiplicacao(ctx: Contexto, gradiente=None) -> NoComp:\n",
    "    \"\"\" Igual a de cima, mas agora é multiplicação \"\"\"\n",
    "    if gradiente is None:\n",
    "        # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        retropropaga_multiplicacao(ctx, gradiente)\n",
    "\n",
    "def tangente_hipe(ctx: Contexto, gradiente=None) -> NoComp:\n",
    "    \"\"\" Cria um novo nó computacional cujo valor é a tangente hiperbólica do valor do pai\n",
    "    \n",
    "    :param ctx: Contexto que contém o único pai que irá gerar o novo nó.\n",
    "    Há apenas UM pai SEMPRE, não precisa se preocupar. Utilize np.tanh.\n",
    "    \n",
    "    Retorna um novo nó computacional, um objeto do tipo NoComp.\n",
    "    \"\"\"\n",
    "    if gradiente is None:\n",
    "        # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        retropropaga_tangente_hipe(ctx, gradiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93060ecd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a877e7265cfbfb3d07ba694fd9f4bed",
     "grade": true,
     "grade_id": "testa_computa_basico",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_computa_basico\n",
    "\n",
    "# Coloquei muitos testes para vocês não errarem...\n",
    "raiz1 = NoComp(12)\n",
    "raiz2 = NoComp(19)\n",
    "raiz3 = NoComp(25)\n",
    "ctx_testa_manual1 = Contexto((raiz1, raiz2), adicao)\n",
    "ctx_testa_manual2 = Contexto((raiz1, raiz2), multiplicacao)\n",
    "ctx_testa_manual3 = Contexto((raiz3,)      , tangente_hipe)\n",
    "assert type(adicao       (ctx_testa_manual1)) == NoComp\n",
    "assert type(multiplicacao(ctx_testa_manual2)) == NoComp\n",
    "assert type(tangente_hipe(ctx_testa_manual3)) == NoComp\n",
    "assert type(adicao       (ctx_testa_manual1).ctx) == Contexto\n",
    "assert type(multiplicacao(ctx_testa_manual2).ctx) == Contexto\n",
    "assert type(tangente_hipe(ctx_testa_manual3).ctx) == Contexto\n",
    "assert abs(adicao       (ctx_testa_manual1).valor -  31) < 1e-9\n",
    "assert abs(multiplicacao(ctx_testa_manual2).valor - 228) < 1e-9\n",
    "assert abs(tangente_hipe(ctx_testa_manual3).valor -   1) < 1e-9\n",
    "\n",
    "# Agora fica legal, observe, estamos montando o grafo:\n",
    "a = raiz1 + raiz2\n",
    "b = raiz2 * raiz3\n",
    "c = a + b * a + b * b\n",
    "d = c.tanh()\n",
    "e = NoComp(1.3).tanh()\n",
    "f = NoComp(1.3) + NoComp(4.1) * NoComp(-1.9) + NoComp(0).tanh()\n",
    "\n",
    "assert abs(a.valor -     31) < 1e-9\n",
    "assert abs(b.valor -    475) < 1e-9\n",
    "assert abs(c.valor - 240381) < 1e-9\n",
    "assert abs(d.valor -      1) < 1e-9\n",
    "assert abs(e.valor - 0.86172315931) < 1e-9\n",
    "assert a.ctx.pais[0] == raiz1\n",
    "assert a.ctx.pais[1] == raiz2\n",
    "assert len(a.ctx.pais) == 2\n",
    "assert b.ctx.pais[0] == raiz2\n",
    "assert b.ctx.pais[1] == raiz3\n",
    "assert len(b.ctx.pais) == 2\n",
    "assert d.ctx.pais[0] == c\n",
    "assert len(d.ctx.pais) == 1\n",
    "assert abs(f.valor + 6.49) < 1e-9\n",
    "\n",
    "# Deve abstrair os valores n-dimensionais:\n",
    "raizm = NoComp(np.array([[1, 34, 4], [-1, -3, 8]]))\n",
    "contasm = raizm + raizm * raizm.tanh()\n",
    "assert np.linalg.norm(contasm.valor - np.array((\n",
    "      [[ 1.7616e+00,  6.8000e+01,  7.9973e+00],\n",
    "       [-2.3841e-01, -1.4836e-02,  1.6000e+01]]))) < 1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4e5ef",
   "metadata": {},
   "source": [
    "Eu coloquei um zilhão de testes aí em cima, por que se você errar essa questão, você erra o lab inteiro. Tente entende o que cada teste desses está fazendo. Por exemplo, para comparar dois números de pontos flutuantes nós verificamos se a diferença entre eles está dentro de um delta.\n",
    "\n",
    "Bem, agora que você implementou as operações básicas, você merece ver o grafo resultante.\n",
    "\n",
    "Só vamos primeiro salvar o nome das variáveis que declaramos anteriormente como também o valor da string nome_visual dos nossos Nós Computacionais. E também vamos definir um nome mais sucinto para os nossos operatores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nome, obj in list(globals().items()):\n",
    "    if type(obj) == NoComp:\n",
    "        obj.nome_visual = nome\n",
    "\n",
    "nomes_op = {adicao: '+', multiplicacao: '*', tangente_hipe: 'tanh'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd0ed2",
   "metadata": {},
   "source": [
    "E agora visualizamos o grafo com a biblioteca graphviz. Salvamos essa função como método da classe `NoComp`.\n",
    "\n",
    "Perceba como nossos nós são os retângulos com os cantos arredondados, há nós de contas intermediárias, sem nomes, e as operações são as arestas. Olhe por exemplo que no caso do valor `b`, há duas arestas saindo dele próprio para um mesmo nó intermediário (o caso de `b*b`), é muito importante ter cuidado nisso durante a implementação do backpropagation na próxima parte.\n",
    "\n",
    "Inclusive, esse código do `desenha` já é uma forma de percorrer o gráfico (recursivamente), mas que foi implementado em profundidade e não em largura. Além disso, ele não repete elementos que já foram visitados (`ja_desenhados`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201820a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(NoComp)\n",
    "def desenha(self: NoComp, grafo_visual: Union[Digraph,None]=None, ja_desenhados: Union[Digraph, Set]=None):\n",
    "    ja_desenhados = ja_desenhados or set()\n",
    "    if grafo_visual is None:\n",
    "        grafo_visual = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "        grafo_visual.attr('node', shape='record', style='rounded')\n",
    "    if self in ja_desenhados:\n",
    "        return grafo_visual\n",
    "    grafo_visual.node(str(id(self)), str(self))\n",
    "    ja_desenhados.add(self)\n",
    "    if self.ctx is not None:\n",
    "        for pai in self.ctx.pais:\n",
    "            grafo_visual.edge(str(id(pai)), str(id(self)), nomes_op[self.ctx.operacao])\n",
    "            pai.desenha(grafo_visual, ja_desenhados)\n",
    "    return grafo_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c3bd2",
   "metadata": {},
   "source": [
    "**BIZU PARA A PRÓXIMA QUESTÃO**\n",
    "\n",
    "Vou deixar aqui essa outra implementação que percorre o grafo em largura, como bizu para a próxima questão (para facilitar a vida). Observe que a recursão aqui perde um pouco o 'fôlego' e é substituída mais claramento por um loop. A intuição para isso é que agora temos mais uma fila (é uma lista em Python) `a_desenhar` que representa a ordem dos próximos nós a serem visitados. Perceba que novamente não desenhamos nós que já foram desenhados (a implementação mais eficiente disso seria com uma flag, mas usamos um `set()` em Python que é mais lento mas fica mais fácil para não precisar acrescentar mais atributos ao nó)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eeea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(NoComp)\n",
    "def desenha(self: NoComp, grafo_visual:Union[Digraph,None]=None):\n",
    "    if grafo_visual is None:\n",
    "        grafo_visual = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "        grafo_visual.attr('node', shape='record', style='rounded')\n",
    "    grafo_visual.node(str(id(self)), str(self))\n",
    "    ja_desenhados = set()\n",
    "    a_desenhar = [self]\n",
    "    while len(a_desenhar) > 0:\n",
    "        atual = a_desenhar.pop(0)\n",
    "        if atual in ja_desenhados:\n",
    "            continue\n",
    "        if atual.ctx is not None:\n",
    "            a_desenhar.extend(atual.ctx.pais)\n",
    "            for pai in atual.ctx.pais:\n",
    "                grafo_visual.edge(str(id(pai)), str(id(atual)), nomes_op[atual.ctx.operacao])\n",
    "        grafo_visual.node(str(id(atual)), str(atual))\n",
    "        ja_desenhados.add(atual)\n",
    "    return grafo_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "contasm.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb3345",
   "metadata": {},
   "source": [
    "## Retropropagação Básica (3 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e5620",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0477e",
   "metadata": {},
   "source": [
    "Temos que inicializar o nosso atributo dos gradientes (inicialmente estava `None`, mas depois já copiei pro init). Vamos fazer logo recursivamente, para inicializarmos todos os elementos da rede (se começarmos pelo elemento mais 'longe', 'mais no fundo', a loss). Ela já está vector-friendly utilizando o `np.zeros_like` do NumPy.\n",
    "\n",
    "Observe que essa função `inicializa_gradiente` percorre o grafo em profundidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(NoComp)\n",
    "def inicializa_gradiente(self: NoComp):\n",
    "    self.gradiente = np.zeros_like(self.valor) if type(self.valor) == np.ndarray else 0\n",
    "    if self.ctx is None:\n",
    "        return\n",
    "    for pai in self.ctx.pais:\n",
    "        if pai.gradiente is None or np.any(pai.gradiente != 0):\n",
    "            pai.inicializa_gradiente()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0cdc5b",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente as funções `retropropaga_multiplicacao` e `retropropaga_tangente_hipe` com base no exemplo do `retropropaga_adicao`. Essas funções são chamadas durante a retropropagação e elas acumulam o gradiente que o nó filho tem e passa para os seus pais. Para isso você precisa utilizar a regra da cadeia e saber calcular a *derivada* da operação que vocês fizeram. Essa retropropagação dessas funções é **LOCAL**, isto é, ela opera apenas nos nós imediatos.\n",
    "\n",
    "Para fazer a retropropagação **GLOBAL**, implemente a função `retropropaga` (que vai ser usada como método da classe NoComp)\n",
    "\n",
    "Atenção, você deve percorrer o grafo em largura (e não em profundidade), pois temos que garantir que todos os gradientes de um nó já foram completamente acumulados, antes de que ele possa retropropagar para seus pais.\n",
    "\n",
    "![Animação de Percorrer o Grafo por Largura](https://upload.wikimedia.org/wikipedia/commons/4/46/Animated_BFS.gif)\n",
    "\n",
    "Atente-se para o fato que os gradientes devem se acumular os gradientes (e não sobreescrevê-los, lembre-se de `b*b` lá de cima (se somar as derivadas de cada um dá $b + b = 2 b$ que é o valor esperado da retropropagação.\n",
    "\n",
    "Se tiver dúvidas sobre BFS, pode se sentir a vontade para pesquisar outras aulas / materiais (mas não a resposta pronta). Veja por exemplo do [Nipun Ramakrishnan no youtube](https://www.youtube.com/watch?v=xlVX7dXLS64)\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT).  **NÃO** pesquise a resposta pronta na internet (**NÃO** copie o código de repositórios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documentação do NumPy (mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Para as questões do `retropropaga_multiplicacao` e do `retropropaga_tangente_hipe` basta modificar o gradiente dos seus pais de acordo com a regra da cadeia e a derivada analítica da função. Não se esqueça que os gradientes devem ser acumulados, e não sobreescritos.\n",
    "    \n",
    "A derivada da multiplicação é o outro elemento que está multiplicando.\n",
    "\n",
    "A derivada de tanh(x) é (1 - tanh(x)**2), use np.tanh e **2 para x².\n",
    "\n",
    "Para a questão do `retropropaga` olhe a função do desenha que implementei duas vezes justamente para você ficar de bizu (a última implementação é por largura), basta você verificar que as operações são exatamente as mesmas (só cuidado para não deixar o loop mais interno, porque o retropropaga_adicao/multiplicacao/tangente_hipe já retropropaga um nível para todos os pais.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89485e84",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce678f3237fee47696962a9cf4808584",
     "grade": false,
     "grade_id": "questao_retropropaga_basico",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_retropropaga_basico\n",
    "\n",
    "def retropropaga_adicao(ctx: Contexto, gradiente: Union[float,np.ndarray]):\n",
    "    \"\"\" Esse eu deixei de DICA, para dar uma dica ainda maior do que fazer \n",
    "        Perceba que o valor da derivade de c = a + b é 1 para dc/db e dc/da\"\"\"\n",
    "    ctx.pais[0].gradiente += gradiente * 1\n",
    "    ctx.pais[1].gradiente += gradiente * 1\n",
    "\n",
    "def retropropaga_multiplicacao(ctx: Contexto, gradiente: Union[float,np.ndarray]):\n",
    "    \"\"\" Retropropaga o gradiente localmente, da multiplicação, apenas para os pais imediatos \n",
    "    \n",
    "    :param ctx: Contexto do nó atual que está retropropagando para os nós que o gerou (seus pais)\n",
    "    :param gradiente: Valor float ou matriz numpy que representa o gradiente que o nó atual tem\n",
    "    \n",
    "    Essa função não retorna nenhuma valor, mas modifica os valores dos gradientes dos ctx.pais.\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def retropropaga_tangente_hipe(ctx: Contexto, gradiente: Union[float,np.ndarray]):\n",
    "    \"\"\" Retropropaga o gradiente localmente, da tangente hiperbólica, apenas para o seu pai imediato\n",
    "    \n",
    "    :param ctx: Contexto do nó atual que está retropropagando para o nó que o gerou (seu pai)\n",
    "    :param gradiente: Valor float ou matriz numpy que representa o gradiente que o nó atual tem\n",
    "    \n",
    "    Essa função não retorna nenhuma valor, mas modifica o valor do gradiente do ctx.pais[0].\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "@patch_to(NoComp)\n",
    "def retropropaga(self: NoComp, gradiente: Union[float, np.ndarray] = None):\n",
    "    \"\"\" Retropropaga o gradiente globalmente, a todos os seus antepassados até as raízes.\n",
    "    \n",
    "    :param self: Instância de NoComp\n",
    "    :param gradiente: Float ou matrix numpy que representa o gradiente a ser retropropagado,\n",
    "                      se não for passado, é assumido como valor unitário (ou matriz)\n",
    "    \n",
    "    Essa função não retorna nenhuma valor, mas modifica os valores dos gradientes de todos os nós antecessores.\n",
    "    \"\"\"\n",
    "    self.gradiente = gradiente or np.ones_like(self.valor) if type(self.valor) == np.ndarray else 1\n",
    "    if gradiente is None and np.size(self.gradiente) > 1:\n",
    "        print('ATENÇÃO, essa função apenas calcula os gradientes, precisamos de um escalar, (o jacobiano é interno), assumimos pesos unitários para as dimensões')\n",
    "    ja_retropropagados = set()\n",
    "    a_retropropagar = [self]\n",
    "    while len(a_retropropagar) > 0:\n",
    "        # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19318b7e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b66cf41f902c94db0080feac0cb45848",
     "grade": true,
     "grade_id": "testa_retropropaga_basico",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_retropropaga_basico\n",
    "\n",
    "m = NoComp(0.4, nome_visual='m')\n",
    "b = NoComp(-0.1, nome_visual='b')\n",
    "x = NoComp(8.1, nome_visual='x')\n",
    "y = m * x + b; y.nome_visual = 'y'\n",
    "y.inicializa_gradiente()\n",
    "y.retropropaga()\n",
    "assert abs(b.gradiente - 1  ) < 1e-9\n",
    "assert abs(x.gradiente - 0.4) < 1e-9\n",
    "assert abs(m.gradiente - 8.1) < 1e-9\n",
    "\n",
    "def testa_func(dw01=0, dw02=0, dw11=0, dw22=0, dx2=0):\n",
    "    x0, x1, x2, x3 = NoComp(1), NoComp(4.1), NoComp(-0.3+dx2), NoComp(2.1)\n",
    "    w01, w11, w21, w31 = NoComp(0.1+dw01), NoComp(-0.3+dw11), NoComp(-1), NoComp(0.9)\n",
    "    w02, w12, w22, w32 = NoComp(-0.3+dw02), NoComp(0.5), NoComp(-0.2+dw22), NoComp(0.4)\n",
    "    z1 = x0*w01 + x1*w11 + x2*w21 + x3*w31\n",
    "    z2 = x0*w02 + x1*w12 + x2*w22 + x3*w32\n",
    "    a0, a1, a2 = NoComp(1), z1.tanh(), z2.tanh()\n",
    "    wf0, wf1, wf2 =  NoComp(0.2), NoComp(-0.8), NoComp(0.4)\n",
    "    zf = a0*wf0 + a1*wf1 + a2*wf2\n",
    "    yf = zf.tanh()\n",
    "    for nome, obj in list(locals().items()):\n",
    "        if type(obj) == NoComp:\n",
    "            obj.nome_visual = nome\n",
    "    return yf, (w01, w02, w11, w22, x2)\n",
    "\n",
    "yf0,_ = testa_func()\n",
    "delta = 1e-9\n",
    "yfdw01, (dw01, _, _, _, _) = testa_func(dw01=delta)\n",
    "yfdw01.inicializa_gradiente()\n",
    "yfdw01.retropropaga()\n",
    "dyfdw01 = (yfdw01.valor - yf0.valor)/delta\n",
    "assert abs(dyfdw01 - dw01.gradiente) < 1e-6\n",
    "yfdx2, (_, _, _, _, dx2) = testa_func(dx2=delta)\n",
    "yfdx2.inicializa_gradiente()\n",
    "yfdx2.retropropaga()\n",
    "dyfdx2 = (yfdx2.valor - yf0.valor)/delta\n",
    "assert abs(dyfdx2 - dx2.gradiente) < 1e-6\n",
    "\n",
    "# Deve abstrair os valores n-dimensionais:\n",
    "raizm = NoComp(np.array([[0.6, -1.8, -0.5], [-0.2, 0.4, 1.2]]))\n",
    "contasm = raizm + raizm * raizm.tanh()\n",
    "contasm.inicializa_gradiente()\n",
    "contasm.retropropaga()\n",
    "assert np.linalg.norm(raizm.gradiente - np.array([\n",
    "    [ 1.96399622455, -0.13321108611,  0.14465897626],\n",
    "    [ 0.61041608318,  1.72220447669,  2.19967860246]])) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab561e3",
   "metadata": {},
   "source": [
    "Como vamos calcular os gradientes, seria legal se pudéssemos ver o seu valor no grafo também:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limita_str(valor):\n",
    "    string = str(valor)\n",
    "    return string if len(string) < 30 else valor.shape\n",
    "\n",
    "@patch_to(NoComp) # Vamos mostrar os gradientes agora também\n",
    "def __str__(self: NoComp):\n",
    "    return f\"{{ {self.nome_visual} | valor {limita_str(self.valor)} | grad {limita_str(self.gradiente)}}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14177b81",
   "metadata": {},
   "source": [
    "Se estiver com dificuldades, observe os grafos abaixo, principalmente o primeiro mais simples do $y=m x + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb59fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6f84d",
   "metadata": {},
   "source": [
    "E agora o grafo maior que é quase como uma implementação de uma pequena rede neural, sendo que cada nó é um neurônio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762de501",
   "metadata": {},
   "outputs": [],
   "source": [
    "yfdx2.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258895d",
   "metadata": {},
   "source": [
    "Se você não ficou emocionado com esse grafo, você não tem coração...\n",
    "\n",
    "Tente brincar com ele, mude os valores (recalcule) e repropague e veja o que acontece com os gradientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e740b56",
   "metadata": {},
   "source": [
    "## Operações Vetorizadas (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae7b11",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "Tratar cada nó como um neurônio separado é muito ineficiente. Podemos fazer toda essa operação de combinação linear (multiplicação pelos pesos e soma) por uma operação de multiplicação matricial.\n",
    "\n",
    "Assim, retornamos ao mesmo paradigma da primeira questão e da questão anterior, mas agora estamos trabalhando apenas com valores que são matrizes. Inclusive, se você tiver implementado agnosticamente do tipo do valor, as questões anteriores já funcionam com matrizes, sem que você tenha se preocupado com isso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef04da",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função abaixo, `multi_matrici`, no mesmo estilo das funções da primeira questão (`adicao`, `multiplicacao`, `tangente_hipe`) mas agora os valores dos pais são garantidamente matrizes numpy compatíveis.\n",
    "\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT).  **NÃO** pesquise a resposta na internet (**NÃO** copie o código de repositórios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documentação do NumPy (mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Observe o que já foi implementado nas questões anteriores.\n",
    "\n",
    "Verifique se é propagação direta (gradiente is None) ou se é retropropagacao.\n",
    "Garantidamente ctx.pais são vetores numpy cujas as dimensões estão corretas.\n",
    "Há apenas DOIS pais, não precisa se preocupar. Utilize @ para operar sobre matrizes numpy.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2022928",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76f9b40f260bfa22551facda72cb06ef",
     "grade": false,
     "grade_id": "questao_vetoriza",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_vetoriza\n",
    "\n",
    "def multi_matrici(ctx: Contexto, gradiente: np.ndarray = None) -> Union[NoComp, None]:\n",
    "    \"\"\" Cria um novo nó computacional cujo valor é a multiplicação matricial entre os pais. (a ordem importa)\n",
    "    \n",
    "    :param ctx: Contexto que contém os dois pais que irão gerar o novo nó.\n",
    "    \n",
    "    Verifique se é propagação direta (gradiente is None) ou se é retropropagacao.\n",
    "    Garantidamente ctx.pais são vetores numpy cujas as dimensões estão corretas.\n",
    "    Há apenas DOIS pais, não precisa se preocupar. Utilize @ para operar sobre matrizes numpy.\n",
    "    \n",
    "    Retorna um novo nó computacional resultad da multiplicação matricial\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f18d19",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d0d2ffe4198945fc5ab531f6420165a",
     "grade": true,
     "grade_id": "testa_vetoriza",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_vetoriza\n",
    "\n",
    "X = NoComp(np.array([[ 1  ,  4.1, -0.3, 2.1]]), nome_visual='X')\n",
    "W = NoComp(np.array([[ 0.1, -0.3],\n",
    "                     [-0.3,  0.5],\n",
    "                     [-1  , -0.2],\n",
    "                     [ 0.9,  0.4]]), nome_visual='W')\n",
    "Z = X @ W; Z.nome_visual='Z'\n",
    "A = Z.tanh(); A.nome_visual='A'\n",
    "Wf = NoComp(np.array([[-0.8],[0.4]])); Wf.nome_visual='Wf'\n",
    "Zf = A @ Wf; Zf.nome_visual = 'Zf'\n",
    "Yf = Zf.tanh(); Yf.nome_visual='Yf'\n",
    "\n",
    "Yf.inicializa_gradiente()\n",
    "Yf.retropropaga()\n",
    "assert np.linalg.norm(X.gradiente - np.array([\n",
    "  [-0.0312697368 ,  0.09081125174,  0.28871369872, -0.25819345159]])) < 1e-6\n",
    "\n",
    "L = Yf @ Yf @ Yf\n",
    "L.inicializa_gradiente()\n",
    "L.retropropaga()\n",
    "assert np.linalg.norm(X.gradiente - np.array([\n",
    "  [-0.00489385712,  0.0142123771 ,  0.04518501706, -0.04040845851]])) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f0f47",
   "metadata": {},
   "source": [
    "Vamos atualizar a nossa variável global com os nomes das nossas operações, `nomes_op`, para acrescentar a nossa operação de `@` e imprimir o nosso grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_op[multi_matrici] = '@'\n",
    "\n",
    "Yf.inicializa_gradiente()\n",
    "Yf.retropropaga()\n",
    "\n",
    "Yf.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976549a",
   "metadata": {},
   "source": [
    "## Rede Neural (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ccd40",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "Agora temos todas as ferramentas a nossa disposição para construírmos a nossa própria rede neural. Veja o exemplo da classe abaixo que recebe uma tupla de inteiros que indica quantos neurônios temos por camada.\n",
    "\n",
    "Ela guarda os parâmetros que são treináveis, são os nós raízes (folhas da retropropagação), pois se o X mudar eles não mudam (enquanto as ativações são todas recalculadas).\n",
    "\n",
    "Olha o legal da classe abaixo: quando você invoca um objeto que é instância dela (função `__call__`) ela te retorna um novo nó computacional. É por isso que os frameworks de deep learning tem os 'parênteses duplos', tipo torch.ReLU()(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedeNeuralSequencial:\n",
    "    def __init__(self, neuronios_por_camada: Tuple[int]):\n",
    "        self.pesos_sinapticos, self.bias = constroi_rede_neural_sequencial(neuronios_por_camada)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        ativacao = x\n",
    "        for i, (peso, b) in enumerate(zip(self.pesos_sinapticos, self.bias)):\n",
    "            combinacao_linear = (ativacao @ peso) + b\n",
    "            ativacao = combinacao_linear.tanh()\n",
    "            combinacao_linear.nome_visual = f\"Z^({i+1})\"\n",
    "            ativacao.nome_visual = f\"A^({i+1})\"\n",
    "        return ativacao\n",
    "    \n",
    "    def parametros(self):\n",
    "        return self.pesos_sinapticos + self.bias\n",
    "    \n",
    "    def zera_gradientes_parametros(self):\n",
    "        for parametro in self.parametros():\n",
    "            parametro.gradiente.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084f004",
   "metadata": {},
   "source": [
    "Para inicializar os pesos da nossa rede aleatoriamente, já implementamos a função abaixo que amostra de uma distribuição gaussiana com variância escalada pela quantidade de neurônios da camada anterior, isto é, a quantidade de entradas a um neurônio, o seu $fan_{in}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04717980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valor_inicial(n_linhas:int, n_colunas:int) -> np.ndarray:\n",
    "    \"\"\" Retorna uma matriz numpy com (n_linhas, n_colunas) inicializada aleatoriamente\"\"\"\n",
    "    shape = (n_linhas, n_colunas)\n",
    "    if n_linhas == 1:\n",
    "        return np.zeros(shape, dtype=np.float64)\n",
    "    return np.random.normal(scale=1/np.sqrt(n_linhas), size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67610ef",
   "metadata": {},
   "source": [
    "Além disso, como estamos usando um termo de bias agora (pois não implementamos um nó de concatenação), e pelo fato do NumPy já fazer automaticamente o broadcasting, temos que consertar a nossa função de retropropagação para poder fazer o inverso do broadcasting, isto é, se o broadcasting repete as dimensões, nós temos que reduzi-las (pela soma). Só fizemos isso para a soma e só pensando no caso $(N, M)$, mas se fosse um framework geral, teríamos que pensar também nas outras operações e em dimensões maiores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retropropaga_adicao(ctx: Contexto, gradiente):\n",
    "    for pai in ctx.pais: # Para resolver o broadcasting do bias\n",
    "        if type(gradiente)==np.ndarray and gradiente.shape[0] > pai.gradiente.shape[0] and pai.gradiente.shape[0] == 1:\n",
    "            pai.gradiente += gradiente.sum(axis=0)[None]\n",
    "        else:\n",
    "            pai.gradiente += gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1697915",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função `constroi_rede_neural_sequencial`.\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT).  **NÃO** pesquise a resposta na internet (**NÃO** copie o código de repositórios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documentação do NumPy (mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Basta usar a função `.append`, criando novos Nos Computacionais com as dimensões adequadas.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e793e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b5d25b3515b98a79770ced801a8049f",
     "grade": false,
     "grade_id": "questao_rede_neural",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_rede_neural\n",
    "\n",
    "\n",
    "def constroi_rede_neural_sequencial(neuronios_camada: Tuple[int]) -> Tuple[List[NoComp], List[NoComp]]:\n",
    "    \"\"\" Função que constrói os parâmetros de uma rede neural sequencial\n",
    "    \n",
    "    :param neuronios_camada: Tupla de inteiros que indica a quantidade de neurônios por camada\n",
    "    \n",
    "    Utilize a função valor_inicial para obter uma matriz numpy com valores iniciais de cada camada.\n",
    "    \n",
    "    Retorna uma lista de nós que representam o peso sináptico de cada camada e outra lista de nós para os bias\n",
    "    \"\"\"\n",
    "    pesos_sinapticos: List[NoComp] = []\n",
    "    bias: List[NoComp] = []\n",
    "      \n",
    "    for qtd_anterior, qtd_posterior in zip(neuronios_camada, neuronios_camada[1:]):\n",
    "        # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    for i, (peso, b) in enumerate(zip(pesos_sinapticos, bias)):\n",
    "        peso.nome_visual = f\"W^({i+1})\"\n",
    "        b.nome_visual = f\"B^({i+1})\"\n",
    "    \n",
    "    return pesos_sinapticos, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00ae13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32b44474e7f67e0995c30c4276b49694",
     "grade": true,
     "grade_id": "testa_rede_neural",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_rede_neural\n",
    "\n",
    "np.random.seed(42)\n",
    "pesos_sinapticos, bias = constroi_rede_neural_sequencial((2, 32, 64, 32, 1))\n",
    "assert len(pesos_sinapticos) == 4\n",
    "assert len(bias) == 4\n",
    "assert pesos_sinapticos[0].valor.shape == (2, 32)\n",
    "assert pesos_sinapticos[2].valor.shape == (64,32)\n",
    "assert pesos_sinapticos[3].valor.shape == (32, 1)\n",
    "assert bias[1].valor.shape == (1, 64)\n",
    "np.random.seed(42)\n",
    "w1v = valor_inicial(2, 32)\n",
    "assert np.all(w1v == pesos_sinapticos[0].valor)\n",
    "\n",
    "\n",
    "modelo = RedeNeuralSequencial((2, 32, 64, 32, 1))\n",
    "X = NoComp(np.random.uniform(size=(19, 2)), nome_visual='X')\n",
    "estimativa = modelo(X); estimativa.nome_visual='Ŷ'\n",
    "assert estimativa.valor.shape == (19, 1)\n",
    "estimativa.retropropaga() # Isso imprime a mesagem de atenção, pois não é um escalar\n",
    "\n",
    "um_res = RedeNeuralSequencial((2, 1))(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150247b",
   "metadata": {},
   "source": [
    "Observe como fica a nossa rede neural sequencial. Atente para o seu grafo e os nomes dos parâmetros, sobretudo os nós que não tem pai (eles são os parâmetros que iremos otimizar, a menos do X que é valor de entrada).\n",
    "\n",
    "É claro que se um modelo puramente sequencial é um grafo muito simplificado (uma 'tripa')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17962df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimativa.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96053ca2",
   "metadata": {},
   "source": [
    "## Gradiente Descendente e Otimização (1 ponto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bf04f",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "Agora já temos as nossas ferramentas e o nosso modelo (a faca e o bolo), devemos treiná-lo, isto é, aproveitá-lo.\n",
    "\n",
    "Para isso vamos definir uma loss, no caso a única que dá para fazer com as operações que temos é a MSE.\n",
    "\n",
    "Além disso para a função custo, precisamos somar em todas as dimensões, por isso, implementamos ainda a função soma abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28295133",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(NoComp)\n",
    "def soma(self: NoComp):\n",
    "    return soma_total(Contexto((self,), soma_total))\n",
    "\n",
    "def soma_total(ctx, gradiente=None):\n",
    "    if gradiente is None:\n",
    "        return NoComp(np.sum(ctx.pais[0].valor), ctx)\n",
    "    ctx.pais[0].gradiente += gradiente * np.ones_like(ctx.pais[0])\n",
    "\n",
    "nomes_op = {adicao: '+', multiplicacao: '*', multi_matrici: '@', tangente_hipe: 'tanh', soma_total:'soma'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce417f1",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função `gradiente_descendente`. Observe a função de teste e veja como a loss e o custo foram implementados.\n",
    "\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT).  **NÃO** pesquise a resposta na internet (**NÃO** copie o código de repositórios de autograd, microGrad, nanoGrad, femtoGrad ...).\n",
    "**Pode** olhar a documentação do NumPy (mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Basta implementar a equação de atualização dos pesos que o gradiente faz $\\theta_{atualizado} = \\theta_{antigo} - \\alpha \\nabla_w\\theta$\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216df3f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06d060d100cd70983c3e96fee074b997",
     "grade": false,
     "grade_id": "questao_gradiente",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_gradiente\n",
    "\n",
    "def gradiente_descendente(parametros: List[NoComp], gradiente: List[np.ndarray], alpha:float) -> None:\n",
    "    \"\"\"Implementa apenas um passo do algoritmo de otimização do gradiente descendente.\n",
    "    \n",
    "    :param parametros: Uma lista de nós computacionais que representa os parâmetros a serem atualizados\n",
    "    :param gradiente: Uma lista de matrizes numpy (ou floats) que representam os gradientes que calculamos (pela retropropagação)\n",
    "    \n",
    "    Essa função não retorna nenhuma valor\n",
    "    \"\"\"\n",
    "    for p, grad in zip(parametros, gradiente):\n",
    "        # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f157b0e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48214772224196957301c3c33ef1569b",
     "grade": true,
     "grade_id": "testa_gradiente",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_gradiente\n",
    "\n",
    "np.random.seed(42)\n",
    "modelo = RedeNeuralSequencial((2, 32, 64, 32, 1))\n",
    "X = NoComp(matriz_X, nome_visual='X')\n",
    "Y = NoComp(matriz_Y, nome_visual='y')\n",
    "\n",
    "def uma_epoca(rede, x, y, alpha=0.001):\n",
    "    ŷ = rede(x); ŷ.nome_visual = 'Ŷ'\n",
    "    \n",
    "    # Como nós não temos operação de subtração, tive que multiplicar por -1, kkkkk\n",
    "    loss = ((ŷ + NoComp(-1) * y) * (ŷ + NoComp(-1) * y)); loss.nome_visual = '𝓛'\n",
    "    custo = loss.soma(); custo.nome_visual = '𝓒'\n",
    "    \n",
    "    rede.zera_gradientes_parametros()\n",
    "    custo.retropropaga()\n",
    "    \n",
    "    params = rede.parametros()\n",
    "    gradiente_descendente(params, [p.gradiente for p in params], alpha)\n",
    "    return custo, ŷ\n",
    "\n",
    "custo1, ŷ1 = uma_epoca(modelo, X, Y)\n",
    "epocas = 100\n",
    "custos = []\n",
    "for i in range(epocas):\n",
    "  custo2, ŷ2 = uma_epoca(modelo, X, Y, alpha=0.0020-0.0015*i/epocas)\n",
    "  custos.append(custo2.valor)\n",
    "assert np.all(custo2.valor < custo1.valor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6330ea",
   "metadata": {},
   "source": [
    "Estude com atenção a função `uma_epoca`, entenda como ela calcula a loss, depois o custo, depois zera os gradientes dos parâmetros, faz e retropropagação, e por último faz o gradiente descentende (atualização dos parâmetros).\n",
    "\n",
    "Ela era uma função que vocês deveriam implementar, mas como já estava estourando o tempo do lab, peço que você entendam ela, principalmente o porque de zerar os gradientes dos parâmetros entre uma época e outra (se não estaríamos fazendo uma soma dos gradientes ao longo das épocas).\n",
    "\n",
    "É exatamente esse mesmo loop que iremos implementar no próximo Laboratório em PyTorch. Você entenderão o PyTorch muito mais rapidamente (é a mesma intuição do nanoGrad, só que mais bodoso).\n",
    "\n",
    "Observe como o custo diminuiu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "custo1, custo2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac41936",
   "metadata": {},
   "source": [
    "Veja também que o custo é um escalar (por isso não tivemos a chamada de atenção do retropropaga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb09824",
   "metadata": {},
   "outputs": [],
   "source": [
    "custo1.desenha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927776b9",
   "metadata": {},
   "source": [
    "Veja como o nosso custo evolui ao longo das épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([custo1.valor] + custos)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Custo do Treinamento (MSE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7196505",
   "metadata": {},
   "source": [
    "Perceba que apesar de dos dados Y serem 0 ou 1 (indicarem uma categoria) nós utilizamos uma função custo MSE só porque não implementamos nem a função log, nem a exponencial, para podermos calcular a entropia binária. Mas isso na teoria é 'feio', apesar de 'funcionar'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fdcf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-1.5, 2.5),\n",
    "                     np.linspace(-1, 1.5))\n",
    "x_amostra = np.concatenate([xx.reshape(-1,1), yy.reshape(-1,1)], axis=1)\n",
    "\n",
    "\n",
    "ŷ = modelo(NoComp(x_amostra))\n",
    "\n",
    "\n",
    "plt.contourf(xx, yy, np.clip(ŷ.valor.reshape(xx.shape),0,1), levels=50, cmap='plasma')\n",
    "plt.colorbar()\n",
    "plt.scatter(matriz_X[matriz_Y[:,0] == 0][:, 0], matriz_X[matriz_Y[:,0] == 0][:, 1], color='cyan', marker='^', label='$y = 0$')\n",
    "plt.scatter(matriz_X[matriz_Y[:,0] == 1][:, 0], matriz_X[matriz_Y[:,0] == 1][:, 1], color='crimson', marker='o', label='$y = 1$')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.legend(labelcolor='linecolor')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d724e1e",
   "metadata": {},
   "source": [
    "Veja como ficaría se não tivéssemos treinado o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cfe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-1.5, 2.5),\n",
    "                     np.linspace(-1, 1.5))\n",
    "x_amostra = np.concatenate([xx.reshape(-1,1), yy.reshape(-1,1)], axis=1)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "modelo_sem_treinar = RedeNeuralSequencial((2, 32, 64, 32, 1))\n",
    "ŷ_sem_treinar = modelo_sem_treinar(NoComp(x_amostra))\n",
    "\n",
    "\n",
    "plt.contourf(xx, yy, np.clip(ŷ_sem_treinar.valor.reshape(xx.shape),0,1), levels=50, cmap='plasma')\n",
    "plt.colorbar()\n",
    "plt.scatter(matriz_X[matriz_Y[:,0] == 0][:, 0], matriz_X[matriz_Y[:,0] == 0][:, 1], color='cyan', marker='^', label='$y = 0$')\n",
    "plt.scatter(matriz_X[matriz_Y[:,0] == 1][:, 0], matriz_X[matriz_Y[:,0] == 1][:, 1], color='crimson', marker='o', label='$y = 1$')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.legend(labelcolor='linecolor')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78564c",
   "metadata": {},
   "source": [
    "Espero que você olhem para o grafo computacional de cima e sintam orgulho do trabalho que vocês fizeram nesse lab. Isso é a base dos frameworks de redes neurais PyTorch, TensorFlow, JAX e hoje vocês conseguiram implementar a sua essência."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9fb06b",
   "metadata": {},
   "source": [
    "# Seus dados e feedback aqui:\n",
    "\n",
    "Coloque o seu feedback sobre o lab aqui para podermos melhorá-lo para as próximas turmas e também 'calibrar' os próximos labs (idealmente os 80% dos alunos terminar em bem menos de 3h)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48adc30",
   "metadata": {},
   "source": [
    "Preencha as seguintes variáveis com a quantidade de horas gasta no lab, a dificuldade percebida e a nota esperada (pode apagar o `raise` e o comentário):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42120429",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "778a2222d3ca581bc7f769af2c73f0a8",
     "grade": true,
     "grade_id": "meta_eval",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# meta_eval\n",
    "\n",
    "horas_gastas = None    # 1.5   - Número float com a quantidade de horas \n",
    "dificuldade_lab = None # 0     - Número float de 0 a 10 (inclusive)\n",
    "nota_esperada = None   # 10    - Número float de 0 a 10 (inclusive)\n",
    "\n",
    "# ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f90712",
   "metadata": {},
   "source": [
    "Escreva abaixo (na célula discursiva) outros comentários e feedbacks sobre o lab, pode ser em termos gerais, ou específico sobre alguma questão. Se tiver alguma dúvida que restou também pode colocar aqui.\n",
    "\n",
    "Quaisquer erros, por menor que forem (português, o jupyter não tem corretor gramatical para português e a extensão do navegador não pega na célula), pode comentar abaixo para podermos melhorar e corrigir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e5355",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f2631795cfc36bf2f1ec9ad84af8d32",
     "grade": true,
     "grade_id": "meta_eval_discursivo",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "ESCREVA A SOLUÇÃO ABAIXO (não mude essa primeira linha):\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**QUESTÃO DISCURSIVA**\n",
    "\n",
    "ESCREVA SUA RESPOSTA AQUI (não apague esta célula para não perder o ID)\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551bc06",
   "metadata": {},
   "source": [
    "Fim do laboratório."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
