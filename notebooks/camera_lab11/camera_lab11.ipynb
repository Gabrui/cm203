{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea32aaa",
   "metadata": {},
   "source": [
    "**Aeronautics Institute of Technology – ITA**\n",
    "\n",
    "**Computer Vision – CM-203**\n",
    "\n",
    "**Professors:** \n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "Gabriel Adriano de Melo\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Before submitting your lab, be sure that everything is running correctly (in sequence): first, **restart the kernel** (`Runtime->Restart Runtime` in Colab or `Kernel->Restart` in Jupyter). Then, execute all cells (`Runtime->Run All` in Colab or `Cell->Run All` in Jupyter) and verifies that all cells run without any errors, expecially the automatic grading ones, i.e. the ones with `assert`s.\n",
    "\n",
    "**Do not delete the answer cells**, i.e. the ones that contains `WRITE YOUR CODE HERE` or `WRITE YOUR ANSWER HERE`, because they contain metadata with the ids of the cells for the grading system. For the same reason, **do not delete the test cells**, i.e. the ones with `assert`s. The autograding system executes all the code sequentially, adding extra tests in the test cells. There is no problem in creating new cells, as long as you do not delete answer or test cells. Moreover, keep your solutions within the reserved spaces.\n",
    "\n",
    "The notebooks are implemented to be compatible with Google Colab, and they install the dependencies and download the datasets automatically. The commands which start with ! (exclamation mark) are bash commands and can be executed in a Linux terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225a268",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac376f4",
   "metadata": {},
   "source": [
    "# Camera Model Laboratory\n",
    "\n",
    "In this laboratory, we will implement a camera model and use some OpenCV functions for Perspective-n-Point, camera calibration, and stereo vision.\n",
    "\n",
    "This lab was originally created by Gabriel Melo. Marcos Maximo translated it to English and made some minor improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ab79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install opencv-contrib-python==4.8.0.74 Pillow==9.4.0 matplotlib==3.7.1 scipy==1.11.3 gdown==4.6.0 numpy==1.23.5\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3638147",
   "metadata": {},
   "source": [
    "If you are going to execute this locally and want to change the Path, you can do it without any problems for the automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "! [ ! -d \"/content/calib_esq\" ] && gdown -O /content/calib_esq.zip 1vg2fnoLjcYAdF44HxwK41basZUj0xYMN && unzip /content/calib_esq.zip -d /content && rm /content/calib_esq.zip\n",
    "! [ ! -d \"/content/calib_dir\" ] && gdown -O /content/calib_dir.zip 1d0OeP9YCxx1sWjwDMvf0y6f35Lx3ELPN && unzip /content/calib_dir.zip -d /content && rm /content/calib_dir.zip\n",
    "! [ ! -d \"/content/tsukuba\" ] && gdown -O /content/tsukuba.zip 1Ghpx9_x8E26SzJP3X-Itt94QD3yVM8lT && unzip /content/tsukuba.zip -d /content && rm /content/tsukuba.zip\n",
    "root_path = Path(\"/content\")\n",
    "imgs_left_path = root_path/\"calib_esq\"\n",
    "imgs_right_path = root_path/\"calib_dir\"\n",
    "tsukuba_path = root_path/\"tsukuba\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994dd34",
   "metadata": {},
   "source": [
    "Tip: if you want to access the documentation of a given function, add a question mark to the end of the function name. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37217ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a4f26",
   "metadata": {},
   "source": [
    "## Camera Model\n",
    "\n",
    "Initially, you will implement some equations related to the camera model we presented in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c45f57",
   "metadata": {},
   "source": [
    "### Equivalence Between Convergent Lens and Pinhole\n",
    "\n",
    "Firstly, we will derive some equations regarding the camera model. In our perspective projection model, the light rays converge into a single point (optical center) and form an inverted image in a plane (sensor plane).\n",
    "The thin lens camera is equivalent to the pinhole model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e019db",
   "metadata": {},
   "source": [
    "\n",
    "**OBSERVATION:** As explained in class, the focal lengths of the pinhole model and of a covergent lens are not the same. However, they became basically the same when the object is far from the camera. When we usually focal length in computer vision, we mean the focal length of the pinhole model $f_{pinhole}$, which is the distance between the projection center and the projection plane, as illustrated in the figure below.\n",
    "\n",
    "![Modelo de Câmera Pinhole](https://gam.dev/cm203/imgs/pinhole.svg)\n",
    "\n",
    "The lens (or system of lens) has its own focal length $f_{lens}$. Moreover, we also have the distance from the lens to the image $d_i$. The relationship between the elements in the figure below is given by the \"Thin Lens Equation\".\n",
    "\n",
    "Therefore, we basically have an equivalence between $d_i$ and $f_{pinhole}$. When the object is far from the lens, $d_o >> f_{lens}$, then $d_i \\approx f_{lens}$.\n",
    "\n",
    "![Modelo de Câmera com Lente Equivalente](https://gam.dev/cm203/imgs/lens.svg)\n",
    "\n",
    "Let us compute how $f_{pinhole}$ relates to $f_{lens}$ and $d_o$ using the \"Thin Lens Equation\".\n",
    "\n",
    "To express your solution, implement the function below (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9661f51",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "331997e128102d4463dab1fcecd00cfc",
     "grade": false,
     "grade_id": "equacao_pinhole_f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def focal_length_pinhole(f_lens: np.ndarray, d_o: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Implements the equation that computes the focal length equivalent to a pinhole camera (d_i).\n",
    "    To make testing easier, the implementation considers that the inputs and the output are NumPy arrays.\n",
    "    Therefore, use element-wise operations for these arrays.\n",
    "    :param f_lens: the focal length of a convergent lens, in meters.\n",
    "    :param d_o: distance from the lens to the object plane in focus, in meters.\n",
    "    :return: the focal length equivalent to a pinhole camera, in meters.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return f_pinhole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2708b",
   "metadata": {},
   "source": [
    "Notice that a test for a focal length of 8 mm and a focal plane of 10 m, $f_{pinhole}$ is basically equal to $f_{lens}$, with a difference smaller than 0.08%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0448d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abce1d00cb268a0f1e1b83ef18f1caa3",
     "grade": true,
     "grade_id": "corrige_equacao_pinhole_f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(focal_length_pinhole(0.008,  10) - 0.00800640) < 1e-6\n",
    "assert abs(focal_length_pinhole(np.array([0.008]),  np.array([10.])) - 0.00800640) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddbcc20",
   "metadata": {},
   "source": [
    "### Field of View (FoV)\n",
    "\n",
    "Now, let us compute the FoV (in degrees) for a pinhole camera taking into account the sensor width and the distance from the sensor to the projection center."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0ed7a",
   "metadata": {},
   "source": [
    "![Campo de Visão (Field of View) FoV](https://gam.dev/cm203/imgs/fov.svg)\n",
    "\n",
    "Implement the function below to compute the FoV (0.5 points).\n",
    "\n",
    "**Tip:**\n",
    "Use the function `np.arctan` and the constant `np.pi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c54c6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a81e11fa35af287ef3304534508395eb",
     "grade": false,
     "grade_id": "equacao_fov",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_fov(f_pinhole: np.ndarray, sensor_width: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the field of view in degrees for a pinhole camera.\n",
    "    Consider that the inputs and output are NumPy arrays and use element-wise operations.\n",
    "    :param f_pinhole: focal length, in meters.\n",
    "    :param sensor_width: sensor width, in meters.\n",
    "    :return: field of view in degrees.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return fov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f287c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a1af17d5b391304cc3da5bec825e2b3",
     "grade": true,
     "grade_id": "corrige_equacao_fov",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(compute_fov(1,  2) - 90) < 1e-6\n",
    "assert abs(compute_fov(np.array([1.]),  np.array([2.]))[0] - 90) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219ec11",
   "metadata": {},
   "source": [
    "Let us do the same computation, but taking into account a thin lens camera focused on a object plane at a distance $d_o$ (0.5 points).\n",
    "\n",
    "**Tip:**\n",
    "Use the relationship between $f_{pinhole}$ and $f_{lens}$ you computed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050980ef",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d25d6bca0950c4898609fc441b4d79d6",
     "grade": false,
     "grade_id": "equacao_fov_lente",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_fov_lens(f_lens: np.ndarray, sensor_width: np.ndarray, d_o: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the field of view in degrees for a thin lens camera.\n",
    "    Consider that the inputs and output are NumPy arrays and use element-wise operations.\n",
    "    :param f_lens: focal length of the lens, in meters.\n",
    "    :param sensor_width: sensor width, in meters.\n",
    "    :return: the field of view in degrees.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return fov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782d4f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecf439621f89e7c35c65686b8372ae13",
     "grade": true,
     "grade_id": "corrige_equacao_fov_lente",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(compute_fov_lens(0.001,  0.002, 100) - 89.999427) < 1e-6\n",
    "assert abs(compute_fov_lens(np.array([0.001]),  np.array([0.002]), np.array([100.]))[0] - 89.999427) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334cd05b",
   "metadata": {},
   "source": [
    "Notice that the FoV changes very little with respect to the distance to the plane in focus. The change is larger for objects which are very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60daa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0.07, 5, 99), compute_fov_lens(np.array([0.001]*99), np.array([0.002]*99), np.linspace(0.07, 5,99)))\n",
    "plt.xlabel('Plane in focus (m)')\n",
    "plt.ylabel('Plane in focus (degrees)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65258f3c",
   "metadata": {},
   "source": [
    "Now, let's consider a more practical setting, where the camera manufacturer gives us the field of view in degrees and we want to be able to compute the perspective projection for this camera. Therefore, let's compute $f_{pinhole}$ so we can construct the intrinsic matrix related to this camera. We will compute the horizontal FoV, but the calculation is the same for the vertical axis (0.5 points).\n",
    "\n",
    "**Tip:**\n",
    "Use `np.tan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd260d62",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "761814153b5f03c26c636257ddd39a69",
     "grade": false,
     "grade_id": "equacao_dist_focal",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_focal_length(fov: float, sensor_width: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the focal length in pixels for a pinhole camera.\n",
    "    :param fov: field of view in degrees.\n",
    "    :param sensor_width: sensor width in pixels.\n",
    "    :return: the focal length in pixels.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return f_pinhole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee249da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3ef991be36f830db812c658eee8a1a1",
     "grade": true,
     "grade_id": "corrige_equacao_dist_focal",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(compute_focal_length(90, 1920) - 960) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdece06",
   "metadata": {},
   "source": [
    "In the exercise above, the focal length is in pixels, but if we want to convert it to meters, we will need to know the pixel size, or equivalently, the sensor width. Then, we can just make a unit conversion from pixels to meters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3321d4e",
   "metadata": {},
   "source": [
    "### Homogeneous Coordinates and Projective Geometry\n",
    "\n",
    "Let's go now to the projective model:\n",
    "\n",
    "To simplify, we consider the projection plane in front of the projection center.\n",
    "\n",
    "Notice that the result is equivalent: the projected image has exactly the same dimensions, and the focal length is the same in absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d52ba",
   "metadata": {},
   "source": [
    "![Câmera Pinhole Plano de projeção Frontal](https://gam.dev/cm203/imgs/pinhole_cam.svg)\n",
    "\n",
    "Pay attention to our camera coordinate system, where our origin is the Projection Center. Our projection plane is orthogonal to the Z axis, which points outward of the camera.\n",
    "\n",
    "![Câmera Pinhole Plano de projeção Frontal](https://gam.dev/cm203/imgs/pinhole_cam_proj.svg)\n",
    "\n",
    "\n",
    "A point in the real world $(X, Y, Z)$ measured in meters is projected to the image plane as: $(X_p, Y_p, Z_p) = (X\\frac{f_{metros}}{Z}, Y\\frac{f_{metros}}{Z}, f)$.\n",
    "\n",
    "\n",
    "![Câmera Pinhole Plano de projeção Frontal](https://gam.dev/cm203/imgs/pinhole_cam_proj_ext.svg)\n",
    "\n",
    "\n",
    "Since we have a division by $Z$, this is not a linear transform and we can't write it in matricial form. To solve this issue, we introduce the concept of homogenous coordinates.\n",
    "\n",
    "To transform a $N$-dimensional Cartesian point in its homogenous equivalent, we only need to include a new dimension with unit value. Inversely, we need to divide by this value, taking care with points that go to infinite (division by zero). Therefore, the 2D Cartesian point $(X, Y)$ becomes $(X, Y, 1)$ in homogeneous coordinates. Moreover, the homogeneous point $(X, Y, Z, W)$ becomes $(X/W, Y/W, Z/W)$ in the Cartesian space. Notice that infinite homogeneous points are mapped into the same Cartesian point.\n",
    "\n",
    "Therefore, in the exercises below (0.5 points each), implement the transform from Cartesian coordinates to homogeneous coordinates and vice-versa. Don't worry about points in the infinite.\n",
    "\n",
    "**Tip:**\n",
    "Use `np.vstack` (which receives a tuple of column vectors) to concatenate two arrays vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6916017",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a21add4ee4ff8d98bac5e47f4c60bac1",
     "grade": false,
     "grade_id": "coord_carte_homoge",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cartesian_to_homogeneous(cartesian: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transforms from Cartesian to homogeneous coordinates.\n",
    "    :param cartesian: column array in Cartesian coordinates.\n",
    "    :return: column array in homogeneous coordinates.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return homogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad7058",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05d4c9afd27596c675dad4aa8c5739aa",
     "grade": true,
     "grade_id": "testa_coord_carte_homoge",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.linalg.norm(cartesian_to_homogeneous(np.array([[2],\n",
    "                                                          [4]])) - np.array([[2],\n",
    "                                                                             [4],\n",
    "                                                                             [1]])) < 1e-6\n",
    "assert np.linalg.norm(cartesian_to_homogeneous(np.array([[19],\n",
    "                                                          [19],\n",
    "                                                          [19]])) - np.array([[19],\n",
    "                                                                              [19],\n",
    "                                                                              [19],\n",
    "                                                                              [1 ]])) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c09e1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a21e7885f4454e2b58d972e1911f835b",
     "grade": false,
     "grade_id": "coord_homoge_carte",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def homogeneous_to_cartesian(homogeneous: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transforms from homogeneous to Cartesian coordinates.\n",
    "    This function doesn't take into account points in infinite.\n",
    "    :param homogeneous: column array in homogeneous coordinates.\n",
    "    :return: column array in Cartesian coordinates.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254c6a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52f536a4ddf9409702a989736ea5f931",
     "grade": true,
     "grade_id": "testa_coord_homoge_carte",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.linalg.norm(homogeneous_to_cartesian(np.array([[4],\n",
    "                                                          [8],\n",
    "                                                          [2]])) - np.array([[2],\n",
    "                                                                             [4]])) < 1e-6\n",
    "assert np.linalg.norm(homogeneous_to_cartesian(np.array([[19],\n",
    "                                                          [19],\n",
    "                                                          [19],\n",
    "                                                          [19]])) - np.array([[1],\n",
    "                                                                              [1],\n",
    "                                                                              [1]])) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79185af4",
   "metadata": {},
   "source": [
    "However, we don't want to know where the point is in meters, we want to know where the point in is the image in pixels. Therefore, let's use a simple rule of three (linear transform):\n",
    "\n",
    "We first need to know where the point $(0, 0, f_{meters})$ of the projection plane is in the image in pixels. We define this position as the center of the image: $(c_x, c_y)$. Then, we need to know the relationship between pixels and meters to transform the focal length in meters to pixels, i.e., we need the pixel size $\\mu$.\n",
    "\n",
    "Therefore, our 3D point $(X_p, Y_p, Z_p)$ measured in meters becomes a 2D point $\\left( c_x + X_p \\mu, c_y + Y_p \\mu \\right)$ measured in pixels. Substituting, we have $\\left(c_x + X\\frac{f_{metros}}{Z}\\mu, c_y + Y\\frac{f_{metros}}{Z}\\mu\\right)$.\n",
    "\n",
    "Notice that the term $\\mu f_{metros}$ is the focal length in pixels, which we'll call now simply as $f$.\n",
    "\n",
    "Therefore, we have a mapping from a 3D point $(X, Y, Z)$ in meters to $\\left(c_x + X\\frac{f}{Z}, c_y + Y\\frac{f}{Z}\\right)$, where the values $c_x$, $c_y$, and $f$ are measured in pixels.\n",
    "\n",
    "Implement the perspective projection below (0.5 pontos). The output is given by $(f X + c_x Z, f Y + c_y Z, Z)$.\n",
    "\n",
    "**Tip:**\n",
    "Use the operator `@` from NumPy for matrix multiplication `A @ B`, which is equivalent to `np.dot(A, B)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c611651",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7f89de7157de00bfdbbce1735d41f07",
     "grade": false,
     "grade_id": "transf_proj",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def projective_transform(point3d: np.ndarray, c_x: float, c_y: float, f: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Projects a point from space to a point in the image plane represented with homogeneous coordinates.\n",
    "    :param c_x: x coordinate of the image center, in pixels.\n",
    "    :param c_y: y coordinate of the image center, in pixels.\n",
    "    :param f: focal length in pixels.\n",
    "    :return: a column array in homogeneous coordinates and the transform matrix (intrinsic matrix).\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return homogeneous_point, intrinsic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ad572",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30390a710c759ab63a56d2a5ac8ef953",
     "grade": true,
     "grade_id": "test_transf_proj",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "point, mtx = projective_transform(np.array([[1],[1],[1]]), 1, 1, 1) # Tudo 1 para não espiar :)\n",
    "assert np.linalg.norm(mtx/mtx[-1,-1] - np.array([[1, 0, 1], [0, 1, 1], [0, 0, 1]])) < 1e-6\n",
    "assert np.linalg.norm(homogeneous_to_cartesian(point) - np.array([[2], [2]])) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3e226",
   "metadata": {},
   "source": [
    "Beyond the previous mapping which takes to $\\left(c_x + X\\frac{f}{Z}, c_y + Y\\frac{f}{Z}\\right)$, we can also consider a focal length different for each axes $x$ and $y$. This may happen in practice if the pixel sensor is not squared. In this setting, we call *aspect ratio* $f_y = a f_x$.\n",
    "\n",
    "Another effect is the image *skew*, which can model the case where the sensor lines are misaligned, making a parallelogram instead of a rectangle.\n",
    "\n",
    "Therefore, a more complete model would be: $\\left(c_x + \\frac{X f_x + Y skew}{Z}, c_y + Y\\frac{f_y}{Z}\\right)$. In the function below, define a new intrinsic matrix which takes into account the effects discussed here (0.5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8897588",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b4bbe4af7acd98687b3fdc0a4579d37",
     "grade": false,
     "grade_id": "mtx",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def intrinsic_matrix_skew(c_x: float, c_y: float, f_x: float, f_y: float, skew: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Builds a intrinsic matrix for a camera defined by the parameters c_x, c_y, f_x, f_y, skew.\n",
    "    Consider that the output is in homogeneous coordinates.\n",
    "    :param c_x: x coordinate of the image center, em pixels.\n",
    "    :param c_y: y coordinate of the image center, em pixels.\n",
    "    :param f_x: focal length in pixels for the axis x.\n",
    "    :param f_y: focal length in pixels for the axis y.\n",
    "    :param skew: skew coefficient, in pixels.\n",
    "    :return: the intrinsic matrix which considers that f_x != f_y and the existence of a skew coefficient.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return intrinsic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720dadf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf2136d599aba2095aa00fcb1fac560c",
     "grade": true,
     "grade_id": "test_mtx",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mtx = intrinsic_matrix_skew(1, 1, 1, 1, 1) # Tudo 1 para não espiar :) mas temos mais testes, cuida\n",
    "assert np.linalg.norm(mtx - np.array([[1, 1, 1], [0, 1, 1], [0, 0, 1]])) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346d359",
   "metadata": {},
   "source": [
    "## Perspective-n-Point\n",
    "\n",
    "Let's do a direct application of Perspective-n-Point (PnP). The PnP algorithm finds the pose (rotation and translation) which minimize the reprojection error, given the intrinsic parameters of the camera (which are known)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a97619",
   "metadata": {},
   "source": [
    "Find the pose of the camera using the function `cv2.solvePnP`. To understand how to use this function, see its documentation in the next cell. \n",
    "Moreover, use the flag `cv2.SOLVEPNP_EPNP`, because the default one, which is `cv2.SOLVEPNP_ITERATIVE`, requires more points if a initial guess for `[R, T]` is not given (1 point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.solvePnP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3249d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b58f470ce2f58074a60702722811bcb5",
     "grade": false,
     "grade_id": "pnp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pnp(verts3d, verts2d, camera_params):\n",
    "    \"\"\"\n",
    "    Obtaints the pose of an object in space, given its geometry, the corresponding 2D points, and\n",
    "    the intrinsic parameters of the camera.\n",
    "    :param verts3d: the coordinates of the 3D points of the object in its own coordinate system, in meters.\n",
    "    :param verts2d: the corresponding 2D points in the image, in pixels.\n",
    "    :param camera_parameters: tuple with the intrinsic matrix and the distortion coefficients.\n",
    "    :return: the reprojection error and the rotation (in Rodrigues notation, in radians) and translation vectors.\n",
    "    \"\"\"\n",
    "    intrinsic_matrix, distortion_coeffs = camera_params\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return reproj_error, rvec, tvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d0fa5",
   "metadata": {},
   "source": [
    "Notice that in the test below, we start from random pose and geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f792a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7384470a8797700e958a13d3108f5c83",
     "grade": true,
     "grade_id": "testa_pnp",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "verts3d = np.array([[-1,-1,0],[-1,1,0],[1,1,0],[1,-1,0],[0,0,2]], dtype=np.float32)\n",
    "mtx = np.array([[1000,0,1000],[0,1000,500],[0,0,1]], dtype=np.float32)\n",
    "dist = np.zeros((5,), dtype=np.float32)\n",
    "rvec = np.array([[0.3], [-0.2], [0.6]])\n",
    "tvec = np.array([[0.1], [-0.2], [7]])\n",
    "verts2d, _ = cv2.projectPoints(verts3d, rvec, tvec, mtx, dist)\n",
    "reproj_error, rvec_rec, tvec_rec = pnp(verts3d, verts2d, (mtx, dist))\n",
    "assert np.linalg.norm(R.from_rotvec(rvec.ravel()).as_matrix() - R.from_rotvec(rvec_rec.ravel()).as_matrix()) < 0.01\n",
    "assert np.linalg.norm(tvec - tvec_rec) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfcc76f",
   "metadata": {},
   "source": [
    "Let's visualize the projection of these points in space. Feel free to change the values of rvec, tvec, and the intrinsic matrix.\n",
    "\n",
    "We are only connecting the projected points with green lines so we can have an idea of the object in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "f = 1000 * 1\n",
    "d = 4 * 1\n",
    "mtx = np.array([[f,0,1000],[0,f,500],[0,0,1]], dtype=np.float32)\n",
    "rvec = np.array([[0.7], [0.3], [0]], dtype=np.float32)\n",
    "tvec = np.array([[0.1], [0.8], [d]])\n",
    "projected_points, _ = cv2.projectPoints(verts3d, rvec, tvec, mtx, dist)\n",
    "for i, p1 in enumerate(projected_points):\n",
    "    if i + 2 < len(projected_points):\n",
    "        p2 = projected_points[i+1].astype(np.uint).ravel()\n",
    "    else:\n",
    "        p2 = projected_points[0].astype(np.uint).ravel()\n",
    "    cv2.line(img, p1.astype(np.uint).ravel(), p2, (0, 255, 0))\n",
    "    cv2.line(img, p1.astype(np.uint).ravel(), projected_points[-1].astype(np.uint).ravel(), (0, 255, 0))\n",
    "    cv2.circle(img, p1.astype(np.uint).ravel(), 4, (0, 255, 0))\n",
    "cv2.drawFrameAxes(img, mtx, dist, rvec, tvec, 1)\n",
    "\n",
    "PIL.Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8334c5",
   "metadata": {},
   "source": [
    "## Stereo Calibration\n",
    "\n",
    "In stereo calibration, we wan to find the camera pose with respect to another camera. In this case, we can use the intrinsic parameters already estimated in the monocular calibration (but they can also be refined if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca45029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verts2d(path, nverts, criteria):\n",
    "    pathnames = [str(p) for p in sorted(path.glob('*.png'))]\n",
    "    imgs = [cv2.cvtColor(cv2.imread(p)[:,:,0], cv2.COLOR_BAYER_GB2GRAY) for p in pathnames]\n",
    "    verts2d = [cv2.findChessboardCorners(img, nverts) for img in imgs]\n",
    "    return [cv2.cornerSubPix(img, v, (7,7), (-1,-1), criteria) if r else None for img, (r, v) in zip(imgs, verts2d)]\n",
    "\n",
    "image_size = cv2.imread(str(imgs_left_path/'00.png')).shape[1::-1]\n",
    "nverts = (10, 7)\n",
    "\n",
    "l_square = 0.1 # metros\n",
    "verts3d = np.zeros((nverts[0] * nverts[1], 3), dtype=np.float32)\n",
    "i = 0\n",
    "for y in range(nverts[1]):\n",
    "    for x in range(nverts[0]):\n",
    "        verts3d[i] = (x * l_square, y * l_square, 0)\n",
    "        i += 1\n",
    "\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 50, 5e-4)\n",
    "verts2d_left = get_verts2d(imgs_left_path, nverts, criteria)\n",
    "verts2d_right = get_verts2d(imgs_right_path, nverts, criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b206b",
   "metadata": {},
   "source": [
    "You'll now do a stereo calibration to find the rotation and translation from one camera with respect to the other one. To do this, use the function `cv2.stereoCalibrateExtended`. Notice that the input `objectPoints` of the OpenCV function is a list of arrays (representing a set of the 3D points) for each image, while `verts3d` in our function is a single array. Moreover, take into account that we may not detect points in some images, so you may need to filter some elements of `verts2d` to keep only situations where corresponding points are found on the left and right images.\n",
    "The stereo calibration is also able to estimate intrinsic parameters, but it is usually better to find these parameters using a monocular calibration. The default flag cv2.CALIB_FIX_INTRINSIC already considers fixed intrinsic parameters during the calibration. For the inputs R and T, we can use `None` or give null vectors (1.5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.stereoCalibrateExtended?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd0295",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a7da385799216c1d9e91c80169f738b",
     "grade": false,
     "grade_id": "calib_stereo",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def stereo_calibration(verts3d, verts2d_left, verts2d_right, cam_left, cam_right, image_size):\n",
    "    \"\"\"\n",
    "    Executes a stereo calibration for a pair of cameras, without optimizing the intrinsic parameters.\n",
    "    :param verts3d: the coordinates of the 3D points in its own coordinate system.\n",
    "    :param verts2d_left: list of 2D points found in the images captured by the left camera.\n",
    "    :param verts2d_right:  list of 2D points found in the images captured by the right camera.\n",
    "    :param cam_left: tuple with the intrinsic matrix and distortion coefficients of the left camera.\n",
    "    :param cam_right: tuple with the intrinsic matrix and distortion coefficients of the right camera.\n",
    "    :param image_size: tuple with the image size in pixels.\n",
    "    :return: the mean reprojection error of the calibration, the rotation matrix and the translation vector that\n",
    "    takes the 3D points from the left camera's coordinate system to the right one, the essential matrix, the\n",
    "    fundamental matrix, and the reprojection error for each image.\n",
    "    \"\"\"\n",
    "    ml, dl = cam_left\n",
    "    mr, dr = cam_right\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return mean_error, rotation, translation, essential, fundamental, per_view_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ed381",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e714c810b1edf3863b20eb4e213c2d4",
     "grade": true,
     "grade_id": "testa_calib_stereo",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cam_left = (np.array([[1.27677228e+03, 0.00000000e+00, 9.46863329e+02],\n",
    "                     [0.00000000e+00, 1.28093814e+03, 5.26800967e+02],\n",
    "                     [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]], dtype=np.float32),\n",
    "    np.array([ 0.00463763,  0.00506855, -0.00252541, -0.00054011,  0.        ], dtype=np.float32))\n",
    "cam_right = (np.array([[1.28316167e+03, 0.00000000e+00, 9.83705999e+02],\n",
    "                     [0.00000000e+00, 1.28959033e+03, 4.74866554e+02],\n",
    "                     [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]),\n",
    "    np.array([ 0.00438509,  0.01102175, -0.00740251,  0.00029485,  0.        ], dtype=np.float32))\n",
    "err, rmat, T, E, F, per_view_error = stereo_calibration(verts3d, verts2d_left[:-6], \n",
    "                                                        verts2d_right[:-6], cam_left, cam_right, image_size)\n",
    "assert err < 0.7\n",
    "assert np.mean(per_view_error) < 0.7\n",
    "assert per_view_error.shape == (28, 2)\n",
    "assert np.linalg.norm(rmat - np.array([[ 0.9330993 , -0.00167611,  0.35961491],\n",
    "                                       [-0.00388421,  0.99988383,  0.01473875],\n",
    "                                       [-0.35959784, -0.01514954,  0.9329844 ]])) < 1e-5\n",
    "assert np.linalg.norm(T - np.array([[-9.54630632e-01],[ 3.48152961e-04],[ 1.50440555e-01]])) < 1e-5\n",
    "assert np.linalg.norm(E - np.array([[ 4.59148223e-04, -1.50428353e-01, -1.89248456e-03],\n",
    "                                    [-2.02907133e-01, -1.47143694e-02,  9.44756152e-01],\n",
    "                                    [ 3.38312816e-03, -9.54519153e-01, -1.41952640e-02]])) < 1e-5\n",
    "assert np.linalg.norm(F - np.array([[-3.71746733e-09,  1.21397368e-06, -6.16439367e-04],\n",
    "                                    [ 1.63463644e-06,  1.18154655e-07, -1.13275774e-02],\n",
    "                                    [-8.07724752e-04,  8.63399180e-03,  1.00000000e+00]])) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6de5d",
   "metadata": {},
   "source": [
    "## Stereo Retification\n",
    "\n",
    "Find homographies for each one of the cameras so we have parallel epipolar lines (epipoles at the infinity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28903ba6",
   "metadata": {},
   "source": [
    "Use the function `cv2.stereoRectify` to find the retification matrix for each one of the cameras. Beyond this homography, it also updates the projection matrix (i.e. the matrix that results from the multiplication between intrinsic and extrinsic matrices `K[RT]`) (1.5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.stereoRectify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a6d54",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfbada0c0321b9c60649ff0015297d8d",
     "grade": false,
     "grade_id": "retifica",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def stereo_retification(cam_left, cam_right, image_size, rotation, translation):\n",
    "    \"\"\"\n",
    "    Executes the stereo retification to make the epipolar lines parallel to the projection of the horizontal lines.\n",
    "    :param cam_left: tuple with the intrinsic matrix and the distortion coefficients of the left camera.\n",
    "    :param cam_right: tuple with the intrinsic matrix and distortion coefficients of the right camera.\n",
    "    :param image_size: tuple with the image size in pixels.\n",
    "    :param rotation: rotation matrix of the left camera measured in the coordinate system of right one.\n",
    "    :param translation: left camera's position measured in the coordinate system of right one.\n",
    "    :return: the left and right retification matrices, the left and right projection matrices, the depth-to-disparity\n",
    "             mapping matrix.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return retification_left, retification_right, projection_left, projection_right, Q\n",
    "\n",
    "def apply_mapping(image, camera_params, retification_matrix, projection_matrix, image_size):\n",
    "    mappings = cv2.initUndistortRectifyMap(*camera_params, retification_matrix, projection_matrix,\n",
    "                                           image_size, cv2.CV_32FC1)\n",
    "    return cv2.remap(image, *mappings, cv2.INTER_LANCZOS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc96eb1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3260b68bbcc6a23def4066b2d54cb6a",
     "grade": true,
     "grade_id": "testa_retifica",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "R1, R2, P1, P2, Q = stereo_retification(cam_left, cam_right, image_size, rmat, T)\n",
    "assert np.linalg.norm(R1 - np.array([[ 9.77703755e-01,  3.42425370e-04,  2.09988690e-01],\n",
    "                                     [-1.94226256e-03,  9.99970641e-01,  7.41250433e-03],\n",
    "                                     [-2.09979987e-01, -7.65508649e-03,  9.77675715e-01]])) < 1e-5\n",
    "assert np.linalg.norm(P1 - np.array([[1.28526422e+03, 0.00000000e+00, 9.02986206e+02, 0.00000000e+00],\n",
    "                                     [0.00000000e+00, 1.28526422e+03, 4.93165016e+02, 0.00000000e+00],\n",
    "                                     [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00]])) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e694716",
   "metadata": {},
   "source": [
    "Observe the images after retification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496499f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_left = cv2.cvtColor(cv2.imread(str(imgs_left_path/'23.png'), cv2.IMREAD_ANYCOLOR), cv2.COLOR_BAYER_GB2RGB)\n",
    "img_right = cv2.cvtColor(cv2.imread(str(imgs_right_path/'23.png'), cv2.IMREAD_ANYCOLOR), cv2.COLOR_BAYER_GB2RGB)\n",
    "\n",
    "img_left_ret_full = apply_mapping(img_left, cam_left, R1, P1, image_size)\n",
    "img_right_ret_full = apply_mapping(img_right, cam_right, R2, P2, image_size)\n",
    "img_left_ret_full = np.clip(img_left_ret_full.astype(np.float32)*1.3, 0, 255).astype(np.uint8)\n",
    "img_right_ret_full = np.clip(img_right_ret_full.astype(np.float32)*1.3, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f88ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.fromarray(img_left_ret_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.fromarray(img_right_ret_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6e50b",
   "metadata": {},
   "source": [
    "## Disparity\n",
    "\n",
    "Let's now use a correlator that uses both retified images, i.e. it supposes that the epipolar constraint happens only in the horizontal lines (horizontal and parallel epipolar lines).\n",
    "\n",
    "Feel free to change the parameters below, especially the values of `blockSize` which defines the size of the correlation window and `speckleWindowSize` and `speckleRange` which filter spurious points. Notice that the cameras are separated by 1 meter (and they were also inclined), the disparity gets to the order of 500 pixels for the closest point, which can be seen by `numDisparities=16*32` = 512 (160 after the resize). Only for points at the infinity we'll have `minDisparity=0`.\n",
    "\n",
    "Notice that due to the lack of texture in the image, especially on the ground and on the wall, most points don't have a defined depth, because the algorithm couldn't find its match. Since the disparity are also large the algorithm finds many spurious correlations.\n",
    "\n",
    "Moreover, we also need to reduce the image size to have less noisy results. Try to experiment with other values of `s`, which incrase or reduce the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae76f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.22\n",
    "img_left_ret = cv2.resize(img_left_ret_full, None, None, s, s, cv2.INTER_AREA)\n",
    "img_right_ret = cv2.resize(img_right_ret_full, None, None, s, s, cv2.INTER_AREA)\n",
    "print(img_left_ret.shape)\n",
    "PIL.Image.fromarray(img_left_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlator = cv2.StereoSGBM_create(minDisparity=0, numDisparities=int(16*32*s), blockSize=11, \n",
    "                  P1=8*3*11*11, P2=32*3*11*11, speckleWindowSize = 50, speckleRange = 2, disp12MaxDiff=20)\n",
    "corr_left = correlator.compute(img_left_ret, img_right_ret)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.axis(False)\n",
    "plt.imshow(corr_left, cmap='gray', vmin=0)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de35952",
   "metadata": {},
   "source": [
    "We can reapply the correlator with the images switched and later use a filter between the antipode results.\n",
    "\n",
    "Notice that the colors are inverted, in reality this happens because the disparity that was previously positive is now negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44130af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_correlator = cv2.ximgproc.createRightMatcher(correlator)\n",
    "corr_right = inverse_correlator.compute(img_right_ret, img_left_ret)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.axis(False)\n",
    "plt.imshow(corr_right, cmap='gray', vmax=0)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384172c",
   "metadata": {},
   "source": [
    "Applying the filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = cv2.ximgproc.createDisparityWLSFilter(matcher_left=correlator)\n",
    "filt.setLambda(1300)\n",
    "filt.setSigmaColor(1.9)\n",
    "filtered_result = filt.filter(np.int16(corr_left), img_left_ret, None, np.int16(corr_right))\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.axis(False)\n",
    "plt.imshow(filtered_result, cmap='gray',vmin=-16*16*2)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e2986",
   "metadata": {},
   "source": [
    "For an image with more texture and smaller disparities (in the order of 16 pixels):\n",
    "\n",
    "(Obs.: the image is already retified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "ileft = cv2.imread(str(tsukuba_path/\"tsukuba_l.png\"), cv2.IMREAD_ANYCOLOR)\n",
    "iright = cv2.imread(str(tsukuba_path/\"tsukuba_r.png\"), cv2.IMREAD_ANYCOLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0643ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlator = cv2.StereoSGBM_create(minDisparity=-16, numDisparities=32, blockSize=9, \n",
    "                  P1=4*3*9*9, P2=16*3*9*9, speckleWindowSize = 50, speckleRange = 1, disp12MaxDiff=20)\n",
    "\n",
    "corr_left = correlator.compute(ileft, iright)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.axis(False)\n",
    "plt.imshow(corr_left, cmap='gray', vmin=0)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59afee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08a9cbd4",
   "metadata": {},
   "source": [
    "# Your data and feedback:\n",
    "\n",
    "Write a feedback for the lab so we can make it better for the next years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ae2aba",
   "metadata": {},
   "source": [
    "In the following variables, write the number of hours spent on this lab, the perceived difficulty, and the expected grade (you may delete the `raise` and the comments):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2c5e2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36f613d80a345c28aaf937672ec9e9f5",
     "grade": true,
     "grade_id": "meta_eval",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# meta_eval manual_graded_answer 0\n",
    "\n",
    "horas_gastas = None    # 1.5   - Float number with the number of hours spent \n",
    "dificuldade_lab = None # 0     - Float number from 0.0 to 10.0 (inclusive)\n",
    "nota_esperada = None   # 10    - Float number from 0.0 to 10.0 (inclusive)\n",
    "\n",
    "# WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1594e",
   "metadata": {},
   "source": [
    "Write below other comments or feedbacks about the lab. If you did not understand anything about the lab, please also comment here.\n",
    "\n",
    "If you find any typo or bug in the lab, please comment below so we can fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa846d1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc6431aff9a971fe5ed82c62bc668c96",
     "grade": true,
     "grade_id": "meta_eval_discursivo",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "WRITE YOUR SOLUTION HERE! (do not change this first line):\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**DISCURSIVE QUESTION**\n",
    "\n",
    "WRITE YOUR ANSWER HERE (do not delete this cell so the ID is not lost)\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e7d00",
   "metadata": {},
   "source": [
    "**End of the lab!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
