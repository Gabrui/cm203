{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4705a94",
   "metadata": {},
   "source": [
    "**Aeronautics Institute of Technology ‚Äì ITA**\n",
    "\n",
    "**Computer Vision ‚Äì CM-203**\n",
    "\n",
    "**Professors:** \n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "Gabriel Adriano de Melo\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Before submitting your lab, be sure that everything is running correctly (in sequence): first, **restart the kernel** (`Runtime->Restart Runtime` in Colab or `Kernel->Restart` in Jupyter). Then, execute all cells (`Runtime->Run All` in Colab or `Cell->Run All` in Jupyter) and verifies that all cells run without any errors, expecially the automatic grading ones, i.e. the ones with `assert`s.\n",
    "\n",
    "**Do not delete the answer cells**, i.e. the ones that contains `WRITE YOUR CODE HERE` or `WRITE YOUR ANSWER HERE`, because they contain metadata with the ids of the cells for the grading system. For the same reason, **do not delete the test cells**, i.e. the ones with `assert`s. The autograding system executes all the code sequentially, adding extra tests in the test cells. There is no problem in creating new cells, as long as you do not delete answer or test cells. Moreover, keep your solutions within the reserved spaces.\n",
    "\n",
    "The notebooks are implemented to be compatible with Google Colab, and they install the dependencies and download the datasets automatically. The commands which start with ! (exclamation mark) are bash commands and can be executed in a Linux terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e916a00",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a26919",
   "metadata": {},
   "source": [
    "# Visual Attention Laboratory\n",
    "\n",
    "This is a simplified implementation and verification of a convolutional attention model, a Transformer head, and inference using diffusion by a generative model. Therefore, you can exercise the main concepts of the attention lecture.\n",
    "\n",
    "You will need a GPU with CUDA to execute the Stable Diffusion (you have access to this in Colab). To use the GPU in Colab, go to `Edit > Notebook setting` or `Runtime > Change runtime type` and select `GPU` as the acceleration hardware. For automatic evaluation, we are going to only execute the `assert`s, so you don't need to worry if you aren't able to run this using a GPU in Colab.\n",
    "\n",
    "This lab was originally created by Gabriel Melo. Marcos Maximo translated it to English and made some minor improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66760b45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64339e8b15b3e6af615a7d61f11ea9a5",
     "grade": false,
     "grade_id": "imports-baixar",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install -Uq diffusers==0.7.2 transformers==4.24.0 fastcore==1.5.29 # opencv-contrib-python==4.6.0.66 torch==1.12.1\n",
    "# Googledrive blocked the download of a large file, but you can download manually and move to the folder if you prefer\n",
    "# You can later define local_files_only=True in model.from_pretrained\n",
    "#! [ ! -d ~/.cache/huggingface ] && mkdir -p ~/.cache/huggingface && gdown -O ~/.cache/models.tar 1S2cvcS-XlNZFam5kLSwfyWFcViA-on80 && tar -xf ~/.cache/models.tar -C ~/.cache/ && rm ~/.cache/models.tar\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler, DDPMScheduler\n",
    "import logging\n",
    "import warnings\n",
    "hf_token = '' # PUT HERE YOUR Token to download from Hugging Face\n",
    "execute_transformer = False\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b487247",
   "metadata": {},
   "source": [
    "## Convolutional Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452212b",
   "metadata": {},
   "source": [
    "For images, we have the so-called spatial attention. This is used in many state-of-the-art convolutional models:\n",
    "\n",
    "$\\textbf{M}_{s}\\left(F\\right) = \\sigma\\left(f^{7x7}\\left(\\left[\\text{AvgPool}\\left(F\\right);\\text{MaxPool}\\left(F\\right)\\right]\\right)\\right)$\n",
    "\n",
    "$\\textbf{M}_{s}\\left(F\\right) = \\sigma\\left(f^{7x7}\\left(\\left[\\mathbf{F}^{s}_{avg};\\mathbf{F}^{s}_{max} \\right]\\right)\\right)$\n",
    "\n",
    "![Spatial Attention Module](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.27_PM_CjrAZaI.png)\n",
    "\n",
    "Implement the function below, which is related to the spatial attention module (SAM) of the CBAM model from [Woo et. al (2018)](https://arxiv.org/abs/1807.06521). (3 points)\n",
    "\n",
    "Use the following operations:\n",
    "- `torch.cat`: concatenate two or more tensors;\n",
    "- `torch.max`: compute the maximum value across a dimension;\n",
    "- `torch.mean`: compute the average across a dimension;\n",
    "- `torch.sigmoid`: apply the sigmoid activation function to each element of the tensor (element-wise).\n",
    "\n",
    "Verify Pytorch's documentation for each function so you better understand how to use them.\n",
    "When using `torch.max` and `torch.mean`, you can change the shape of the tensor. To correct this, you can use `.unsqueeze`/`.view`/`.reshape`/`tensor[:, None, :, :]` or you can use the parameter `keepdim` as `True` to keep the dimension.\n",
    "\n",
    "To multiply two tensors element-wise, use `*` or `torch.multiply`.\n",
    "\n",
    "To execute the convolutional operation, use it as a function call, i.e. as `operation(input)`. \n",
    "<details><summary><b>---Hint---</b></summary>\n",
    "<p>\n",
    "In SAM, the average and maximum values are computed across the channels. In a tensor of shape (N, C, H, W), the channels are at the dimension 1 (C).\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e707af5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad24ff2c491c8f6d41d8530f3289edc8",
     "grade": false,
     "grade_id": "atencao-conv",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def spatial_attention(convolutional_operation: nn.Module, features: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Implements the spatial attention module (SAM) by appplying an attention map to the\n",
    "    features in a multiplicative form. Obs.: concatenates the Avg before the Max.\n",
    "    \n",
    "    :param convolutional_operation: convolutional operation (using a filter ùëì) which \n",
    "                                         receives an image with two channels as input\n",
    "                                         and returns an image with a single channel.\n",
    "    :param features: tensor of shape (N, C, H, W), where N is the number of batches,\n",
    "                             C is the number of channels, H is the height, and W is the width.\n",
    "    return: tensor of shape (N, C, H, W), the resulting tensor after the spatial attention operation.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144a288",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f99e5dc3d8ef405547d30d5053702d4",
     "grade": true,
     "grade_id": "testa-atencao-conv",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "conv.load_state_dict(OrderedDict([('weight', torch.tensor([[[ [-0.0165, -0.0119, -0.1987],\n",
    "                                                              [ 0.1247, -0.0875,  0.0907],\n",
    "                                                              [-0.1658,  0.2204,  0.1959]],\n",
    "                                                             [[-0.0925, -0.1122, -0.2284],\n",
    "                                                              [ 0.0141,  0.0105, -0.0061],\n",
    "                                                              [ 0.0526,  0.1581, -0.1757]] ]])),\n",
    "                                  ('bias', torch.tensor([0.0675]))]))\n",
    "features = torch.tensor([[[[-0.0165, -0.0119, -0.1987, 1, 1, 1, 1],\n",
    "                           [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                           [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                           [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                           [-0.1658,  0.2204,  0.1959, 1, 1, 1, 1]],\n",
    "                          [[-0.0165, -0.0119, -0.1987, 1, 1, 1, 1],\n",
    "                           [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                           [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                           [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                           [-0.1658,  0.2204,  0.1959, 1, 1, 1, 1]],\n",
    "                          [[-0.0165, -0.0119, -0.1987, 1, 1, 1, 1],\n",
    "                           [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                           [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                           [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                           [-0.1658,  0.2204,  0.1959, 1, 1, 1, 1]]\n",
    "                        ]])\n",
    "result = spatial_attention(conv, features)\n",
    "assert result.requires_grad\n",
    "assert torch.norm(result - torch.tensor(\n",
    "       [[[[-0.0087, -0.0060, -0.1107,  0.6073,  0.6223,  0.6223,  0.5974],\n",
    "          [ 0.0656, -0.0469,  0.0407,  0.4866,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0660, -0.0440,  0.0401,  0.4788,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0628, -0.0473,  0.0402,  0.4758,  0.4599,  0.4599,  0.5403],\n",
    "          [-0.0879,  0.1103,  0.0852,  0.3873,  0.3902,  0.3902,  0.4741]],\n",
    "\n",
    "         [[-0.0087, -0.0060, -0.1107,  0.6073,  0.6223,  0.6223,  0.5974],\n",
    "          [ 0.0656, -0.0469,  0.0407,  0.4866,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0660, -0.0440,  0.0401,  0.4788,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0628, -0.0473,  0.0402,  0.4758,  0.4599,  0.4599,  0.5403],\n",
    "          [-0.0879,  0.1103,  0.0852,  0.3873,  0.3902,  0.3902,  0.4741]],\n",
    "\n",
    "         [[-0.0087, -0.0060, -0.1107,  0.6073,  0.6223,  0.6223,  0.5974],\n",
    "          [ 0.0656, -0.0469,  0.0407,  0.4866,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0660, -0.0440,  0.0401,  0.4788,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0628, -0.0473,  0.0402,  0.4758,  0.4599,  0.4599,  0.5403],\n",
    "          [-0.0879,  0.1103,  0.0852,  0.3873,  0.3902,  0.3902,  0.4741]]]])).item() < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1bdde",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7386c63",
   "metadata": {},
   "source": [
    "An attention model initially proposed for Natural Language Processing (NLP) to allow highly parallelizable operations and facilitate the training of long sequences. It has already been adapted to Computer Vision through the Vision Transformer (ViT), which essentially treats the image as a sequence of patches.\n",
    "\n",
    "![Transformer Architecture](https://lilianweng.github.io/posts/2018-06-24-attention/transformer.png)\n",
    "\n",
    "The Transformer was initially proposed by [Vaswani et. al (2017)](https://arxiv.org/abs/1706.03762v5), following the equations below:\n",
    "\n",
    "$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{n}})\\mathbf{V}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= [\\text{head}_1; \\dots; \\text{head}_h]\\mathbf{W}^O \\\\\n",
    "\\text{where head}_i &= \\text{Attention}(\\mathbf{Q}\\mathbf{W}^Q_i, \\mathbf{K}\\mathbf{W}^K_i, \\mathbf{V}\\mathbf{W}^V_i)\n",
    "\\end{aligned}$\n",
    "\n",
    "Implement the attention head of the Transformer. (4 points)\n",
    "\n",
    "Use the function `.transpose(dim1, dim2)` to transpose the dimensions 1 and 2 of a tensor. To do a regular matrix multiplication across the last two dimensions of a tensor, use the operator `@` or the function `torch.matmul`. To obtain the dimensions of a tensor, use `.shape ` or `.size()`.\n",
    "<details><summary><b>---Hint---</b></summary>\n",
    "<p>\n",
    "- To use the projection operations, use them as functions, similarly to what you have done in the previous exercise.\n",
    "- Use `tensor.transpose(-1, -2)` to transpose the last two dimensions of a tensor.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19c84c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e5af8cd54f03f86b2ef0dbf76f4ae53",
     "grade": false,
     "grade_id": "transformer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def transformer_head(q: torch.tensor, k: torch.tensor, v: torch.tensor, \n",
    "                     proj_q: nn.Module, proj_k: nn.Module, proj_v: nn.Module, \n",
    "                     reproj: nn.Module, softmax: nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the head of the Transformer architecture given by equation\n",
    "    (softmax((q' @ k'.T) / sqrt(D_k)) @ v')' where ' represents the operation of linear\n",
    "    projection for each tensor on its vector space.\n",
    "    \n",
    "    :param q: query tensor of shape (N, L, D).\n",
    "    :param k: key tensor of shape (N, L, D).\n",
    "    :param v: value tensor of shape (N, L, D).\n",
    "    :param proj_q: operation that does a linear projection of a tensor (N, L, D) on \n",
    "                   H vector spaces of dimension D_k, resulting in a tensor (N, H, L, D_k).\n",
    "    :param proj_k: operation that does a linear projection of a tensor (N, L, D) on\n",
    "                   H vector spaces of dimension D_k, resulting in a tensor (N, H, L, D_k).\n",
    "    :param proj_v: operation that does a linear projection of a tensor (N, L, D) on\n",
    "                   H vector spaces of dimension D_v, resulting in a tensor (N, H, L, D_v).\n",
    "    :param reproj: operation that does a concatenation and linear projection of a tensor \n",
    "                   (N, H, L, D_v) of dimension E2, resulting in a tensor (N, L, D).\n",
    "    :param softmax: softmax operation over the tensor last dimension.\n",
    "    :return: tensor of shape (N, L, D), final result of the attention module.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return result\n",
    "\n",
    "class Projection(nn.Linear):\n",
    "    def __init__(self, dim_in, num_projs, dim_out, **kwargs):\n",
    "        super().__init__(dim_in, num_projs * dim_out, **kwargs)\n",
    "        self.num_projs = num_projs\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_flat = super().forward(x)\n",
    "        return x_flat.reshape(*x_flat.shape[:2], self.num_projs, -1).transpose(1, 2)\n",
    "\n",
    "class ReProjection(nn.Linear):\n",
    "    def __init__(self, dim_in, num_projs, dim_out, **kwargs):\n",
    "        super().__init__(dim_in * num_projs, dim_out, **kwargs)\n",
    "        self.num_projs = num_projs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_flat = x.transpose(1, 2).contiguous()\n",
    "        return super().forward(x_flat.reshape(*x_flat.shape[:2], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a6b90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2691e759e4a60cfd70784cbb2a65f8d",
     "grade": true,
     "grade_id": "testa-transformer",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=-1)\n",
    "proj_q = Projection(4, 2, 3)\n",
    "proj_k = Projection(4, 2, 3)\n",
    "proj_v = Projection(4, 2, 5)\n",
    "reproj = ReProjection(5, 2, 4)\n",
    "proj_q.load_state_dict(OrderedDict([('weight', torch.tensor([[-0.2903,  0.1144, -0.2388,  0.3808],\n",
    "                      [ 0.4571, -0.1722, -0.3059,  0.0529],\n",
    "                      [-0.0764,  0.4670, -0.2218, -0.3888],\n",
    "                      [ 0.4820, -0.0895,  0.0496, -0.4707],\n",
    "                      [ 0.0030, -0.0348,  0.4132,  0.3539],\n",
    "                      [-0.2953,  0.2528,  0.2744, -0.0833]])),\n",
    "    ('bias', torch.tensor([-0.2345, -0.0744,  0.1075, -0.1458, -0.4157, -0.3114]))]))\n",
    "proj_k.load_state_dict(OrderedDict([('weight', torch.tensor([[ 0.2761, -0.4321,  0.3839,  0.1454],\n",
    "                      [-0.3980,  0.1969, -0.4166, -0.2317],\n",
    "                      [-0.1690, -0.1395,  0.3167, -0.3027],\n",
    "                      [-0.1422, -0.2583, -0.4430, -0.0448],\n",
    "                      [ 0.4761,  0.1354,  0.1436,  0.3219],\n",
    "                      [ 0.3956,  0.0885,  0.2519,  0.1227]])),\n",
    "    ('bias', torch.tensor([0.4507, 0.3675, 0.3032, 0.3873, 0.4495, 0.1289]))]))\n",
    "proj_v.load_state_dict(OrderedDict([('weight', torch.tensor([[ 0.2719, -0.0664,  0.3742,  0.3409],\n",
    "                      [ 0.1177,  0.4588, -0.3498, -0.4507],\n",
    "                      [-0.4851, -0.3594,  0.2124, -0.1817],\n",
    "                      [-0.3162, -0.1503, -0.1955,  0.3816],\n",
    "                      [ 0.0005, -0.0776, -0.4964, -0.1608],\n",
    "                      [ 0.0581, -0.1783, -0.2951,  0.0964],\n",
    "                      [-0.3771,  0.1194, -0.4692,  0.1051],\n",
    "                      [-0.4773, -0.0826,  0.4722, -0.2247],\n",
    "                      [-0.1419,  0.0064,  0.3859,  0.1678],\n",
    "                      [ 0.2845, -0.4944,  0.4023, -0.2722]])),\n",
    "('bias', torch.tensor([ 0.1750,  0.1422, -0.3162,  0.2938,  0.0050, -0.0249,  0.2706, -0.2545, 0.0081, -0.2179]))]))\n",
    "reproj.load_state_dict(OrderedDict([('weight',\n",
    "    torch.tensor([[ 0.2090, -0.1364,  0.2534,  0.0466, -0.1310,  0.1216,  0.0299, -0.2586, 0.3095, -0.2708],\n",
    "            [-0.2939,  0.2829, -0.0410,  0.2643,  0.0008, -0.3008,  0.2150,  0.0737, -0.0611, -0.0701],\n",
    "            [ 0.2135, -0.1298, -0.3017,  0.2684, -0.0917, -0.1428,  0.1363, -0.1012, 0.2472,  0.2877],\n",
    "            [ 0.1128,  0.0359, -0.1215,  0.2214,  0.2173, -0.1789,  0.1038,  0.0059, 0.2911, -0.0398]])),\n",
    "             ('bias', torch.tensor([ 0.2898,  0.0237, -0.2518, -0.2610]))]))\n",
    "sequence = torch.tensor([[\n",
    "                   [ 0.1177,  0.4588, -0.3498, -0.4507],\n",
    "                   [-0.0764,  0.4670, -0.2218, -0.3888],\n",
    "                   [-0.3771,  0.1194, -0.4692,  0.1051],\n",
    "                   [-0.1419,  0.0064,  0.3859,  0.1678],\n",
    "                   [-0.3162, -0.1503, -0.1955,  0.3816],\n",
    "                   ]])\n",
    "assert torch.norm(transformer_head(sequence, sequence, sequence, proj_q, proj_k, proj_v, reproj, softmax) -\\\n",
    "    torch.tensor([[[ 0.3581,  0.3061, -0.1407, -0.0638],\n",
    "                   [ 0.3595,  0.3037, -0.1386, -0.0639],\n",
    "                   [ 0.3618,  0.3005, -0.1352, -0.0635],\n",
    "                   [ 0.3583,  0.3030, -0.1361, -0.0625],\n",
    "                   [ 0.3604,  0.3014, -0.1349, -0.0626]]])).item() < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0fc79",
   "metadata": {},
   "source": [
    "## Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b17d7b",
   "metadata": {},
   "source": [
    "![Generated Examples](https://techcrunch.com/wp-content/uploads/2022/08/53118410-9cce-468a-8bf6-1b8ce4dd1390_1600x925.webp?resize=1200,694)\n",
    "\n",
    "The models downloaded in Googledrive were distributed by [HuggingFace](https://huggingface.co/) and the example below by [FastAI](https://github.com/fastai/diffusion-nbs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_encode_string(string_list, tokenizer, text_encoder):\n",
    "    ml = tokenizer.model_max_length\n",
    "    inp = tokenizer(string_list, padding=\"max_length\", max_length=ml, truncation=True, return_tensors=\"pt\")\n",
    "    encoded = text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
    "    emp = tokenizer([\"\"] * len(string_list), padding=\"max_length\", max_length=ml, truncation=True, return_tensors=\"pt\")\n",
    "    empty = text_encoder(emp.input_ids.to(\"cuda\"))[0].half()\n",
    "    return torch.cat([empty, encoded])\n",
    "\n",
    "def show_image(latent_space, vae, indice=0):\n",
    "    with torch.no_grad():\n",
    "        normalized_image = vae.decode(1 / 0.18215 * latent_space).sample[indice]\n",
    "    image = (normalized_image/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n",
    "    return Image.fromarray((image*255).round().astype(\"uint8\"))\n",
    "\n",
    "def diffusion_loop(encoded_text, unet, scheduler, steps=70, g=7.5, width=512, height=512, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    scheduler.set_timesteps(steps)\n",
    "    latent_space = torch.randn((encoded_text.shape[0]//2, unet.in_channels, height//8, width//8))\n",
    "    latent_space = latent_space.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
    "    for step in tqdm(scheduler.timesteps):\n",
    "        inp = scheduler.scale_model_input(torch.cat([latent_space] * 2), step)\n",
    "        with torch.no_grad():\n",
    "            u, t = unet(inp, step, encoder_hidden_states=encoded_text).sample.chunk(2)\n",
    "        noise = u + g*(t-u)\n",
    "        latent_space = scheduler.step(noise, step, latent_space).prev_sample\n",
    "    return latent_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca21362",
   "metadata": {},
   "source": [
    "The functions above do the tokenization and encoding of the textual inputs, the conversion between the latent space and the image pixels in RGB, and, mainly, the diffusion loop, where the latent space is iteratively updated, reducing the noise, which is estimated by the model at each iteration.\n",
    "\n",
    "Investigate the Stable Diffusion model by inputting your own string to guide the diffusion. We strongly suggest that you give a input in English, since the text encoder (CLIP) was mainly trained using English. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31376d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "906b84eb326fb8d2bdeb9948055155c8",
     "grade": false,
     "grade_id": "difusao",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_textual_input() -> str:\n",
    "    \"\"\"\n",
    "    Returns a input string for the Diffusion model.\n",
    "    \n",
    "    :return: string to guide the diffusion.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e1455",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "017e42783ad9927b10182b4d2d806672",
     "grade": true,
     "grade_id": "testa-difusao",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "input_string = generate_textual_input()\n",
    "assert input_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88375f",
   "metadata": {},
   "source": [
    "Now, see the execution of the Stable Diffusion using your input (change `execute_transformer` to `True` in the beginning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71685dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert execute_transformer # This assert won't be evaluated\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", use_auth_token=hf_token, torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", use_auth_token=hf_token, torch_dtype=torch.float16).to(\"cuda\")\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", use_auth_token=hf_token, torch_dtype=torch.float16).to(\"cuda\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=hf_token, subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert execute_transformer # This assert won't be evaluated\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085,beta_end=0.012,beta_schedule=\"scaled_linear\",num_train_timesteps=1000)\n",
    "encoded_text = tokenize_encode_string([input_string], tokenizer, text_encoder)\n",
    "generated_images = diffusion_loop(encoded_text, unet, scheduler, steps=70, seed=42)\n",
    "show_image(generated_images, vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca032da",
   "metadata": {},
   "source": [
    "Implement the diffusion loop below with the coefficients of the simple update, where the original image is recovered by subtracting its repsective noise, both weighted by factors. (2 points)\n",
    "\n",
    "Since this is a simple scheduler model, the DDPM (Denoising Diffusion Probabilistic Models) needs more steps of diffusion, each one with a smaller amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe29ed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68b789af4f177b6a05fac35280580edd",
     "grade": false,
     "grade_id": "escalonador-simples",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_update(estimated_noise: torch.tensor, latent_image: torch.tensor, \n",
    "                  noise_factor: float, image_factor: float) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Implements a simple update of the scheduler to remove the noise present in the latent image.\n",
    "    The noise is multiplied by noise_factor and the image by the image_factor, which depend of the diffusion step.\n",
    "    \n",
    "    :param estimated_noise: noise tensor in the latent space of shape (N, C, H, W).\n",
    "    :param latent_image: image tensor in the latent space of shape (N, C, H, W).\n",
    "    :noise_factor: factor that multiplies the noise.\n",
    "    :image_factor: factor that multiplies the latent image.\n",
    "    :return: tensor of shape (N, C, H, W) estimated from the noise removal of the image, weighted by their\n",
    "             respective factors.\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()\n",
    "    return reconstructed_latent_image\n",
    "\n",
    "def simple_diffusion_loop(encoded_text, unet, steps=70, g=7.5, width=512, height=512, seed=42, train_steps=1000, beta_start=0.00085, beta_end=0.012):\n",
    "    torch.manual_seed(seed)\n",
    "    betas = np.linspace(beta_start, beta_end, train_steps)\n",
    "    alphas = 1.0 - betas\n",
    "    alpha_prod = np.pad(np.cumprod(alphas, axis=0), (0, 1), constant_values=1)\n",
    "    beta_prod = 1 - alpha_prod\n",
    "    latent_space = torch.randn((encoded_text.shape[0]//2, unet.in_channels, height//8, width//8)).to(\"cuda\").half()\n",
    "    for t in tqdm(np.arange(0, train_steps, train_steps // steps)[::-1]):\n",
    "        inp = torch.cat([latent_space] * 2)\n",
    "        with torch.no_grad():\n",
    "            uncond, text = unet(inp, t, encoder_hidden_states=encoded_text).sample.chunk(2)\n",
    "        ruido_estimado = uncond + g*(text-uncond)\n",
    "        latent_space_original = torch.clamp(simple_update(ruido_estimado, latent_space, \n",
    "                                beta_prod[t]**0.5/alpha_prod[t]**0.5, 1/alpha_prod[t]**0.5), -1, 1)\n",
    "        coeff_original = alpha_prod[t-1]**0.5 * betas[t]/beta_prod[t]\n",
    "        coeff_current = alphas[t]**0.5 * beta_prod[t-1]/beta_prod[t]\n",
    "        latent_space = coeff_original*latent_space_original + coeff_current*latent_space\n",
    "        variance = np.clip(beta_prod[t-1]/beta_prod[t] * betas[t], 1e-20, None)\n",
    "        ruido = torch.randn(ruido_estimado.shape, device=ruido_estimado.device, dtype=ruido_estimado.dtype)\n",
    "        latent_space = latent_space + variance**0.5 * ruido\n",
    "    return latent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdfcf0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1abf7b0a44bea4b7623e18df83f5577b",
     "grade": true,
     "grade_id": "testa-escalonador-simples",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert torch.norm(simple_update(torch.tensor([[[[0.1, -0.1], [0.2,-0.2]]]]), \n",
    "                                torch.tensor([[[[2, 3], [4,3]]]]), 0.4, 0.9) - \\\n",
    "                  torch.tensor([[[[1.76, 2.74],[3.52, 2.78]]]])) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5b804",
   "metadata": {},
   "source": [
    "Apesar do nome de simples, e de ser a mais simples que realmente √© empregada durante o treino do modelo, a DDPM requer uma quantidade de itera√ß√µes que justamente de aproxima do treino, no caso, mil passos, para que tenha uma converg√™ncia adequada. Por isso o outro m√©todo acaba sendo mais de uma ordem de grandeza mais r√°pido para infer√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert execute_transformer # This assert won't be evaluated\n",
    "generated_images = simple_diffusion_loop(encoded_text, unet, steps=1000, seed=42)\n",
    "show_image(generated_images, vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159d3e3",
   "metadata": {},
   "source": [
    "We incentivize you to explore the model with other textual inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01d83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18a2f95a",
   "metadata": {},
   "source": [
    "# Your data and feedback:\n",
    "\n",
    "Write a feedback for the lab so we can make it better for the next years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34213b26",
   "metadata": {},
   "source": [
    "In the following variables, write the number of hours spent on this lab, the perceived difficulty, and the expected grade (you may delete the `raise` and the comments):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b36de7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36f613d80a345c28aaf937672ec9e9f5",
     "grade": true,
     "grade_id": "meta_eval",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# meta_eval manual_graded_answer 0\n",
    "\n",
    "horas_gastas = None    # 1.5   - Float number with the number of hours spent \n",
    "dificuldade_lab = None # 0     - Float number from 0.0 to 10.0 (inclusive)\n",
    "nota_esperada = None   # 10    - Float number from 0.0 to 10.0 (inclusive)\n",
    "\n",
    "# WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb3d19",
   "metadata": {},
   "source": [
    "Write below other comments or feedbacks about the lab. If you did not understand anything about the lab, please also comment here.\n",
    "\n",
    "If you find any typo or bug in the lab, please comment below so we can fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7dbcf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc6431aff9a971fe5ed82c62bc668c96",
     "grade": true,
     "grade_id": "meta_eval_discursivo",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "WRITE YOUR SOLUTION HERE! (do not change this first line):\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**DISCURSIVE QUESTION**\n",
    "\n",
    "WRITE YOUR ANSWER HERE (do not delete this cell so the ID is not lost)\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8ab82",
   "metadata": {},
   "source": [
    "**End of the lab!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
