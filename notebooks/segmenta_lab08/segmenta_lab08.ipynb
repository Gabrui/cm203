{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8a2f1e",
   "metadata": {},
   "source": [
    "**Aeronautics Institute of Technology – ITA**\n",
    "\n",
    "**Computer Vision – CM-203**\n",
    "\n",
    "**Professors:** \n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "Gabriel Adriano de Melo\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Before submitting your lab, be sure that everything is running correctly (in sequence): first, **restart the kernel** (`Runtime->Restart Runtime` in Colab or `Kernel->Restart` in Jupyter). Then, execute all cells (`Runtime->Run All` in Colab or `Cell->Run All` in Jupyter) and verifies that all cells run without any errors, expecially the automatic grading ones, i.e. the ones with `assert`s.\n",
    "\n",
    "**Do not delete the answer cells**, i.e. the ones that contains `WRITE YOUR CODE HERE` or `WRITE YOUR ANSWER HERE`, because they contain metadata with the ids of the cells for the grading system. For the same reason, **do not delete the test cells**, i.e. the ones with `assert`s. The autograding system executes all the code sequentially, adding extra tests in the test cells. There is no problem in creating new cells, as long as you do not delete answer or test cells. Moreover, keep your solutions within the reserved spaces.\n",
    "\n",
    "The notebooks are implemented to be compatible with Google Colab, and they install the dependencies and download the datasets automatically. The commands which start with ! (exclamation mark) are bash commands and can be executed in a Linux terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bb0d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3022d",
   "metadata": {},
   "source": [
    "# Laboratório 8 - Segmentação Semântica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e76bc3",
   "metadata": {},
   "source": [
    "Neste laboratório exploraremos a segmentação semântica, isto é, a classificação de cada pixel de uma imagem. Isso ainda é uma etapa anterior à segmentação de instância, que além de classificar da pixel, atribui ele a uma instância específica (se houvessem vários objetos colados um no outro haveria distinção de cada instância de objeto).\n",
    "\n",
    "Assim este laboratório está mais simples do que a de detecção de objetos (nanoYOLO, lab6). Iniciaremos com o Data Augmentation, que ficou faltando no lab6; definiremos a arquitetura da rede de segmentação, que apresenta um bottleneck e conexões residuais, tanto para o encoder quanto para o decoder; definiremos a função de loss, que é mais simples e finalmente realizaremos o treinamento completo, usando o transfer learning nos pesos do encoder de uma rede já treinada em classificação (de uma imagem inteira)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86521dc",
   "metadata": {},
   "source": [
    "## Imports and Data Downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313e926",
   "metadata": {},
   "source": [
    "Todos os nossos imports serão realizados automaticamente pela FastAI. Enquanto ma maior parte das bibliotecas pode ser uma má ideia utilizar um star import (`*`) pois irá poluir o seu ambiente, o FastAI toma cuidado de limitar o que é exportado (importado pelo `*`) por meio da definição da variável `__all__` dentro de um módulo. Assim, tudo que é exportado/importado por ele é intencional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6211301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import albumentations as A\n",
    "from fastai.vision.all import *\n",
    "from torchvision.ops import Conv2dNormActivation, SqueezeExcitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a7ccd",
   "metadata": {},
   "source": [
    "Atenção, se por algum motivo você for mudar a pasta `/content` ou a flag de `RETRAIN`, mude na célula abaixo, pois ela é sobrescrita durante a correção (*read_only*) uma vez que para a correção funcionar o `base_path` tem que apontar para a pasta correta. Durante a correção, o `RETRAIN` será `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b4cc8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b25695c96999aa87ece1c769c5df90a",
     "grade": false,
     "grade_id": "dataset_lab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# dataset_lab read_only\n",
    "\n",
    "! [ ! -d \"/content/segmenta\" ] && gdown -O /content/segmenta.zip \"1H2JlJ3mDq9a4wSXNRruDl-0fO42yzeTh\" &&  unzip -q /content/segmenta.zip -d /content && rm /content/segmenta.zip\n",
    "! [ ! -d \"/content/segmenta\" ] && wget -P /content/ \"http://ia.gam.dev/cm203/23/lab08/segmenta.zip\"  &&  unzip -q /content/segmenta.zip -d /content && rm /content/segmenta.zip\n",
    "base_path = Path(\"/content/segmenta\")\n",
    "%cd /content\n",
    "\n",
    "RETRAIN = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0a3c5",
   "metadata": {},
   "source": [
    "## Data Augmentation (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca2076",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "Apesar do FastAI já ter várias implementações internas de Data Augmentations (rotação, translação, escala, flip, entre outros) vamos utilizar uma outra biblioteca chamada [Albumentations](https://albumentations.ai), que apresenta ainda mais transformações. Contudo, essa biblioteca só opera a nível de matrizes (tensores) do formato Numpy, e não em tensores de PyTorch, nem em GPU, apenas CPU.\n",
    "\n",
    "Lembre-se que em um dataset de segmentação cada pixel tem a sua própria anotação individual, e isso é guardado em uma outra matriz (que é uma imagem dos labels)\n",
    "\n",
    "![Segmentation with labels](https://www.jeremyjordan.me/content/images/2018/05/Screen-Shot-2018-05-16-at-9.36.38-PM.png)\n",
    "\n",
    "Vamos baixar os dados de imagens gravadas a partir da frente de um carro (tal qual se fosse um carro autônomo). Esse é o dataset [Cambridge-driving Labeled Video Database (CamVid)](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/). Para este lab usaremos uma versão com resolução menor (tiny) para fins de treino mais rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = untar_data(URLs.CAMVID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd8396",
   "metadata": {},
   "source": [
    "As imagens tem as suas respectivas labels (anotações) de segmentação, vejamos os nomes de cada uma delas nesse dataset (atenção, não confunda com a da ilustração acima):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ff49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_labels = np.loadtxt(data_path/'codes.txt', dtype=str)\n",
    "len(nomes_labels), nomes_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b8b76",
   "metadata": {},
   "source": [
    "Veja um dataloader de segmentação criado usando a API de DataBlock do FastAI. Atente-se que a diferença maior agora é o `MaskBlock` que é justamente uma 'imagem' com os labels. Perceba que nesse dataset elas ficaram em outra pasta chamada `labels`, que nós precisamos acessa no `get_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b380538",
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid = DataBlock(\n",
    "    blocks=(ImageBlock, MaskBlock(nomes_labels)),\n",
    "    get_items=get_image_files,\n",
    "    splitter=RandomSplitter(),\n",
    "    get_y=lambda o: o.parent.parent/'labels'/f'{o.stem}_P{o.suffix}'\n",
    ").dataloaders(data_path/\"images\", path=data_path, bs=8)\n",
    "\n",
    "if RETRAIN:\n",
    "    camvid.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49761a7d",
   "metadata": {},
   "source": [
    "Observe a largura das imagens e de suas respectivas máscaras de segmentação $(960, 720)$. Veja como é o shape dos tensores respectivamente das imagens e das máscaras. Observe que a máscara é composta por números inteiros positivos e sequenciais que representam a classe do pixel. Durante a aplicação da loss function, teremos que convertê-los para uma representação de one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagems, mascaras_segmenta = camvid.one_batch()\n",
    "imagems.shape, mascaras_segmenta.shape, mascaras_segmenta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81403a77",
   "metadata": {},
   "source": [
    "Agora vamos ver as transformações que podem ser aplicadas às imagens e às anotações de segmentação.\n",
    "\n",
    "Como as transformações são aplicadas antes do batch, vamos carregar uma imagem manualmente em conjunto com o seu label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = PILImage.create(data_path/'images'/'0016E5_06660.png')\n",
    "label0 = PILImage.create(data_path/'labels'/'0016E5_06660_P.png')\n",
    "\n",
    "if RETRAIN:\n",
    "    plt.imshow(np.array(label0)[..., 0], cmap='gist_ncar')\n",
    "    plt.colorbar()\n",
    "img0 if RETRAIN else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a544e2",
   "metadata": {},
   "source": [
    "Veja agora como aplicamos uma transformação da biblioteca Albumentations: \n",
    "\n",
    "Primeiro criamos a transformação com os parâmetros (limites e probabilidades) escolhidos; depois a aplicamos sobre a imagem e sobre as anotações de segmentação, simultaneamente, para que a mesma transformação (mesmos valores aleatórios) sejam aplicados igualmente sobre ambos. Caso contrário, poderíamos ter uma valor aleatório diferente de rotação entre a sua imagem e a sua anotação de segmentação. Para isso usamos atributos com nome `image` para a imagem original e `mask` para as anotações de segmentação. Essa distinção é importante, pois há transformações (de cores, por exemplo) onde apenas a imagem é alterada.\n",
    "\n",
    "O resultado é um dicionário que contém as keys de `image` e de `mask` também, que podemos acessar. O formato de entrada e de saída são imagens (matrizes) Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1efc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform1 = A.ShiftScaleRotate(always_apply=True)\n",
    "res = transform1(image=np.array(img0), mask=np.array(label0))\n",
    "\n",
    "if RETRAIN:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12,5))\n",
    "    axs[0].imshow(res['image'])\n",
    "    axs[1].imshow(res['mask'][..., 0], cmap='gist_ncar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca1943",
   "metadata": {},
   "source": [
    "Além disso, podemos compor várias transformações (como se fosse um Pipeline do FastAI) por meio do `Compose`. Olhe também outras transformações que não existem no FastAI, como o `RandomFog`, o `RandomRain`, o `RandomSunFlare` entre outras.\n",
    "\n",
    "Execute várias vezes, e veja o sol, a chuva, as cores que variam um pouco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_compose = A.Compose([\n",
    "    A.RandomCrop(width=512, height=512, always_apply=True),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(always_apply=True),\n",
    "    A.ElasticTransform(),\n",
    "    A.RandomBrightnessContrast(),\n",
    "    A.RandomSunFlare(src_radius=90),\n",
    "    A.RandomFog(fog_coef_upper=1),\n",
    "    A.RandomRain(blur_value=2)\n",
    "])\n",
    "\n",
    "res2 = transform_compose(image=np.array(img0), mask=np.array(label0))\n",
    "\n",
    "if RETRAIN:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12,5))\n",
    "    axs[0].imshow(res2['image'])\n",
    "    axs[1].imshow(res2['mask'][..., 0], cmap='gist_ncar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079a8a5",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função `encodes` abaixo, de acordo com a sua documentação. A ideia é aplicar as trasformações do `NossoTransformaAugment`, declaradas na sua inicialização. Perceba que essa é uma classe do tipo `ItemTransform` que será utilizada posteriormente no `item_tfms` do `DataBlock` durante o treino de verdade.\n",
    "\n",
    "O uso do Albumentations nessa aplicação foi apenas didático (não chove e está sempre bem iluminado), mas na vida real pode ser útil, por exemplo, veja mais exemplos de [Augmentations neste link](https://albumentations.ai/docs/getting_started/transforms_and_targets/)\n",
    "\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT) para pegar a resposta pronta. **NÃO** pesquise a resposta pronta na internet.\n",
    "\n",
    "**Pode** olhar a documentação das bibliotecas ([PyTorch](https://pytorch.org/docs/stable/), [FastAI](https://docs.fast.ai/), [Albumentations](https://albumentations.ai/docs/) mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "\n",
    "Lembrando dos labs anteriores, utilize `np.array` para converter uma imagem do tipo PIL para uma matriz NumPy. Utilize `PILImage.create` e `PILMask.create` para fazer o inverso, lembrando que esses são tipos do FastAI.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b6e89",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "419d132a2c90c01e4395fe76e8489038",
     "grade": false,
     "grade_id": "questao_augmentation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_augmentation autograded_answer\n",
    "\n",
    "class NossoTrasformaAugment(ItemTransform):\n",
    "    split_idx = 0\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tfm1 = A.HorizontalFlip(p=0.5)\n",
    "        self.tfm2 = A.RandomBrightnessContrast(p=0.9)\n",
    "        self.tfm3 = A.ShiftScaleRotate(always_apply=True)\n",
    "        self.tfm4 = A.RandomRain(p=0.1, blur_value=1)\n",
    "        self.transforma_total = A.Compose([self.tfm1, self.tfm2, self.tfm3, self.tfm4])\n",
    "\n",
    "    def encodes(self, x):\n",
    "        \"\"\"Aplica as transformações tfm1/2/3 sequencialmente e igualmente para a\n",
    "        imagem original e para as anotações de segmentação. Cada transformação\n",
    "        poderá ser chamada apenas uma única vez.\n",
    "        \n",
    "        Args:\n",
    "            x: Tupla com imagem original e imagem de anotações segmentadas\n",
    "                tuple[PILImage, PILMask]\n",
    "\n",
    "        Returns:\n",
    "            Tupla com a imagem do tipo PILImage e PILMask\n",
    "        \"\"\"\n",
    "        # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7e773",
   "metadata": {},
   "source": [
    "Se você usou uma LLM, escreva a sua conversa com ela aqui nesta própria célula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42501c3e",
   "metadata": {},
   "source": [
    "Vamos começar o teste com uma imagem mais de zeros apenas para verificar se os valores de retorno estão condizentes com o esperado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3913fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "706661f835d261106f56a979c16fa8ad",
     "grade": true,
     "grade_id": "testa_1_questao_augmentation",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_1_questao_augmentation autograder_tests 0.3\n",
    "\n",
    "tfm_aug1 = NossoTrasformaAugment()\n",
    "pil_img_zeros = PILImage.create(np.zeros((100, 100, 3), dtype=np.uint8))\n",
    "pil_mask_zeros = PILMask.create(np.zeros((100, 100), dtype=np.uint8))\n",
    "\n",
    "res_encode = tfm_aug1.encodes((pil_img_zeros, pil_mask_zeros))\n",
    "\n",
    "assert type(res_encode) is tuple\n",
    "assert len(res_encode) == 2\n",
    "assert type(res_encode[0]) is PILImage\n",
    "assert type(res_encode[1]) is PILMask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b6147",
   "metadata": {},
   "source": [
    "Vamos verificar se você chamou na ordem correta e se foram todas as transformações mesmo. Como os valores são pseudo-aleatórios, vamos reinicializar os estados (seed) dos geradores de números pseudo-aleatórios (a função `set_seed` vem do FastAI e ela age sobre o PyTorch, Numpy, Python), para que haja reproducidibilidade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd221a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd0eaf64717a137767679efcf18a316c",
     "grade": true,
     "grade_id": "testa_2_questao_augmentation",
     "locked": true,
     "points": 1.7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_2_questao_augmentation autograder_tests 1.7\n",
    "\n",
    "set_seed(2023_10_13, reproducible=True)\n",
    "\n",
    "tfm_aug2 = NossoTrasformaAugment()\n",
    "img_t2 = PILImage.create(data_path/'images'/'0016E5_06660.png')\n",
    "label_t2 = PILImage.create(data_path/'labels'/'0016E5_06660_P.png')\n",
    "\n",
    "res_img2, res_label2 = tfm_aug2.encodes((img_t2, label_t2))\n",
    "\n",
    "assert np.array(res_img2).shape == (720, 960, 3)\n",
    "assert res_label2.shape == (720, 960)\n",
    "assert np.all(np.array(res_img2).sum(axis=(0,1)) == [47285429, 49826794, 52152949])\n",
    "assert abs(np.array(res_label2).mean() - 19.371778) < 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91186543",
   "metadata": {},
   "source": [
    "Veja o resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_img2 if RETRAIN else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49302377",
   "metadata": {},
   "source": [
    "## Arquitetura do Encoder  (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c5c3f",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "Nós poderíamos utilizar diretamente o `unet_learner` do FastAI, que já faria automaticamente toda a definição da arquitetura baseada na Unet com encoder, decoder e loss-function adequadas à segmentação. Contudo, vamos realizar a definição de cada uma dessas partes separadamente, para que você possa entendê-la mais profundamente e também aplicar os conceitos de transfer-learning, além de continuar se familiarizando com o framework do PyTorch e com implementações de redes neurais.\n",
    "\n",
    "A arquitetura do encoder nada mais é do que a mesma que encontramos em redes de classificação de imagens. Nós vamos aproveitar justamente o fato de que na maioria dessas arquitetura há diminuição das dimensões espaciais, em geral, em um fator de 2 a cada módulo, conforme observado na Figura abaixo. Na UNet do paper original eles não utilizavam padding, e a saída ficava com um tamanho espacial diferente da entrada, mas nas implementações posteriores, tornou-se muito comum utilizar padding same nas convoluções.\n",
    "\n",
    "![Arquitetura baseada em UNet](https://nchlis.github.io/2019_10_30/architecture_unetV2.png)\n",
    "\n",
    "Em vez de utilizar uma arquitetura já implementada nos labs anteriores (ResNet ou ResNext) vamos para outra um pouco mais recente: MobileNet-v3. É de 2019 (que ainda é bem antigo) mas é bem mais recente do que as ResNext (2016).\n",
    "\n",
    "Vejamos os seus parâmetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos_mnv3 = MobileNet_V3_Large_Weights.IMAGENET1K_V2.get_state_dict(progress=True)\n",
    "list(pesos_mnv3.keys()), pesos_mnv3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a00bf3",
   "metadata": {},
   "source": [
    "Assim, vamos definir abaixo a arquitetura da rede, que é simplesmente a aplicação sequencial de blocos menores. Um dos 'segredos' da MobileNet está justamente na definição desses hyperparâmetros, que foram selecionados automaticamento pelo treino de milhares de redes em paralelo com diferentes combinações (AutoML).\n",
    "\n",
    "Todos esses blocos convolucionais são agrupados sequencialmente no módulo `self.features`. Em seguida temos um global average pooling seguido pela aplicação de uma rede completamente conectada (densa, *MLP*) `self.classifier`.\n",
    "\n",
    "Em suma, essa é a arquitetura da MobileNet, apresentada no paper [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244), cujo código está implementado no próprio PyTorch. O código abaixo foi retirado da implementação padrão.\n",
    "\n",
    "Para mais alguns detalhes, observe que o batch_norm2d foi chamado por meio da função built-in do Python `partial` para passar os parâmetros de momento e de EPS (que é o número que se soma ao divisor para evitar divisão por zero). A convolução é seguida diretamente pela camada de normalização e pela função de ativação, tudo isso embutido na camada `Conv2dNormActivation`. Utiliza-se a função de ativação `Hardswish`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc030152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobNet3large(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        batch_norm2d = partial(nn.BatchNorm2d, eps=0.001, momentum=0.01)\n",
    "        \n",
    "        # Camada de Entrada\n",
    "        camadas = [Conv2dNormActivation(3, 16, kernel_size=3, stride=2,\n",
    "                                norm_layer=batch_norm2d, activation_layer=nn.Hardswish)]\n",
    "        \n",
    "        # Blocos\n",
    "        hyperparams_blocks = [(16,  3,  16,  16, False, False, 1),\n",
    "                              (16,  3,  64,  24, False, False, 2),\n",
    "                              (24,  3,  72,  24, False, False, 1),\n",
    "                              (24,  5,  72,  40,  True, False, 2),\n",
    "                              (40,  5, 120,  40,  True, False, 1),\n",
    "                              (40,  5, 120,  40,  True, False, 1),\n",
    "                              (40,  3, 240,  80, False,  True, 2),\n",
    "                              (80,  3, 200,  80, False,  True, 1),\n",
    "                              (80,  3, 184,  80, False,  True, 1),\n",
    "                              (80,  3, 184,  80, False,  True, 1),\n",
    "                              (80,  3, 480, 112,  True,  True, 1),\n",
    "                              (112, 3, 672, 112,  True,  True, 1),\n",
    "                              (112, 5, 672, 160,  True,  True, 2),\n",
    "                              (160, 5, 960, 160,  True,  True, 1),\n",
    "                              (160, 5, 960, 160,  True,  True, 1)]\n",
    "        for hparam in hyperparams_blocks:\n",
    "            camadas.append(MN3Block(hparam, batch_norm2d))\n",
    "        \n",
    "        # Convolução de Saída\n",
    "        camadas.append(Conv2dNormActivation(160, 960, kernel_size=1,\n",
    "                                norm_layer=batch_norm2d, activation_layer=nn.Hardswish))\n",
    "        self.features = nn.Sequential(*camadas)\n",
    "        self.classifier = nn.Sequential(nn.Linear(960, 1280),\n",
    "                                        nn.Hardswish(inplace=True),\n",
    "                                        nn.Dropout(p=0.2, inplace=True),\n",
    "                                        nn.Linear(1280, num_classes))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e3e55",
   "metadata": {},
   "source": [
    "Observe que o bloco nós chamamos de `MN3Block` e será implementado por você a seguir, ele recebe uma tupla que define os seus hyper-parâmetros: *in_chan, kernel, expanded_chan, out_chan, use_se, use_hs, stride*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b5dbfb",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função `forward` da classe `MN3Block` abaixo, de acordo com a sua documentação. A ideia é aplicar todos os módulos sequencialmente e, se houver conexão residual, aplicá-la também.\n",
    "\n",
    "Observe a figura abaixo com os quatro módulos (convoluções) que são aplicados: \n",
    "- a convolução de entrada apenas sobre os canais (`kernel_size=1`)\n",
    "- a convolução apenas espacial, separada para cada canal (`groups=channels`)\n",
    "- a camada de Squeeze-and-Excite que simplemente multiplica cada canal por um escalar diferente (definido por uma rede densa)\n",
    "- a camada de saída convolucional, que assim como a entrada, atua apenas sobre os canais (`kernel_size=1`)\n",
    "\n",
    "![Mobile Net V3](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-21_at_11.03.15_PM.png)\n",
    "\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT) para pegar a resposta pronta. **NÃO** pesquise a resposta pronta na internet.\n",
    "\n",
    "**Pode** olhar a documentação das bibliotecas (PyTorch, mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Use o `self.block` para chamar todos os módulos sequencialmente.\n",
    "\n",
    "Lembre-se que para implementar uma conexão residual basta somar a entrada (antes de realizar qualquer operação) à saída. Nesse caso as dimensões já estão casadas.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5262ff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bec20af577ac37baf45ef815a882cd6",
     "grade": false,
     "grade_id": "questao_encoder",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_encoder autograded_answer\n",
    "\n",
    "class MN3Block(nn.Module):\n",
    "    def __init__(self, hyperparams, norm_layer):\n",
    "        super().__init__()\n",
    "        in_chan, kernel, expanded_chan, out_chan, use_se, use_hs, stride = hyperparams\n",
    "        act_layer = nn.Hardswish if use_hs else nn.ReLU\n",
    "        se_layer = partial(SqueezeExcitation, scale_activation=nn.Hardsigmoid)\n",
    "        lista = []\n",
    "        \n",
    "        # input to expanded (channels)\n",
    "        if expanded_chan != in_chan:\n",
    "            lista.append(Conv2dNormActivation(in_chan, expanded_chan, kernel_size=1,\n",
    "                            norm_layer=norm_layer, activation_layer=act_layer))\n",
    "        \n",
    "        # depthwise convolution\n",
    "        lista.append(Conv2dNormActivation(expanded_chan, expanded_chan, kernel_size=kernel,\n",
    "            stride=stride, groups=expanded_chan, norm_layer=norm_layer, activation_layer=act_layer))\n",
    "        \n",
    "        # Squeeze-and-Excite Layer\n",
    "        if use_se:\n",
    "            squeeze_chan = int((expanded_chan // 4) + 4) // 8 * 8\n",
    "            squeeze_chan += 8 if squeeze_chan < 0.9 * (expanded_chan // 4) else 0\n",
    "            lista.append(se_layer(expanded_chan, squeeze_chan))\n",
    "        \n",
    "        # expanded to output (channels)\n",
    "        lista.append(Conv2dNormActivation(expanded_chan, out_chan, kernel_size=1,\n",
    "                                           norm_layer=norm_layer, activation_layer=None))\n",
    "        \n",
    "        self.block = nn.Sequential(*lista)\n",
    "        self.existe_conexao_residual = (stride == 1 and in_chan == out_chan)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\" Realiza a propagação direta do bloco\n",
    "        Aplique todas as camadas sequencialmente\n",
    "        Se houver conexão residual, aplique-a também\n",
    "\n",
    "        Args:\n",
    "            x: tensor de entrada ao bloco de shape (B, in_chan, H, W)\n",
    "\n",
    "        Returns:\n",
    "            tensor com shape (B, out_chan, H, W)\n",
    "        \"\"\"\n",
    "        # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d5adc",
   "metadata": {},
   "source": [
    "Se você usou uma LLM, escreva a sua conversa com ela aqui nesta própria célula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b94db",
   "metadata": {},
   "source": [
    "Verifica se o shape de saída está correto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604fc22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77b55939279e9377dd63fce2ac573155",
     "grade": true,
     "grade_id": "testa_1_questao_encoder",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_1_questao_encoder autograder_tests 0.1\n",
    "\n",
    "bl0 = MN3Block((16,  3,  32,  32, False, False, 1), nn.BatchNorm2d)\n",
    "x0 = torch.randn((3, 16, 7, 7))\n",
    "\n",
    "out0 = bl0(x0)\n",
    "assert out0.shape == (3, 32, 7, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e1d8d",
   "metadata": {},
   "source": [
    "Verifica se o código ainda funciona quando há conexão residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3418d486",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a1dd17ca75c910709bdba4dc847b4ce",
     "grade": true,
     "grade_id": "testa_2_questao_encoder",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_2_questao_encoder autograder_tests 0.1\n",
    "\n",
    "bl1 = MN3Block((16,  3,  16,  16, False, False, 1), nn.BatchNorm2d)\n",
    "x1 = torch.randn((3, 16, 7, 7))\n",
    "\n",
    "out1 = bl1(x1)\n",
    "assert out1.shape == (3, 16, 7, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2af9a5",
   "metadata": {},
   "source": [
    "Verifica se a implementação está correta, comparando com a implementação padrão do PyTorch (OBS: é importantíssimo utilizar o `.train(False)` para reconfigurar o dropout e o batch_norm para inferência, senão eles agiriam de forma aleatória):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c65b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07b28103942fc8d831cb669aa970190b",
     "grade": true,
     "grade_id": "testa_3_questao_encoder",
     "locked": true,
     "points": 1.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_3_questao_encoder autograder_tests 1.8\n",
    "\n",
    "pesos_mnv3 = MobileNet_V3_Large_Weights.IMAGENET1K_V2.get_state_dict(progress=True)\n",
    "\n",
    "mn3l = MobNet3large()\n",
    "mn3l.load_state_dict(pesos_mnv3)\n",
    "mn3l.train(False)\n",
    "\n",
    "padrao = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V2)\n",
    "padrao.train(False)\n",
    "\n",
    "x3 = torch.randn((1, 3, 256, 256))\n",
    "\n",
    "with torch.no_grad():\n",
    "  out3 = mn3l(x3)\n",
    "  saida_padrao = padrao(x3)\n",
    "\n",
    "assert torch.linalg.norm(out3 - saida_padrao) < 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd5fab",
   "metadata": {},
   "source": [
    "## Arquitetura do Decoder  (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acacf76c",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "Agora vamos codificar o decoder, que irá receber como entrada o espaço latente codificado pelo encoder, isto é, o tensor com dimensões espaciais reduzidas (mas com mais canais).\n",
    "\n",
    "Enquanto nas arquiteturas mais usuais, há vários estágios para o upsampling, isto é, para o aumento das dimensões espaciais, neste, nós usaremos a arquitetura do DeepLabV3, do artigo [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587). Em poucas palavras, ele utiliza várias convoluções em paralelo, alterando-se a dilatação dessas convoluções, tudo isso encapsulado no módulo de *Atrous Spatial Pyramid Pooling* (ASPP).\n",
    "\n",
    "Abaixo temos o código desse *encoder*, que é simplemente composto de convoluções que não alteram as dimensões espaciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec684303",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_bnrelu = partial(Conv2dNormActivation, norm_layer=nn.BatchNorm2d, activation_layer=nn.ReLU, bias=False)\n",
    "\n",
    "class DeepLabDecoder(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super().__init__(AtrousSpatialPyramidPooling(in_channels),\n",
    "                         conv2d_bnrelu(256, 256, kernel_size=3, padding=1),\n",
    "                         nn.Conv2d(256, num_classes, kernel_size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c7d0c",
   "metadata": {},
   "source": [
    "É importante resaltar que nas operações de UpSampling, recomenda-se que o parâmetro `align_corners` seja `False` para evitar enviesamento da posição, como se observa na figura abaixo. Se a posição ficar enviesada, as camadas convolucionais perdem a sua propriedade de equivariância à translação.\n",
    "\n",
    "![Align Corners](https://discuss.pytorch.org/uploads/default/original/2X/6/6a242715685b8192f07c93a57a1d053b8add97bf.png)\n",
    "\n",
    "Assim, veja abaixo a implementação do módulo `ASPPPooling` que faz um global average pooling (média sobre todas as dimensões espaciais), usa uma camada convolucional e faz um upsampling (repete os valores) para retornar o mesmo tamanho (dimensões espaciais) da entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_chan: int, out_chan: int):\n",
    "        super().__init__(nn.AdaptiveAvgPool2d(1),\n",
    "                         conv2d_bnrelu(in_chan, out_chan, kernel_size=1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        size = x.shape[-2:]\n",
    "        return F.interpolate(super().forward(x), size=size, mode=\"bilinear\", align_corners=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245931df",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função `forward` da classe `AtrousSpatialPyramidPooling` abaixo, de acordo com a sua documentação. A ideia é aplicar as convoluções (`self.convs`, cada uma com sua própria dilatação, assim forma uma pirâmide) todas em paralelo, depois concatene todos esses valores na camada dos canais e utilize a convolução final.\n",
    "\n",
    "![Atrous Spatial Pyramid Pooling](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-28_at_2.59.27_PM.png)\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT) para pegar a resposta pronta. **NÃO** pesquise a resposta pronta na internet.\n",
    "\n",
    "**Pode** olhar a documentação das bibliotecas (PyTorch, FastAI, mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Utilize um for-loop para percorrer todas as convs e vá salvando o resultado em uma lista Python.\n",
    "\n",
    "Use a função `torch.cat` para concatenar os tensores. Utilize a dimensão correta (dos canais).\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54a599",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7a1e036275143813892502c6f983424",
     "grade": false,
     "grade_id": "questao_decoder",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_decoder autograded_answer\n",
    "\n",
    "class AtrousSpatialPyramidPooling(nn.Module):\n",
    "    def __init__(self, in_chan: int, out_chan=256):\n",
    "        super().__init__()\n",
    "        lista = [conv2d_bnrelu(in_chan, out_chan, kernel_size=1)]\n",
    "        for r in (12, 24, 36):\n",
    "            lista.append(conv2d_bnrelu(in_chan, out_chan, kernel_size=3, padding=r, dilation=r))\n",
    "        lista.append(ASPPPooling(in_chan, out_chan))\n",
    "\n",
    "        self.convs = nn.ModuleList(lista)\n",
    "\n",
    "        self.final = nn.Sequential(conv2d_bnrelu(len(lista) * out_chan, out_chan, kernel_size=1),\n",
    "                                   nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Realiza a propagação direta do bloco\n",
    "        Aplique as camadas convs todas paralelamente.\n",
    "        Concatene os resultados dessas convs (na dimensão dos canais)\n",
    "        Aplique a convolução final sobre o tensor concatenado\n",
    "\n",
    "        Args:\n",
    "            x: tensor de entrada ao bloco de shape (B, in_chan, H, W)\n",
    "\n",
    "        Returns:\n",
    "            tensor com shape (B, out_chan, H, W)\n",
    "        \"\"\"\n",
    "        # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e6d936",
   "metadata": {},
   "source": [
    "Se você usou uma LLM, escreva a sua conversa com ela aqui nesta própria célula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b1994",
   "metadata": {},
   "source": [
    "Verifica se o shape de saída está correto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72c37f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b44c4499043f442c47ece0d41aae4247",
     "grade": true,
     "grade_id": "testa_1_questao_decoder",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_1_questao_decoder autograder_tests 0.2\n",
    "\n",
    "aspp0 = AtrousSpatialPyramidPooling(128, 384)\n",
    "\n",
    "x0 = torch.randn((3, 128, 64, 64))\n",
    "\n",
    "out0 = aspp0(x0)\n",
    "assert out0.shape == (3, 384, 64, 64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171fe08",
   "metadata": {},
   "source": [
    "Verifica se a implementação está correta, comparando com o resultado esperado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af162934",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02814d4703ed6d5a7ea1b1f261953d70",
     "grade": true,
     "grade_id": "testa_2_questao_decoder",
     "locked": true,
     "points": 1.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_2_questao_decoder autograder_tests 1.8\n",
    "\n",
    "aspp1 = AtrousSpatialPyramidPooling(64, 128)\n",
    "aspp1.load_state_dict(torch.load(base_path / 'aspp1.pth'))\n",
    "aspp1.train(False)\n",
    "\n",
    "x1 = torch.load(base_path / 'x1.pt')\n",
    "\n",
    "out1 = aspp1(x1)\n",
    "out1_esperado = torch.load(base_path / 'out1.pt')\n",
    "\n",
    "assert out1.shape == (1, 128, 78, 79)\n",
    "assert torch.linalg.norm(out1 - out1_esperado) < 1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c476fbce",
   "metadata": {},
   "source": [
    "## Loss Function  (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ee62f",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "$L = \\frac{-1}{B H W} \\sum_B \\sum_H \\sum_W \\sum_C p_{b,h,w,c} \\log{\\hat{p}_{b,h,w,c}}$ , onde $B$ é a quantidade de dados no batch, $C$ é a quantidade de canais, $H$ e $W$ são a altura e largura, $p_{b,h,w,c}$ é a probabilidade verdadeira para o pixel específico no canal e batch definidos, e $\\hat{p}_{b,h,w,c}$ é a probabilidade estimada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_estimados = torch.randn(size=(1, 17, 120, 130))\n",
    "probabilidades_verdadeiras = F.softmax(torch.randn(size=(1, 17, 120, 130)), dim=1)\n",
    "\n",
    "F.cross_entropy(logits_estimados, probabilidades_verdadeiras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07824387",
   "metadata": {},
   "source": [
    "A função F.cross_entropy do PyTorch também aceita passar os labels verdadeiros (em vez das probabilidades verdadeiras). Imagine que isso simplesmente é antes de fazer o one-hot encoding dos labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_estimados = torch.randn(size=(20, 17))\n",
    "classes_verdadeiras = torch.randint(low=0, high=17, size=(20,))\n",
    "\n",
    "F.cross_entropy(logits_estimados, classes_verdadeiras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d72e3d",
   "metadata": {},
   "source": [
    "E novamente também ela está preparada para lidar com tipos de imagens, isto é, com shape da forma $(B, C, H, W)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_estimados = torch.randn(size=(1, 17, 120, 130))\n",
    "classes_verdadeiras = torch.randint(low=0, high=17, size=(1, 120, 130))\n",
    "\n",
    "F.cross_entropy(logits_estimados, classes_verdadeiras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845233b5",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função `simple_segmentation_loss` abaixo, de acordo com a sua documentação. A ideia é implementar a entropia cruzada pixel-a-pixel (realizando a média de todos esses valores, seja dos pixels ou do batch).\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT) para pegar a resposta pronta. **NÃO** pesquise a resposta pronta na internet.\n",
    "\n",
    "**Pode** olhar a documentação das bibliotecas (PyTorch, FastAI, mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Use a função do PyTorch `F.cross_entropy`, preste atenção para ver se você não está chamando-a de forma errada.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065b45c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8e4312da38ed1c1e181cbb9c489a5a1",
     "grade": false,
     "grade_id": "questao_loss",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_loss autograded_answer\n",
    "\n",
    "def simple_segmentation_loss(ŷ: Tensor, y: Tensor) -> Tensor:\n",
    "    \"\"\"Retorna a loss function da entropia cruzada média (pixel-a-pixel) \n",
    "    \n",
    "    Args:\n",
    "        ŷ: Tensor dos logits dos labels, shape (B, C, H, W) ou (B, C)\n",
    "        y: Tensor dos labels verdadeiros, shape (B, H, W) ou (B, )\n",
    "    \n",
    "    Returns:\n",
    "        Tensor de rank 0 que é loss de segmentação\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea10422",
   "metadata": {},
   "source": [
    "Se você usou uma LLM, escreva a sua conversa com ela aqui nesta própria célula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddd5c11",
   "metadata": {},
   "source": [
    "Verifica se o shape de saída está correto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957dfd42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c86581c86cfccf97d6db5626f0fc861",
     "grade": true,
     "grade_id": "testa_1_questao_loss",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_1_questao_loss autograder_tests 0.2\n",
    "\n",
    "ŷ0 = torch.randn((1,30))\n",
    "y0 = tensor([8])\n",
    "\n",
    "loss0 = simple_segmentation_loss(ŷ0, y0)\n",
    "assert type(loss0) == Tensor\n",
    "assert loss0.shape == tuple()\n",
    "assert loss0.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88edf00b",
   "metadata": {},
   "source": [
    "Verificando o valor para apenas um tensor simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cef6d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "243ad9c03fa5349bb6f7ad9b3ea64bf2",
     "grade": true,
     "grade_id": "testa_2_questao_loss",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_2_questao_loss autograder_tests 0.2\n",
    "\n",
    "ŷ0 = tensor([[-3, 2, 3, 8.]])\n",
    "y0 = tensor([3])\n",
    "\n",
    "assert abs(simple_segmentation_loss(ŷ0, y0) - 0.0092) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba618f91",
   "metadata": {},
   "source": [
    "Verificando o valor para o shape esperado de imagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ec35f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "321a3fe3303b720af29fe645a765cf2b",
     "grade": true,
     "grade_id": "testa_3_questao_loss",
     "locked": true,
     "points": 1.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_3_questao_loss autograder_tests 1.6\n",
    "\n",
    "ŷ0 = tensor([-3, 2, 3, 8, 2, 3, 0, 6.]).reshape(1, 4, 2, 1)\n",
    "y0 = tensor([1, 1]).reshape(1, 2, 1)\n",
    "\n",
    "assert abs(simple_segmentation_loss(ŷ0, y0) - 0.2429) < 1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc04a6",
   "metadata": {},
   "source": [
    "## Arquitetura Final (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6461c6c",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "\n",
    "Após essas operações de convolução que não alteram as dimensões espaciais, há finalmente o upsampling para o tamanho final da imagem.\n",
    "\n",
    "Essa arquitetura é bem simples, mas por causa desse upsampling brusco, peca ao perder os detalhes, principalmente de objetos pequenos ou de bordas. Contudo, por ter menos operações, é uma arquitetura relativamente rápida, enquanto consegue manter parte da acurácia.\n",
    "\n",
    "A imagem abaixo representa esse salto do upsampling, embora no nosso caso seja de 16x (pois há 4 downsampling de 2x na MobileNetV3), o que é ainda mais brusco.\n",
    "\n",
    "![DeepLab](https://www.imaios.com/i/var/site/storage/images/6/9/8/9/499896-2-eng-GB/14%20-%20Architecture%20%20Deeplab%20V3.png?ixlib=php-3.3.1&q=75&w=920)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb460b",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função `forward` da classe `DeepLab` abaixo, de acordo com a sua documentação. A ideia é juntar o que já foi implementado invocando o encoder, o decoder e fazendo o upsampling final.\n",
    "\n",
    "Veja na imagem abaixo como ele representa: O `backbone` é o nosso encoder, as operações piramidais e convoluções já estão embutidas no nosso decoder, e o upsampling é realizado por uma interpolação bilinear não enviesada.\n",
    "\n",
    "![DeepLab](https://www.imaios.com/i/var/site/storage/images/7/8/8/9/499887-1-eng-GB/Atrous-Spatial-Pyramid-Pooling-Module.png?ixlib=php-3.3.1&q=75&w=565)\n",
    "\n",
    "Essa questão acabou que ficou dependendo da questão do decoder.\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT) para pegar a resposta pronta. **NÃO** pesquise a resposta pronta na internet.\n",
    "\n",
    "**Pode** olhar a documentação das bibliotecas (PyTorch, FastAI, mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Basta chamar o encoder e depois o decoder.\n",
    "\n",
    "Utilize a função `F.interpolate` para fazer o upsampling final. Veja como ela é chamada lá acima, na classe do `ASPPPooling`.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f934ee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c44a303477735b8b1cb31c90b9f9bd30",
     "grade": false,
     "grade_id": "questao_arq",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# questao_arq autograded_answer\n",
    "\n",
    "class DeepLab(nn.Module):\n",
    "    def __init__(self, num_classes: int, mobile_net=None):\n",
    "        super().__init__()\n",
    "        if mobile_net is None:\n",
    "            mobile_net = MobNet3large()\n",
    "            mobile_net.load_state_dict(MobileNet_V3_Large_Weights.IMAGENET1K_V2.get_state_dict(progress=True))\n",
    "        self.encoder = mobile_net.features\n",
    "        self.decoder = DeepLabDecoder(960, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\" Realiza a propagação direta do bloco:\n",
    "        Aplique o encoder, em seguida o decoder,\n",
    "        Aplique o upsampling final, interpolação bilinear não enviesada.\n",
    "\n",
    "        Args:\n",
    "            x: tensor de entrada ao bloco de shape (B, 3, H, W)\n",
    "\n",
    "        Returns:\n",
    "            tensor com shape (B, num_classes, H, W)\n",
    "        \"\"\"\n",
    "        # WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06101862",
   "metadata": {},
   "source": [
    "Se você usou uma LLM, escreva a sua conversa com ela aqui nesta própria célula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e321d33d",
   "metadata": {},
   "source": [
    "Verifica se o shape de saída está correto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328cf86",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1c5d20a7e5763dfbe88d1cd82c1e24f",
     "grade": true,
     "grade_id": "testa_1_questao_arq",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_1_questao_arq autograder_tests 0.2\n",
    "\n",
    "mobnet = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V2)\n",
    "dlseg = DeepLab(7, mobnet)\n",
    "\n",
    "imgrand = torch.rand((5, 3, 100, 120))\n",
    "\n",
    "out_rand = dlseg(imgrand)\n",
    "\n",
    "assert out_rand.shape == (5, 7, 100, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d5bf1",
   "metadata": {},
   "source": [
    "Agora vamos verificar se as operações foram implementadas corretamente. Vamos comparar com o resultado esperado de uma rede já treinada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001b107",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c68951c8156c822510faaf51139d0351",
     "grade": true,
     "grade_id": "testa_2_questao_arq",
     "locked": true,
     "points": 1.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_2_questao_arq autograder_tests 1.8\n",
    "\n",
    "mobnet = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V2)\n",
    "dlseg = DeepLab(32, mobnet)\n",
    "\n",
    "img0 = Normalize.from_stats(*imagenet_stats, cuda=False)(ToTensor()(\n",
    "                PILImage.create(data_path/'images'/'0016E5_06660.png'))/255)\n",
    "\n",
    "dlseg.load_state_dict(torch.load(base_path/'treinado.pth', 'cpu'))\n",
    "dlseg.train(False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = dlseg(img0)\n",
    "res_esperado = torch.load(base_path/'res.pt')\n",
    "\n",
    "assert torch.linalg.norm(res - res_esperado) < 1e-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce42f2a",
   "metadata": {},
   "source": [
    "Veja o resultado da imagem inicial em uma rede já treinada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9219be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(PILImage.create(data_path/'images'/'0016E5_06660.png'))\n",
    "    plt.imshow(res.numpy().argmax(axis=1)[0], cmap='gist_ncar', alpha=0.5)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa91755",
   "metadata": {},
   "source": [
    "## Treinando de Verdade (Use GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b166d40",
   "metadata": {},
   "source": [
    "**Atenção: Habilite a GPU no Colab para poder treinar em poucos minutos (ao invés de várias horas na CPU)**\n",
    "\n",
    "No primeiro passo vamos redefinir o nosso dataloader, com atenção para realizar a normalização da imagem de acordo com o que foi utilizado no encoder já pré-treinado (no caso ele treinou no ImageNet que tinha uma média e um desvio padrão específicos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ca446",
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid_datablock = DataBlock(\n",
    "    blocks=(ImageBlock, MaskBlock(nomes_labels)),\n",
    "    get_items=get_image_files,\n",
    "    splitter=RandomSplitter(),\n",
    "    get_y=lambda o: o.parent.parent/'labels'/f'{o.stem}_P{o.suffix}',\n",
    "    item_tfms=NossoTrasformaAugment(),\n",
    "    batch_tfms=Normalize.from_stats(*imagenet_stats)\n",
    ")\n",
    "camvid = camvid_datablock.dataloaders(data_path/\"images\", path=data_path, bs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a593c",
   "metadata": {},
   "source": [
    "Vamos ver as imagens geradas pelo dataloader (com as augmentations que fizemos na primeira questão):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    camvid.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d17946",
   "metadata": {},
   "source": [
    "Agora vamos instanciar a nossa rede, que irá utilizar o que implementamos, seja a arquitetura total que tem o encoder e o decoder e também a nossa loss function que implementamos separadamente.\n",
    "\n",
    "Perceba que nesse primeiro momento desabilitamos o treino do encoder (`requires_grad=False`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d04c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplab = DeepLab(len(nomes_labels))\n",
    "deeplab.encoder.requires_grad = False\n",
    "\n",
    "learner = Learner(camvid, deeplab, simple_segmentation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d5dee",
   "metadata": {},
   "source": [
    "Veja o learning rate que seria adequado para iniciarmos o treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    learner.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34069069",
   "metadata": {},
   "source": [
    "No meu caso, nós podemos nos aventurar mais do que ele sugeriu (mas tem que ser antes de começar a divergir pois o gráfico que ele mostra é suavizado).\n",
    "\n",
    "Se quiser, você também pode treinar por mais de duas épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    learner.fit_one_cycle(n_epoch=2, lr_max=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10640066",
   "metadata": {},
   "source": [
    "Agora vamos habilitar o treino do encoder também, otimizando a rede de ponta-a-ponta. Vejamos também onde fica o *cotovelo* no gráfico da learning rate (LR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplab.encoder.requires_grad = True\n",
    "if RETRAIN:\n",
    "    learner.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06938a50",
   "metadata": {},
   "source": [
    "Agora por segurança, vamos colocar uma LR de pelo menos uma ordem de grandeza antes do cotovelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b4e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    learner.fit_one_cycle(n_epoch=6, lr_max=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa40de8",
   "metadata": {},
   "source": [
    "Em geral, pelo modelo ter apenas um único estágio de upsampling, fazendo um pulo brusco das dimensões espaciais, ele acaba perdendo os detalhes mais finos. O trade-off dele é ser bem mais leve (faz menos operações).\n",
    "\n",
    "Vejamos os resultados qualitativamente (a loss function do FastAI é mais esperta e já faz o argmax na hora de exibir os resultados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    learner_show = Learner(camvid, dlseg, CrossEntropyLossFlat(axis=1))\n",
    "    learner_show.show_results(max_n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15235b7",
   "metadata": {},
   "source": [
    "Se quiser, pode salvar os pesos do seu modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    torch.save(deeplab.state_dict(), base_path / 'treinado_meu.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4c36a",
   "metadata": {},
   "source": [
    "## One-liner com FastAI Segmentation Learner (Use GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3dc8e",
   "metadata": {},
   "source": [
    "O FastAI já automaticamente utiliza a arquitetura completa baseada na UNET e com pesos do encoder já pré-treinados. Nesse caso específico, nós utilizamos a ResNet34 como encoder:\n",
    "\n",
    "Você também pode treinar por mais épocas.\n",
    "\n",
    "**Em caso de erro por falta de memória `RuntimeError: CUDA error: out of memory`, reinicie o kernel e não execute o treinamento da nossa rede acima (apenas até o dataloader)**\n",
    "\n",
    "<sub>(Não se preocupe com warning, o FastAI está chamando uma função deprecada do PyTorch)</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844de16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    learn_fastai = unet_learner(camvid, resnet34)\n",
    "    learn_fastai.fine_tune(epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99d86c",
   "metadata": {},
   "source": [
    "Vejamos os resultados, qualitativamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d872314",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    learn_fastai.show_results(max_n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d77c7",
   "metadata": {},
   "source": [
    "# Your data and feedback:\n",
    "\n",
    "Write a feedback for the lab so we can make it better for the next years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf904f63",
   "metadata": {},
   "source": [
    "In the following variables, write the number of hours spent on this lab, the perceived difficulty, and the expected grade (you may delete the `raise` and the comments):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cea9eb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36f613d80a345c28aaf937672ec9e9f5",
     "grade": true,
     "grade_id": "meta_eval",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# meta_eval manual_graded_answer 0\n",
    "\n",
    "horas_gastas = None    # 1.5   - Float number with the number of hours spent \n",
    "dificuldade_lab = None # 0     - Float number from 0.0 to 10.0 (inclusive)\n",
    "nota_esperada = None   # 10    - Float number from 0.0 to 10.0 (inclusive)\n",
    "\n",
    "# WRITE YOUR CODE HERE! (you can delete this comment, but do not delete this cell so the ID is not lost)\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523713e1",
   "metadata": {},
   "source": [
    "Write below other comments or feedbacks about the lab. If you did not understand anything about the lab, please also comment here.\n",
    "\n",
    "If you find any typo or bug in the lab, please comment below so we can fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23cb59c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc6431aff9a971fe5ed82c62bc668c96",
     "grade": true,
     "grade_id": "meta_eval_discursivo",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "WRITE YOUR SOLUTION HERE! (do not change this first line):\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**DISCURSIVE QUESTION**\n",
    "\n",
    "WRITE YOUR ANSWER HERE (do not delete this cell so the ID is not lost)\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "**ATTENTION**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66f3d7",
   "metadata": {},
   "source": [
    "**End of the lab!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
